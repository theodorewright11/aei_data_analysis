{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ab847",
   "metadata": {},
   "source": [
    "## Imports, Helper Functions, Parameter Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4302d5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fe007551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c923927",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ea1b0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower().strip()                   # lowercase + trim\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)            # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # collapse multiple spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd93a6",
   "metadata": {},
   "source": [
    "### Adjust Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7327ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency mapping. Assuming a 52 week year with 5 working days per week, these are corresponding survey questions::\n",
    "# 1 Once per year or less (Assuming 1 time per year)\n",
    "# 2 More than once per year (Assuming 3 times per year)\n",
    "# 3 More than once per month (Assuming 48 times per year, 3 times per month)\n",
    "# 4 More than once per week (Assuming 130 times per year, 2.5 times per week)\n",
    "# 5 Daily\n",
    "# 6 Several times per day (Assuming 3 times per day)\n",
    "# 7 Hourly or more often (Assuming 12 times per day, 1.5 times per hour)\n",
    "frequency_weights = {\n",
    "    1: 1 / 260,\n",
    "    2: 3 / 260,\n",
    "    3: 48 / 260,\n",
    "    4: 130 / 260,\n",
    "    5: 1,\n",
    "    6: 3,\n",
    "    7: 12\n",
    "}\n",
    "\n",
    "# Adjust inflation for scraped wage data (Jan 2020 to now)\n",
    "jan_2020_inflation_factor = 1.24\n",
    "\n",
    "# Adjust inflation for BLS wage data (May 2015 to now)\n",
    "may_2015_inflation_factor = 1.36\n",
    "\n",
    "# Change to true to get each data frame saved to a csv after every step to a folder named merged_data_files\n",
    "save_files_each_step = False\n",
    "\n",
    "if save_files_each_step:\n",
    "    import os\n",
    "    os.makedirs(\"../merged_data_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dac79",
   "metadata": {},
   "source": [
    "## Step 1: Map Anthropic Task %s to O*NET v20.1 Task Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5adc1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_to_onet_tasks(pct_df, task_statements_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the tasks and percentage of occurrences from the Anthropic data, and merges it with the tasks statement data. \n",
    "        It normalizes the percents based on a weighted and non weighted approach.\n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_df (pd.DataFrame): DataFrame containing the Anthropic data of percent occurances of every task in their conversation data\n",
    "        task_statements_df (pd.DataFrame): DataFrame containing O*NET tasks and SOC titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with percentage of occurrences added.\n",
    "    \"\"\"\n",
    "\n",
    "    task_statements_df.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"soc_code_2010\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Task ID\": \"task_id\",\n",
    "    \"Task\": \"task\",\n",
    "    \"Task Type\": \"task_type\",\n",
    "    \"Incumbents Responding\": \"n_responding\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Domain Source\": \"domain_source\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Normalize task columns\n",
    "    pct_df[\"task_normalized_temp\"] = pct_df[\"task_name\"].apply(normalize_text)\n",
    "    task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].apply(normalize_text)\n",
    "    # task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].str.lower().str.strip()\n",
    "\n",
    "    pct_df = pct_df.groupby(\"task_normalized_temp\", as_index=False).agg({\n",
    "    \"task_name\": \"first\",  # Keep the first task name\n",
    "    \"pct\": \"sum\"  # Sum the percentages for duplicates\n",
    "    })\n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_df.merge(\n",
    "        task_statements_df,\n",
    "        left_on=\"task_normalized_temp\",\n",
    "        right_on=\"task_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted and normalized percentages\n",
    "    merged[\"n_occurrences\"] = merged.groupby(\"task_normalized\")[\"title\"].transform(\"nunique\")\n",
    "    merged[\"pct_weighted\"] = 100 * merged[\"pct\"] / merged[\"pct\"].sum()\n",
    "    merged[\"pct_normalized\"] = 100 * (merged[\"pct\"] / merged[\"n_occurrences\"]) / (merged[\"pct\"] / merged[\"n_occurrences\"]).sum()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"task_name\", \"task_normalized_temp\", \"pct\"], inplace=True)\n",
    "\n",
    "    # Reorder so `task` is first and `task_normalized` is second\n",
    "    cols = [\"task\", \"task_normalized\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\"]]\n",
    "    merged = merged[cols]\n",
    "    \n",
    "    # Sort by O*NET-SOC Code\n",
    "    merged.sort_values(by=\"soc_code_2010\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "task_statements_df = pd.read_csv(\"../data/task_statements_v20.1.csv\")\n",
    "pct_df = pd.read_csv(\"../original_data/onet_task_mappings.csv\")\n",
    "pct_onet_tasks_df = pct_to_onet_tasks(pct_df, task_statements_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    pct_onet_tasks_df.to_csv(\"../merged_data_files/pct_onet_tasks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825bff0",
   "metadata": {},
   "source": [
    "## Step 2: Add SOC Major Occupational Category And Broad Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "522105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_soc_structure(pct_onet_tasks_df, soc_structure_df, detailed_occ_2010) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the previous DataFrame and adds major occupational categories to each row based on the soc structure data\n",
    "        It also creates a column counting how many detailed occupations are in each broad category \n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_onet_tasks_df (pd.DataFrame): DataFrame from previous step containing pcts mapped to task statements and O*NET metadata\n",
    "        soc_structure_df (pd.DataFrame): DataFrame containing the SOC structure with major, minor, and detailed categories for occupations\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with major occupational categories added\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename column\n",
    "    soc_structure_df.rename(columns={\n",
    "    \"SOC or O*NET-SOC 2019 Title\": \"major_occ_category\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Create new df and columns for merging\n",
    "    pct_onet_tasks_df[\"major_group_code\"] = pct_onet_tasks_df[\"soc_code_2010\"].str[:2]\n",
    "    soc_structure_major_df = soc_structure_df.dropna(subset=['Major Group']).copy()\n",
    "    soc_structure_major_df[\"major_group_code\"] = soc_structure_major_df[\"Major Group\"].str[:2]\n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_onet_tasks_df.merge(\n",
    "        soc_structure_major_df[['major_group_code', 'major_occ_category']],\n",
    "        on='major_group_code',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Create broad counts\n",
    "\n",
    "    # Derive 6-digit soc (drop decimals like .03)\n",
    "    detailed_occ_2010[\"soc6\"] = detailed_occ_2010[\"O*NET-SOC 2010 Code\"].str[:7]\n",
    "\n",
    "    # Derive broad code (replace last digit with 0 → e.g. 11-1011 → 11-1010)\n",
    "    detailed_occ_2010[\"broad_code\"] = detailed_occ_2010[\"soc6\"].str.replace(r\"(\\d)$\", \"0\", regex=True)\n",
    "\n",
    "    # Count unique detailed codes per broad\n",
    "    broad_counts_map = (\n",
    "        detailed_occ_2010.groupby(\"broad_code\")[\"soc6\"]\n",
    "            .nunique()\n",
    "            .astype(\"Int64\")\n",
    "            .to_dict()\n",
    "    )\n",
    "   \n",
    "    # Derive broad code from each row's SOC 2010 (e.g., 11-1011.03 -> 11-1010)\n",
    "    merged[\"broad_occ_code\"] = (\n",
    "        merged[\"soc_code_2010\"].astype(str).str[:7].str.replace(r\"(\\d)$\", \"0\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Map counts\n",
    "    merged[\"broad_counts\"] = merged[\"broad_occ_code\"].map(broad_counts_map)\n",
    "\n",
    "    # Cleanup helpers\n",
    "    merged.drop(columns=[\"broad_occ_code\", \"major_group_code\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "soc_structure_df = pd.read_csv(\"../data/soc_structure_2019.csv\")\n",
    "detailed_occ_2010 = pd.read_csv(\"../data/detailed_occ_2010.csv\")\n",
    "pct_tasks_soc_structure_df = add_soc_structure(pct_onet_tasks_df, soc_structure_df, detailed_occ_2010)\n",
    "\n",
    "if save_files_each_step:\n",
    "    pct_tasks_soc_structure_df.to_csv(\"../merged_data_files/pct_tasks_soc_structure.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f3c3f",
   "metadata": {},
   "source": [
    "## Step 3: Add 2024 Wage and Employment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9742d3d",
   "metadata": {},
   "source": [
    "### 3.1: Add Updated (2019) SOC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca5f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "broad_counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "soc_code_2019",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "0080facc-288f-4fef-8cb2-b70504a913d9",
       "rows": [
        [
         "0",
         "Chief Executives",
         "1.0",
         "11-1011"
        ],
        [
         "1",
         "Chief Sustainability Officers",
         "1.0",
         "11-1011"
        ],
        [
         "2",
         "General and Operations Managers",
         "1.0",
         "11-1021"
        ],
        [
         "3",
         "Legislators",
         "1.0",
         "11-1031"
        ],
        [
         "4",
         "Advertising and Promotions Managers",
         "1.0",
         "11-2011"
        ],
        [
         "5",
         "Green Marketers",
         "1.0",
         "11-2011"
        ],
        [
         "6",
         "Marketing Managers",
         "2.0",
         "11-2021"
        ],
        [
         "7",
         "Sales Managers",
         "2.0",
         "11-2022"
        ],
        [
         "8",
         "Public Relations and Fundraising Managers",
         "1.0",
         "11-2032"
        ],
        [
         "9",
         "Public Relations and Fundraising Managers",
         "1.0",
         "11-2033"
        ],
        [
         "10",
         "Administrative Services Managers",
         "1.0",
         "11-3012"
        ],
        [
         "11",
         "Administrative Services Managers",
         "1.0",
         "11-3013"
        ],
        [
         "12",
         "Computer and Information Systems Managers",
         "1.0",
         "11-3021"
        ],
        [
         "13",
         "Treasurers and Controllers",
         "1.0",
         "11-3031"
        ],
        [
         "14",
         "Financial Managers, Branch or Department",
         "1.0",
         "11-3031"
        ],
        [
         "15",
         "Industrial Production Managers",
         "1.0",
         "11-3051"
        ],
        [
         "16",
         "Quality Control Systems Managers",
         "1.0",
         "11-3051"
        ],
        [
         "17",
         "Geothermal Production Managers",
         "1.0",
         "11-3051"
        ],
        [
         "18",
         "Biomass Power Plant Managers",
         "1.0",
         "11-3051"
        ],
        [
         "19",
         "Purchasing Managers",
         "1.0",
         "11-3061"
        ],
        [
         "20",
         "Transportation Managers",
         "1.0",
         "11-3071"
        ],
        [
         "21",
         "Storage and Distribution Managers",
         "1.0",
         "11-3071"
        ],
        [
         "22",
         "Logistics Managers",
         "1.0",
         "11-3071"
        ],
        [
         "23",
         "Compensation and Benefits Managers",
         "1.0",
         "11-3111"
        ],
        [
         "24",
         "Human Resources Managers",
         "1.0",
         "11-3121"
        ],
        [
         "25",
         "Training and Development Managers",
         "1.0",
         "11-3131"
        ],
        [
         "26",
         "Nursery and Greenhouse Managers",
         "1.0",
         "11-9013"
        ],
        [
         "27",
         "Farm and Ranch Managers",
         "1.0",
         "11-9013"
        ],
        [
         "28",
         "Aquacultural Managers",
         "1.0",
         "11-9013"
        ],
        [
         "29",
         "Construction Managers",
         "1.0",
         "11-9021"
        ],
        [
         "30",
         "Education Administrators, Preschool and Childcare Center/Program",
         "4.0",
         "11-9031"
        ],
        [
         "31",
         "Education Administrators, Elementary and Secondary School",
         "4.0",
         "11-9032"
        ],
        [
         "32",
         "Education Administrators, Postsecondary",
         "4.0",
         "11-9033"
        ],
        [
         "33",
         "Distance Learning Coordinators",
         "4.0",
         "11-9039"
        ],
        [
         "34",
         "Fitness and Wellness Coordinators",
         "4.0",
         "11-9179"
        ],
        [
         "35",
         "Architectural and Engineering Managers",
         "1.0",
         "11-9041"
        ],
        [
         "36",
         "Biofuels/Biodiesel Technology and Product Development Managers",
         "1.0",
         "11-9041"
        ],
        [
         "37",
         "Food Service Managers",
         "1.0",
         "11-9051"
        ],
        [
         "38",
         "Funeral Service Managers",
         "1.0",
         "11-9171"
        ],
        [
         "39",
         "Gaming Managers",
         "1.0",
         "11-9071"
        ],
        [
         "40",
         "Lodging Managers",
         "1.0",
         "11-9081"
        ],
        [
         "41",
         "Medical and Health Services Managers",
         "1.0",
         "11-9111"
        ],
        [
         "42",
         "Natural Sciences Managers",
         "1.0",
         "11-9121"
        ],
        [
         "43",
         "Clinical Research Coordinators",
         "1.0",
         "11-9121"
        ],
        [
         "44",
         "Water Resource Specialists",
         "1.0",
         "11-9121"
        ],
        [
         "45",
         "Property, Real Estate, and Community Association Managers",
         "1.0",
         "11-9141"
        ],
        [
         "46",
         "Social and Community Service Managers",
         "1.0",
         "11-9151"
        ],
        [
         "47",
         "Emergency Management Directors",
         "1.0",
         "11-9161"
        ],
        [
         "48",
         "Regulatory Affairs Managers",
         "1.0",
         "11-9199"
        ],
        [
         "49",
         "Compliance Managers",
         "1.0",
         "11-9199"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 782
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>broad_counts</th>\n",
       "      <th>soc_code_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11-1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11-1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11-1021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legislators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11-1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Loading Machine Operators, Underground Mining</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47-5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Cleaners of Vehicles and Equipment</td>\n",
       "      <td>4.0</td>\n",
       "      <td>53-7061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Laborers and Freight, Stock, and Material Move...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>53-7062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Pump Operators, Except Wellhead Pumpers</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53-7072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  broad_counts  \\\n",
       "0                                     Chief Executives           1.0   \n",
       "1                        Chief Sustainability Officers           1.0   \n",
       "2                      General and Operations Managers           1.0   \n",
       "3                                          Legislators           1.0   \n",
       "4                  Advertising and Promotions Managers           1.0   \n",
       "..                                                 ...           ...   \n",
       "777      Loading Machine Operators, Underground Mining           3.0   \n",
       "778                 Cleaners of Vehicles and Equipment           4.0   \n",
       "779  Laborers and Freight, Stock, and Material Move...           4.0   \n",
       "780            Pump Operators, Except Wellhead Pumpers           3.0   \n",
       "781                                                NaN           NaN   \n",
       "\n",
       "    soc_code_2019  \n",
       "0         11-1011  \n",
       "1         11-1011  \n",
       "2         11-1021  \n",
       "3         11-1031  \n",
       "4         11-2011  \n",
       "..            ...  \n",
       "777       47-5044  \n",
       "778       53-7061  \n",
       "779       53-7062  \n",
       "780       53-7072  \n",
       "781           NaN  \n",
       "\n",
       "[782 rows x 3 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get df of updated SOC codes to merge with up to date wage and employment data\n",
    "\n",
    "def add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles from our main df and their corresponding O*NET-SOC 2019 code (some titles are duplicated as they get split into different SOC codes)\n",
    "    This is so we can merge the wage and employment data separate from our main df and merge all at once. \n",
    "\n",
    "    Args:\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from previous step.\n",
    "        soc_crosswalk_df (pd.DataFrame): DataFrame 2010 and 2019 occupation titles and SOC codes\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'soc_code_2019' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename columns\n",
    "    soc_crosswalk_df = soc_crosswalk_df.rename(\n",
    "        columns={\n",
    "            \"O*NET-SOC 2010 Title\": \"title\",\n",
    "            \"O*NET-SOC 2019 Code\": \"onet_soc_code_2019\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    soc_crosswalk_df['soc_code_2019'] = soc_crosswalk_df['onet_soc_code_2019'].str[:7]\n",
    "\n",
    "    # Get unique titles from rolling DataFrame\n",
    "    titles_df = pct_tasks_soc_structure_df[[\"title\", \"broad_counts\"]].drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "    # Merge to attach 2019 SOC codes\n",
    "    merged = titles_df.merge(\n",
    "        soc_crosswalk_df[[\"title\", \"soc_code_2019\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "soc_crosswalk_df = pd.read_csv(\"../data/2010_to_2019_soc_crosswalk.csv\")\n",
    "title_and_2019_soc_df = add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    title_and_2019_soc_df.to_csv(\"../merged_data_files/title_and_2019_soc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb78cf",
   "metadata": {},
   "source": [
    "### 3.2: Add 2024 National Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "edf237b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_wage_2024(title_and_2019_soc_df, nat_wage_df, scraped_wage_df, inflation_fac) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national annual and hourly median salary from 2024. \n",
    "    It also includes a 6 (from previous df) & 5 digit SOC code for use in following merging. \n",
    "\n",
    "    Args:\n",
    "        title_and_2019_soc_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_wage_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "        scraped_wage_df (pd.DataFrame): DataFrame containing scraped wage data from O*NET's website from Jan 2020 \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = nat_wage_df[[\"OCC_CODE\", \"O_GROUP\", \"H_MEDIAN\", \"A_MEDIAN\"]].copy()\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"H_MEDIAN\", \"A_MEDIAN\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = title_and_2019_soc_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on\n",
    "    merged[\"5_digit_soc\"] = merged[\"soc_code_2019\"].astype(str).str[:6]     \n",
    "    wage_df_trimmed[\"5_digit_soc\"] = wage_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    wage_df_trimmed_fallback_1st = wage_df_trimmed[wage_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        wage_df_trimmed_fallback_1st[[\"5_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from fallback columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "    # Create column to merge on and where annual median is missing\n",
    "    scraped_wage_df[\"title\"] = scraped_wage_df[\"JobName\"]\n",
    "    merged_fallback_2nd = merged[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create 2nd fallback df with scraper wage data\n",
    "    fallback_merge_2nd = merged_fallback_2nd.merge(\n",
    "        scraped_wage_df[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\", how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_2nd_unique_titles = fallback_merge_2nd.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge 2nd fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_2nd_unique_titles[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from scraper median columns and make present value due to inflation\n",
    "    inflation_factor = inflation_fac\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"MedianSalary\"] * inflation_factor)\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"A_MEDIAN\"].isna() & merged[\"H_MEDIAN\"].notna(), \"A_MEDIAN\"] = (\n",
    "        merged[\"H_MEDIAN\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].notna(), \"H_MEDIAN\"] = (\n",
    "        merged[\"A_MEDIAN\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Create final national wage columns by averaging for any duplicate titles and drop uneeded columns. \n",
    "    merged[\"h_median_national\"] = merged.groupby(\"title\")[\"H_MEDIAN\"].transform(\"mean\")\n",
    "    merged[\"a_median_national\"] = merged.groupby(\"title\")[\"A_MEDIAN\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"H_MEDIAN\", \"A_MEDIAN\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\", \"MedianSalary\", \"O_GROUP\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_wage_2024_df = pd.read_csv(\"../data/oews_national_2024.csv\")\n",
    "scraped_wage_df = pd.read_csv(\"../data/scraped_wage_data.csv\")\n",
    "titles_and_nat_wage_2024_df = add_nat_wage_2024(title_and_2019_soc_df, nat_wage_2024_df, scraped_wage_df, jan_2020_inflation_factor)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_and_nat_wage_2024_df.to_csv(\"../merged_data_files/titles_and_nat_wage_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5fa20",
   "metadata": {},
   "source": [
    "### 3.3: Add 2024 State Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9bcee71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_wage_2024(titles_and_nat_wage_df, state_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state annual and hourly median salary from 2024. \n",
    "\n",
    "    Args:\n",
    "        titles_and_nat_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        wage_df (pd.DataFrame): DataFrame of OEWS data from 2024 with state level breakdown\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = state_wage_df[[\"OCC_CODE\", \"H_MEDIAN\", \"A_MEDIAN\", \"AREA_TITLE\"]].copy()\n",
    "    wage_df_trimmed = wage_df_trimmed[wage_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\",\n",
    "                                    \"H_MEDIAN\": \"h_median_state\",\n",
    "                                    \"A_MEDIAN\": \"a_median_state\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"h_median_state\", \"a_median_state\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_and_nat_wage_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"a_median_state\"].isna() & merged[\"h_median_state\"].notna(), \"a_median_state\"] = (\n",
    "        merged[\"h_median_state\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"h_median_state\"].isna() & merged[\"a_median_state\"].notna(), \"h_median_state\"] = (\n",
    "        merged[\"a_median_state\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data\n",
    "    merged.loc[merged[\"a_median_state\"].isna(), \"a_median_state\"] = (\n",
    "        merged[\"a_median_national\"]\n",
    "    )\n",
    "    merged.loc[merged[\"h_median_state\"].isna(), \"h_median_state\"] = (\n",
    "        merged[\"h_median_national\"]\n",
    "    )\n",
    "\n",
    "    merged[\"h_median_utah\"] = merged.groupby(\"title\")[\"h_median_state\"].transform(\"mean\")\n",
    "    merged[\"a_median_utah\"] = merged.groupby(\"title\")[\"a_median_state\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"h_median_state\", \"a_median_state\", \"AREA_TITLE\"], inplace=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "state_wage_df_2024 = pd.read_csv(\"../data/oews_states_2024.csv\")\n",
    "titles_nat_and_state_wage_2024_df = add_state_wage_2024(titles_and_nat_wage_2024_df, state_wage_df_2024)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_nat_and_state_wage_2024_df.to_csv(\"../merged_data_files/titles_nat_and_state_wage_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee5942",
   "metadata": {},
   "source": [
    "### 3.4: Add 2024 National Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e12cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_emp_2024(titles_nat_and_state_wage_df, nat_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national employment data from 2024.  \n",
    "\n",
    "    Args:\n",
    "        titles_nat_and_state_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national employment data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"O_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_nat_and_state_wage_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    fallback_merge[\"TOT_EMP_fallback\"] = fallback_merge[\"TOT_EMP_fallback\"] / fallback_merge[\"broad_counts\"]\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurences for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_national\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP_fallback\", \"TOT_EMP\", \"O_GROUP\", \"TOT_EMP_adj\", \"broad_counts\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_emp_df_2024 = pd.read_csv(\"../data/oews_national_2024.csv\")\n",
    "titles_wage_nat_emp_2024_df = add_nat_emp_2024(titles_nat_and_state_wage_2024_df, nat_emp_df_2024)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_wage_nat_emp_2024_df.to_csv(\"../merged_data_files/titles_wage_nat_emp_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd5aae",
   "metadata": {},
   "source": [
    "### 3.5: Add 2024 State Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "38846383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_emp_2024(titles_wage_nat_emp_df, state_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state employment data from 2024.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_nat_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_emp_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state employment data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"AREA_TITLE\"]].copy()\n",
    "    emp_df_trimmed = emp_df_trimmed[emp_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_wage_nat_emp_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[\n",
    "    (state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"AREA_TITLE\"] == \"Utah\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"emp_total_national\"] * utah_share).round())\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurances for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_utah\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP\", \"AREA_TITLE\", \"TOT_EMP_adj\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_2024_df = pd.read_csv(\"../data/oews_states_2024.csv\")\n",
    "titles_wage_all_emp_2024_df = add_state_emp_2024(titles_wage_nat_emp_2024_df, state_emp_2024_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_wage_all_emp_2024_df.to_csv(\"../merged_data_files/titles_wage_all_emp_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6269e53",
   "metadata": {},
   "source": [
    "### 3.6: Merge 2024 Wage and Employment Data Into Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "16d96d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage_emp_to_tasks_2024(titles_wage_all_emp_df, pct_tasks_soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with our wage and employment data from 2024 added to our task data.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_all_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from step 2\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with wage and employment data from 2024 added to task data\n",
    "    \"\"\"\n",
    "\n",
    "    titles_wage_all_emp_df = titles_wage_all_emp_df.drop_duplicates(subset=\"title\").copy()\n",
    "\n",
    "    titles_wage_all_emp_df.drop(columns=[\"5_digit_soc\", \"soc_code_2019\"], inplace=True)\n",
    "\n",
    "    merged = pct_tasks_soc_structure_df.merge(\n",
    "        titles_wage_all_emp_df,\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged.rename(columns={\"h_median_national\": \"h_med_nat_2024\",\n",
    "                                    \"a_median_national\": \"a_med_nat_2024\",\n",
    "                                    \"h_median_utah\": \"h_med_ut_2024\",\n",
    "                                    \"a_median_utah\": \"a_med_ut_2024\",\n",
    "                                    \"emp_total_national\": \"emp_tot_nat_2024\",\n",
    "                                    \"emp_total_utah\": \"emp_tot_ut_2024\"}, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "\n",
    "task_wage_emp_2024_df = wage_emp_to_tasks_2024(titles_wage_all_emp_2024_df, pct_tasks_soc_structure_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    task_wage_emp_2024_df.to_csv(\"../merged_data_files/task_wage_emp_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3e925",
   "metadata": {},
   "source": [
    "## Step 4: Add 2015 Wage and Employment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92805752",
   "metadata": {},
   "source": [
    "### 4.1: Add 2015 National Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bfbdec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_wage_2015(pct_tasks_soc_structure_df, nat_wage_df, inflation_fac) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame of titles and their 2010 SOC codes\n",
    "    Returns DataFrame with occupation titles along with their national annual and hourly median salary from 2015 in real and nominal terms merged with titles and SOC codes. \n",
    "    It also includes a 5 digit SOC code for use in following merging. \n",
    "\n",
    "    Args:\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from Step 2.\n",
    "        nat_wage_df (pd.DataFrame): DataFrame of OEWS data from 2015 \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Make df with titles and SOC codes\n",
    "    title_soc_code_2010_df = pct_tasks_soc_structure_df[[\"title\", \"soc_code_2010\", \"broad_counts\"]].drop_duplicates(subset=\"title\").copy()\n",
    "    title_soc_code_2010_df.reset_index(drop=True, inplace=True)\n",
    "    title_soc_code_2010_df['soc_code_2010'] = title_soc_code_2010_df['soc_code_2010'].str[:7]\n",
    "\n",
    "    # Get only columns needed\n",
    "    wage_df_trimmed = nat_wage_df[[\"OCC_CODE\", \"OCC_GROUP\", \"H_MEDIAN\", \"A_MEDIAN\", \"H_MEAN\", \"A_MEAN\"]].copy()\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"H_MEDIAN\", \"A_MEDIAN\", \"H_MEAN\", \"A_MEAN\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = title_soc_code_2010_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"A_MEDIAN\"].isna() & merged[\"H_MEDIAN\"].notna(), \"A_MEDIAN\"] = (\n",
    "        merged[\"H_MEDIAN\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].notna(), \"H_MEDIAN\"] = (\n",
    "        merged[\"A_MEDIAN\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on\n",
    "    merged[\"5_digit_soc\"] = merged[\"soc_code_2010\"].astype(str).str[:6]     \n",
    "    wage_df_trimmed[\"5_digit_soc\"] = wage_df_trimmed[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    wage_df_trimmed_fallback_1st = wage_df_trimmed[wage_df_trimmed[\"OCC_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        wage_df_trimmed_fallback_1st[[\"5_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from fallback columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "    # Fill missing median values from mean columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEAN\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEAN\"])\n",
    "\n",
    "    # Rename and drop columns for cleanup \n",
    "    merged.rename(columns={\"H_MEDIAN\": \"h_med_nat_nominal\"}, inplace=True)\n",
    "    merged.rename(columns={\"A_MEDIAN\": \"a_med_nat_nominal\"}, inplace=True)\n",
    "    merged.drop(columns=[\"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\", \"H_MEAN\", \"A_MEAN\", \"OCC_GROUP\"], inplace=True)\n",
    "\n",
    "    # Make present value column for inflation\n",
    "    inflation_factor = inflation_fac\n",
    "    merged[\"h_med_nat_real\"] = merged[\"h_med_nat_nominal\"] * inflation_factor\n",
    "    merged[\"a_med_nat_real\"] = merged[\"a_med_nat_nominal\"] * inflation_factor\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_wage_df_2015 = pd.read_csv(\"../data/oews_national_2015.csv\")\n",
    "titles_and_nat_wage_2015_df = add_nat_wage_2015(pct_tasks_soc_structure_df, nat_wage_df_2015, may_2015_inflation_factor)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_and_nat_wage_2015_df.to_csv(\"../merged_data_files/titles_and_nat_wage_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ddefb",
   "metadata": {},
   "source": [
    "### 4.2: Add 2015 State Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a9c10497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_wage_2015(titles_and_nat_wage_df, state_wage_df, inflation_fac) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state annual and hourly median salary from 2015 in nominal and real terms. \n",
    "\n",
    "    Args:\n",
    "        titles_and_nat_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_wage_df (pd.DataFrame): DataFrame of OEWS data from 2015 with state level breakdown\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state wage data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    wage_df_trimmed = state_wage_df[[\"OCC_CODE\", \"H_MEDIAN\", \"A_MEDIAN\", \"ST\"]].copy()\n",
    "    wage_df_trimmed = wage_df_trimmed[wage_df_trimmed[\"ST\"] == \"UT\"]\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\",\n",
    "                                    \"H_MEDIAN\": \"h_median_state\",\n",
    "                                    \"A_MEDIAN\": \"a_median_state\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"h_median_state\", \"a_median_state\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_and_nat_wage_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"a_median_state\"].isna() & merged[\"h_median_state\"].notna(), \"a_median_state\"] = (\n",
    "        merged[\"h_median_state\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"h_median_state\"].isna() & merged[\"a_median_state\"].notna(), \"h_median_state\"] = (\n",
    "        merged[\"a_median_state\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data\n",
    "    merged.loc[merged[\"a_median_state\"].isna(), \"a_median_state\"] = (\n",
    "        merged[\"a_med_nat_nominal\"]\n",
    "    )\n",
    "    merged.loc[merged[\"h_median_state\"].isna(), \"h_median_state\"] = (\n",
    "        merged[\"h_med_nat_nominal\"]\n",
    "    )\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"h_median_state\": \"h_med_utah_nominal\",\n",
    "                                    \"a_median_state\": \"a_med_utah_nominal\"}, inplace=True)\n",
    "    merged.drop(columns=[\"ST\"], inplace=True)\n",
    "\n",
    "    # Make present value column for inflation\n",
    "    inflation_factor = inflation_fac\n",
    "    merged[\"h_med_utah_real\"] = merged[\"h_med_utah_nominal\"] * inflation_factor\n",
    "    merged[\"a_med_utah_real\"] = merged[\"a_med_utah_nominal\"] * inflation_factor\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_wage_df_2015 = pd.read_csv(\"../data/oews_states_2015.csv\")\n",
    "titles_nat_and_state_wage_2015_df = add_state_wage_2015(titles_and_nat_wage_2015_df, state_wage_df_2015, may_2015_inflation_factor)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_nat_and_state_wage_2015_df.to_csv(\"../merged_data_files/titles_nat_and_state_wage_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f22d4",
   "metadata": {},
   "source": [
    "### 4.3: Add 2015 National Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "62763f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_emp_2015(titles_nat_and_state_wage_df, nat_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national employment data from 2015.  \n",
    "\n",
    "    Args:\n",
    "        titles_nat_and_state_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2015.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national employment data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"OCC_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_nat_and_state_wage_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"OCC_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    fallback_merge[\"TOT_EMP_fallback\"] = fallback_merge[\"TOT_EMP_fallback\"] / fallback_merge[\"broad_counts\"]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"emp_tot_nat\"}, inplace=True)\n",
    "    merged.drop(columns=[\"TOT_EMP_fallback\", \"OCC_GROUP\", \"broad_counts\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_emp_df_2015 = pd.read_csv(\"../data/oews_national_2015.csv\")\n",
    "titles_wage_nat_emp_2015_df = add_nat_emp_2015(titles_nat_and_state_wage_2015_df, nat_emp_df_2015)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_wage_nat_emp_2015_df.to_csv(\"../merged_data_files/titles_wage_nat_emp_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97bb12",
   "metadata": {},
   "source": [
    "### 4.4: Add 2015 State Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3d2cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_emp_2015(titles_wage_nat_emp_df, state_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state employment data from 2015.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_nat_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_emp_df (pd.DataFrame): DataFrame of OEWS data from 2015.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state employment data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"ST\"]].copy()\n",
    "    emp_df_trimmed = emp_df_trimmed[emp_df_trimmed[\"ST\"] == \"UT\"]\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_wage_nat_emp_2015_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[(state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"ST\"] == \"UT\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"emp_tot_nat\"] * utah_share).round())\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"emp_tot_utah\"}, inplace=True)\n",
    "    merged.drop(columns=[\"ST\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_2015_df = pd.read_csv(\"../data/oews_states_2015.csv\")\n",
    "titles_wage_all_emp_2015_df = add_state_emp_2015(titles_wage_nat_emp_2015_df, state_emp_2015_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    titles_wage_all_emp_2015_df.to_csv(\"../merged_data_files/titles_wage_all_emp_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef3b3e",
   "metadata": {},
   "source": [
    "### 4.5: Merge 2015 Wage and Employment Data Into Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cba12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage_emp_to_tasks_2015(titles_wage_all_emp_df, pct_tasks_soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with our wage and employment data from 2015 added to our task data.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_all_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from step 2\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with wage and employment data from 2015 added to task data\n",
    "    \"\"\"\n",
    "\n",
    "    titles_wage_all_emp_df = titles_wage_all_emp_df.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    titles_wage_all_emp_df.drop(columns=[\"soc_code_2010\", \"5_digit_soc\"], inplace=True)\n",
    "\n",
    "    merged = pct_tasks_soc_structure_df.merge(\n",
    "        titles_wage_all_emp_df,\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged.rename(columns={\"h_med_nat_nominal\": \"h_med_nat_nominal_2015\",\n",
    "                            \"a_med_nat_nominal\": \"a_med_nat_nominal_2015\",\n",
    "                            \"h_med_nat_real\": \"h_med_nat_real_2015\",\n",
    "                            \"a_med_nat_real\": \"a_med_nat_real_2015\",\n",
    "                            \"h_med_utah_nominal\": \"h_med_ut_nominal_2015\",\n",
    "                            \"a_med_utah_nominal\": \"a_med_ut_nominal_2015\",\n",
    "                            \"h_med_utah_real\": \"h_med_ut_real_2015\",\n",
    "                            \"a_med_utah_real\": \"a_med_ut_real_2015\",\n",
    "                            \"emp_tot_nat\": \"emp_tot_nat_2015\",\n",
    "                            \"emp_tot_utah\": \"emp_tot_ut_2015\"}, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "\n",
    "tasks_all_wage_emp_df = wage_emp_to_tasks_2015(titles_wage_all_emp_2015_df, task_wage_emp_2024_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_all_wage_emp_df.to_csv(\"../merged_data_files/tasks_all_wage_emp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af587a",
   "metadata": {},
   "source": [
    "## Step 5: Adjust Employment Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a19927ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_emp(tasks_all_wage_emp_df, state_emp_2015_df, state_emp_2024_df, nat_emp_df_2015, nat_emp_df_2024) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reallocates employment numbers based on the relative percent of Claude conversations, as we have some duplicate\n",
    "    6 digit SOC codes but different titles  \n",
    "\n",
    "    Args:\n",
    "        tasks_all_wage_emp_df (pd.DataFrame): DataFrame from previous 4.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with correct employment numbers\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_all_wage_emp_df\n",
    "\n",
    "    # 6-digit SOC to remove decimals (e.g., '11-1011.03' -> '11-1011')\n",
    "    df[\"soc6\"] = df[\"soc_code_2010\"].astype(str).str[:7]\n",
    "\n",
    "    # share of each title within its 6-digit SOC based on pct_normalized\n",
    "    title_pct_sum   = df.groupby([\"soc6\",\"title\"])[\"pct_normalized\"].transform(\"sum\")\n",
    "    soc6_pct_sum    = df.groupby(\"soc6\")[\"pct_normalized\"].transform(\"sum\")\n",
    "    df[\"soc6_share\"] = title_pct_sum / soc6_pct_sum\n",
    "\n",
    "    # columns to allocate (only those that exist will be processed)\n",
    "    emp_cols = [c for c in [\"emp_tot_nat_2024\",\"emp_tot_ut_2024\",\n",
    "                            \"emp_tot_nat_2015\",\"emp_tot_ut_2015\"] if c in df.columns]\n",
    "\n",
    "    # Calculate the correct employment numbers by multiplying each by their share in the 6 digit SOC group\n",
    "    for c in emp_cols:\n",
    "        soc6_tot = df.groupby(\"soc6\")[c].transform(\"max\") \n",
    "        alloc_col = f\"{c}_alloc_by_pct\"\n",
    "        df[c] = round(soc6_tot * df[\"soc6_share\"])\n",
    "\n",
    "    # Create percent-of-workforce columns from the reallocated totals\n",
    "    pct_map = {\n",
    "        \"emp_tot_nat_2024\":  \"emp_pct_nat_2024\",\n",
    "        \"emp_tot_ut_2024\":   \"emp_pct_ut_2024\",\n",
    "        \"emp_tot_nat_2015\":  \"emp_pct_nat_2015\",\n",
    "        \"emp_tot_ut_2015\": \"emp_pct_ut_2015\",\n",
    "    }\n",
    "\n",
    "    df_map = {\n",
    "        \"emp_tot_nat_2024\": nat_emp_df_2024,\n",
    "        \"emp_tot_ut_2024\": state_emp_2024_df, \n",
    "        \"emp_tot_nat_2015\": nat_emp_df_2015,\n",
    "        \"emp_tot_ut_2015\": state_emp_2015_df\n",
    "    }\n",
    "\n",
    "    for tot_col, pct_col in pct_map.items():\n",
    "        if tot_col in df.columns:\n",
    "            # Get total employment from \"All Occupations\" row in the corresponding DataFrame\n",
    "            source_df = df_map[tot_col]\n",
    "            total_sum = source_df[source_df[\"OCC_TITLE\"] == \"All Occupations\"][\"TOT_EMP\"].iloc[0]\n",
    "            df[pct_col] = (df.groupby(\"title\")[tot_col].transform(\"first\") / total_sum) * 100\n",
    "\n",
    "    df.drop(columns=[\"soc6\",\"soc6_share\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_wage_emp_final_df = adjust_emp(tasks_all_wage_emp_df, state_emp_2015_df, state_emp_2024_df, nat_emp_df_2015, nat_emp_df_2024)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_wage_emp_final_df.to_csv(\"../merged_data_files/tasks_wage_emp_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f1ca7",
   "metadata": {},
   "source": [
    "## Step 6: Add Task Rating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5489a",
   "metadata": {},
   "source": [
    "### 6.1: Bring In 2025 and 2015 Task Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f21e13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task_ratings(task_ratings_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes frequency, relevance, and importance from May 2025 and Oct 2015 task ratings data from O*NET.\n",
    "        Uses frequency mapping weights to get a single number for frequency\n",
    "\n",
    "    Args:\n",
    "        task_ratings_df (pd.DataFrame): DataFrame with the O*NET Task Rating data from 2025 and 2015\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with task ratings added to other task columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Get freq rows, drop unusable ones, generate freq aggregates\n",
    "    freq_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"FT\"].copy()\n",
    "\n",
    "    # Drop rows without category or invalid categories\n",
    "    freq_df = freq_df[pd.to_numeric(freq_df[\"Category\"], errors='coerce').notnull()]\n",
    "    freq_df[\"Category\"] = freq_df[\"Category\"].astype(int)\n",
    "\n",
    "    # Apply weights\n",
    "    freq_df[\"freq_mean\"] = freq_df[\"Data Value\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_lower\"] = freq_df[\"Lower CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_upper\"] = freq_df[\"Upper CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "\n",
    "    # Sum across categories to get per-task total\n",
    "    freq_agg = freq_df.groupby([\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"]).agg({\n",
    "        \"freq_mean\": \"sum\",\n",
    "        \"freq_lower\": \"sum\",\n",
    "        \"freq_upper\": \"sum\"\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Get importance and relevance ratings\n",
    "    importance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"IM\"].copy()\n",
    "    importance_df = importance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    importance_df = importance_df.rename(columns={\n",
    "        \"Data Value\": \"importance\",\n",
    "        \"Lower CI Bound\": \"importance_lower\",\n",
    "        \"Upper CI Bound\": \"importance_upper\"\n",
    "    })\n",
    "\n",
    "    relevance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"RT\"].copy()\n",
    "    relevance_df = relevance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    relevance_df = relevance_df.rename(columns={\n",
    "        \"Data Value\": \"relevance\",\n",
    "        \"Lower CI Bound\": \"relevance_lower\",\n",
    "        \"Upper CI Bound\": \"relevance_upper\"\n",
    "    })\n",
    "\n",
    "    # Merge ratings\n",
    "    merged_ratings = freq_agg.merge(importance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "    merged_ratings = merged_ratings.merge(relevance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "\n",
    "    merged_ratings[\"task_normalized\"] = merged_ratings[\"Task\"].str.lower().str.strip()\n",
    "\n",
    "    return merged_ratings\n",
    "\n",
    "\n",
    "task_ratings_2025_df = pd.read_csv(\"../data/task_ratings_may_2025.csv\")\n",
    "ratings_cleaned_2025_df = add_task_ratings(task_ratings_2025_df)\n",
    "\n",
    "task_ratings_2015_df = pd.read_csv(\"../data/task_ratings_oct_2015.csv\")\n",
    "ratings_cleaned_2015_df = add_task_ratings(task_ratings_2015_df)\n",
    "\n",
    "ratings_cleaned_2025_df.to_csv(\"../data/ratings_cleaned_2025.csv\", index=False)\n",
    "ratings_cleaned_2015_df.to_csv(\"../data/ratings_cleaned_2015.csv\", index=False)\n",
    "\n",
    "if save_files_each_step:\n",
    "    ratings_cleaned_2025_df.to_csv(\"../merged_data_files/ratings_cleaned_2025.csv\", index=False)\n",
    "    ratings_cleaned_2015_df.to_csv(\"../merged_data_files/ratings_cleaned_2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab8a85",
   "metadata": {},
   "source": [
    "### 6.2: Merge Rating Values Into Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "870e63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_task_ratings(tasks_wage_emp_final_df, ratings_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data for both 2025 and 2015. Some values are missing\n",
    "    \n",
    "    Args:\n",
    "        tasks_wage_emp_final_df (pd.DataFrame): DataFrame from Step 5\n",
    "        ratings_df (pd.DataFrame): DataFrame containing cleaned task ratings (single year).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with task ratings values unfilled.\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_wage_emp_final_df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df[\"task_normalized\"] = df[\"task\"].apply(normalize_text)\n",
    "    ratings_df[\"task_normalized\"] = ratings_df[\"Task\"].apply(normalize_text)\n",
    "    df[\"title_normalized\"] = df[\"title\"].apply(normalize_text)\n",
    "    ratings_df[\"title_normalized\"] = ratings_df[\"Title\"].apply(normalize_text)\n",
    "\n",
    "\n",
    "    # Count how many times each normalized task appears\n",
    "    task_counts = df[\"task_normalized\"].value_counts()\n",
    "    is_duplicate = df[\"task_normalized\"].isin(task_counts[task_counts > 1].index)\n",
    "    is_unique = ~is_duplicate\n",
    "    df_duplicate_tasks = df[is_duplicate].copy()\n",
    "    df_unique_tasks = df[is_unique].copy()\n",
    "\n",
    "    # Count how many times each normalized task appears in ratings\n",
    "    task_counts_ratings = ratings_df[\"task_normalized\"].value_counts()\n",
    "    is_duplicate_ratings = ratings_df[\"task_normalized\"].isin(task_counts_ratings[task_counts_ratings > 1].index)\n",
    "    is_unique_ratings = ~is_duplicate_ratings\n",
    "    df_duplicate_tasks_ratings = ratings_df[is_duplicate_ratings].copy()\n",
    "    df_unique_tasks_ratings = ratings_df[is_unique_ratings].copy()\n",
    "\n",
    "    # Merge on unique tasks\n",
    "    merged_unique = df_unique_tasks.merge(\n",
    "        df_unique_tasks_ratings[\n",
    "            [\"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "             \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "             \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "             \"task_normalized\"]\n",
    "        ],\n",
    "        on=[\"task_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge on both title and task for duplicate ones\n",
    "    merged_duplicate = df_duplicate_tasks.merge(\n",
    "        df_duplicate_tasks_ratings[\n",
    "            [\"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "             \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "             \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "             \"task_normalized\", \"title_normalized\"]\n",
    "        ],\n",
    "        on=[\"task_normalized\", \"title_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged = pd.concat([merged_unique, merged_duplicate], ignore_index=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "tasks_final_2025_unfilled_df = merge_task_ratings(tasks_wage_emp_final_df, ratings_cleaned_2025_df)\n",
    "tasks_final_2015_unfilled_df = merge_task_ratings(tasks_wage_emp_final_df, ratings_cleaned_2015_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_final_2025_unfilled_df.to_csv(\"../merged_data_files/tasks_final_2025_unfilled.csv\", index=False)\n",
    "    tasks_final_2015_unfilled_df.to_csv(\"../merged_data_files/tasks_final_2015_unfilled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c8ba5",
   "metadata": {},
   "source": [
    "### 6.3: Fill Missing Task Rating Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6e600f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_ratings(tasks_final_unfilled_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function fills missing task rating values.\n",
    "    \n",
    "    Args:\n",
    "        tasks_final_unfilled_df (pd.DataFrame): DataFrame from previous step\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrames with task rating data added for both 2025 and 2015\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_final_unfilled_df\n",
    "\n",
    "    # Mark rows that are missing any of the key values\n",
    "    df[\"imputed_rating_mean\"] = False\n",
    "    df[\"imputed_rating_ci\"] = False\n",
    "\n",
    "    # Loop through each metric\n",
    "    for col in [\"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "                \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "                \"relevance\", \"relevance_lower\", \"relevance_upper\"]:\n",
    "        \n",
    "        # Group by title and compute occupation-level mean\n",
    "        occ_means = df.groupby(\"title\")[col].mean()\n",
    "\n",
    "        # Group by major occ category and compute fallback mean\n",
    "        major_occ_means = df.groupby(\"major_occ_category\")[col].mean()\n",
    "\n",
    "        # Go row by row\n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isna(row[col]):\n",
    "                occ_val = occ_means.get(row[\"title\"], None)\n",
    "                occ_count = df[(df[\"title\"] == row[\"title\"]) & (df[col].notna())].shape[0]\n",
    "\n",
    "                if occ_count >= 3 and pd.notna(occ_val):\n",
    "                    df.at[i, col] = occ_val\n",
    "                    if col in [\"freq_mean\", \"importance\", \"relevance\"]:\n",
    "                        df.at[i, \"imputed_rating_mean\"] = True\n",
    "                    else:\n",
    "                        df.at[i, \"imputed_rating_ci\"] = True\n",
    "                else:\n",
    "                    soc_val = major_occ_means.get(row[\"major_occ_category\"], None)\n",
    "                    if pd.notna(soc_val):\n",
    "                        df.at[i, col] = soc_val\n",
    "                        if col in [\"freq_mean\", \"importance\", \"relevance\"]:\n",
    "                            df.at[i, \"imputed_rating_mean\"] = True\n",
    "                        else:\n",
    "                            df.at[i, \"imputed_rating_ci\"] = True\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_final_2025_filled_df = fill_missing_ratings(tasks_final_2025_unfilled_df)\n",
    "tasks_final_2015_filled_df = fill_missing_ratings(tasks_final_2015_unfilled_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_final_2025_filled_df.to_csv(\"../merged_data_files/tasks_final_2025_filled.csv\", index=False)\n",
    "    tasks_final_2015_filled_df.to_csv(\"../merged_data_files/tasks_final_2015_filled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44d768",
   "metadata": {},
   "source": [
    "### 6.4 Merge 2015 and 2025 Task Ratings To One DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6eb01327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_task_ratings_to_one_df(base_df, add_df, base_year, add_year) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and adds year signifiers to the columns.\n",
    "    \n",
    "    Args:\n",
    "        tasks_wage_emp_final_df (pd.DataFrame): DataFrame from Step 5\n",
    "        ratings_df (pd.DataFrame): DataFrame containing cleaned task ratings from either 2025 or 2015.\n",
    "        year (int): Year of the ratings data (e.g., 2015 or 2025)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information.\n",
    "    \"\"\"\n",
    "\n",
    "    base_df = base_df.rename(columns={\n",
    "            \"freq_mean\": f\"freq_mean_{base_year}\",\n",
    "            \"freq_lower\": f\"freq_lower_{base_year}\",\n",
    "            \"freq_upper\": f\"freq_upper_{base_year}\",\n",
    "            \"importance\": f\"importance_{base_year}\",\n",
    "            \"importance_lower\": f\"importance_lower_{base_year}\",\n",
    "            \"importance_upper\": f\"importance_upper_{base_year}\",\n",
    "            \"relevance\": f\"relevance_{base_year}\",\n",
    "            \"relevance_lower\": f\"relevance_lower_{base_year}\",\n",
    "            \"relevance_upper\": f\"relevance_upper_{base_year}\",\n",
    "            \"imputed_rating_mean\": f\"imputed_rating_mean_{base_year}\",\n",
    "            \"imputed_rating_ci\": f\"imputed_rating_ci_{base_year}\"\n",
    "        })\n",
    "    \n",
    "    add_df = add_df.rename(columns={\n",
    "            \"freq_mean\": f\"freq_mean_{add_year}\",\n",
    "            \"freq_lower\": f\"freq_lower_{add_year}\",\n",
    "            \"freq_upper\": f\"freq_upper_{add_year}\",\n",
    "            \"importance\": f\"importance_{add_year}\",\n",
    "            \"importance_lower\": f\"importance_lower_{add_year}\",\n",
    "            \"importance_upper\": f\"importance_upper_{add_year}\",\n",
    "            \"relevance\": f\"relevance_{add_year}\",\n",
    "            \"relevance_lower\": f\"relevance_lower_{add_year}\",\n",
    "            \"relevance_upper\": f\"relevance_upper_{add_year}\",\n",
    "            \"imputed_rating_mean\": f\"imputed_rating_mean_{add_year}\",\n",
    "            \"imputed_rating_ci\": f\"imputed_rating_ci_{add_year}\"\n",
    "        })\n",
    "\n",
    "\n",
    "    merged = base_df.merge(\n",
    "        add_df[[f\"freq_mean_{add_year}\", f\"freq_lower_{add_year}\", f\"freq_upper_{add_year}\",\n",
    "            f\"importance_{add_year}\", f\"importance_lower_{add_year}\", f\"importance_upper_{add_year}\",\n",
    "            f\"relevance_{add_year}\", f\"relevance_lower_{add_year}\", f\"relevance_upper_{add_year}\",\n",
    "            f\"imputed_rating_mean_{add_year}\", f\"imputed_rating_ci_{add_year}\",\n",
    "            \"task_normalized\", \"title_normalized\", \"soc_code_2010\"]],\n",
    "        on=[\"task_normalized\", \"title_normalized\", \"soc_code_2010\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "tasks_final_uncleaned_df = merge_task_ratings_to_one_df(tasks_final_2025_filled_df, tasks_final_2015_filled_df, 2025, 2015)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_final_uncleaned_df.to_csv(\"../merged_data_files/tasks_final_uncleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72d4d4",
   "metadata": {},
   "source": [
    "## 7: Final Cleanup On Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3230a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleanup(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Does some final cleanup and imputing for our data\n",
    "    \n",
    "    Args:\n",
    "        tasks_final_uncleaned_df (pd.DataFrame): DataFrame from Step 5\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop None task row\n",
    "    df = df[df[\"task_normalized\"].notna()].copy()\n",
    "    df[\"pct_weighted\"] = 100 * df[\"pct_weighted\"] / df[\"pct_weighted\"].sum()\n",
    "\n",
    "    # Get current column order\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    # Find the index of 'title' and insert 'title_normalized' right after it\n",
    "    title_idx = cols.index('title')\n",
    "    cols.insert(title_idx + 1, cols.pop(cols.index('title_normalized')))\n",
    "\n",
    "    # Reorder the dataframe\n",
    "    df = df[cols]\n",
    "\n",
    "    mask_missing = df[\"task_type\"].isna()\n",
    "    df.loc[mask_missing, \"task_type\"] = df.loc[mask_missing].apply(\n",
    "        lambda row: \"Core\" if (row[\"relevance_2015\"] >= 67 and row[\"importance_2015\"] >= 3.0) else \"Supplemental\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Fill missing n_responding values with the median within the same occupation\n",
    "    df[\"n_responding\"] = df.groupby(\"soc_code_2010\")[\"n_responding\"].transform(\n",
    "        lambda x: x.fillna(x.median()) if not x.isna().all() else x\n",
    "    )\n",
    "\n",
    "    # Drop specific confidence interval and bound columns\n",
    "    cols_to_drop = [\n",
    "    'freq_lower_2025', 'freq_upper_2025', 'importance_lower_2025', 'importance_upper_2025',\n",
    "    'relevance_lower_2025', 'relevance_upper_2025', 'imputed_rating_ci_2025',\n",
    "    'freq_lower_2015', 'freq_upper_2015', 'importance_lower_2015', 'importance_upper_2015', \n",
    "    'relevance_lower_2015', 'relevance_upper_2015', 'imputed_rating_ci_2015', \"task_id\", \"date\"\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    placeholder_values = [\"#\", \"*\", \"\", \"n/a\", \"na\", \"--\"]\n",
    "    df.replace(placeholder_values, pd.NA, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_final_df = final_cleanup(tasks_final_uncleaned_df)\n",
    "\n",
    "tasks_final_df.to_csv(\"../data/tasks_final.csv\", index=False)\n",
    "\n",
    "if save_files_each_step:\n",
    "    tasks_final_df.to_csv(\"../merged_data_files/tasks_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a779a46",
   "metadata": {},
   "source": [
    "## 8: Create Economy Task Frequency Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32cdb92",
   "metadata": {},
   "source": [
    "### 8.1: National Ratings And Emp Data 2025 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4b58a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_to_ratings_2025(ratings_df, nat_emp_df, soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and adds year signifiers to the columns.\n",
    "    \n",
    "    Args:\n",
    "        ratings_df (pd.DataFrame): 2025 DataFrame from step 6.1\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "        soc_structure_df (pd.DataFrame): DataFrame containing the SOC structure with major,\n",
    "        minor, and detailed categories for occupations\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame with employment data added to task ratings data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"O_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    ratings_df_trimmed = ratings_df[[\"freq_mean\", \"importance\", \"relevance\", \"O*NET-SOC Code\", \"Title\", \"Task\"]].copy()\n",
    "    ratings_df_trimmed.rename(columns={\"O*NET-SOC Code\": \"soc_code_2019\"}, inplace=True)\n",
    "    ratings_df_trimmed[\"soc_code_2019\"] = ratings_df_trimmed[\"soc_code_2019\"].str[:7]\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    merged = ratings_df_trimmed.merge(\n",
    "        emp_df_trimmed,\n",
    "        on=\"soc_code_2019\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    soc_tmp = soc_structure_df.copy()\n",
    "    soc_tmp[\"Broad Occupation\"] = soc_tmp[\"Broad Occupation\"].replace(\"\", np.nan)\n",
    "\n",
    "    # Forward-fill hierarchy column\n",
    "    soc_tmp[[\"Broad Occupation\"]] = (soc_tmp[[\"Broad Occupation\"]].ffill())\n",
    "\n",
    "    # Drop rows where BOTH detailed columns are NA\n",
    "    soc_tmp = soc_tmp.dropna(subset=[\"Detailed Occupation\", \"Detailed O*NET-SOC\"], how=\"all\")\n",
    "\n",
    "    # Create map of broad occupation to count of unique detailed occupations\n",
    "    broad_counts_map = (\n",
    "        soc_tmp.groupby(\"Broad Occupation\")[\"Detailed Occupation\"]\n",
    "            .nunique()\n",
    "            .astype(\"Int64\")\n",
    "            .to_dict()\n",
    "    )\n",
    "\n",
    "    # Derive broad code from each row's SOC 2010 (e.g., 11-1011.03 -> 11-1010)\n",
    "    merged[\"broad_occ_code\"] = (\n",
    "        merged[\"soc_code_2019\"].astype(str).str[:7].str.replace(r\"(\\d)$\", \"0\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Map counts\n",
    "    merged[\"broad_counts\"] = merged[\"broad_occ_code\"].map(broad_counts_map)\n",
    "\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()].copy()\n",
    "    merged_fallback_1st[\"5_digit_soc\"] = merged_fallback_1st[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    fallback_merge[\"TOT_EMP_fallback\"] = fallback_merge[\"TOT_EMP_fallback\"] / fallback_merge[\"broad_counts\"]\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"Title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"Title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"Title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Formatting and cleanup\n",
    "    merged = merged[merged[\"TOT_EMP\"].notna()].reset_index(drop=True)\n",
    "    merged.rename(columns={\"Task\": \"task\", \"Title\": \"title\", \"TOT_EMP\": \"tot_emp_nat\"}, inplace=True)\n",
    "    merged[\"task_normalized\"] = merged[\"task\"].apply(normalize_text)\n",
    "    cols = [\"task\", \"task_normalized\", \"soc_code_2019\", \"title\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\", \"soc_code_2019\", \"title\"]]\n",
    "    merged = merged[cols]\n",
    "    merged.drop(columns=[\"O_GROUP\",\"broad_occ_code\", \"broad_counts\", \"TOT_EMP_fallback\"], inplace=True)\n",
    "    merged.sort_values(by=\"soc_code_2019\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv(\"../data/ratings_cleaned_2025.csv\")\n",
    "nat_emp_df = pd.read_csv(\"../data/oews_national_2024.csv\")\n",
    "soc_structure_df = pd.read_csv(\"../data/soc_structure_2019.csv\")\n",
    "\n",
    "ratings_economy_nat_2025_df = emp_to_ratings_2025(ratings_df, nat_emp_df, soc_structure_df)\n",
    "\n",
    "if save_files_each_step:\n",
    "    ratings_economy_nat_2025_df.to_csv(\"../merged_data_files/ratings_economy_nat_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f969d0b",
   "metadata": {},
   "source": [
    "### 8.2: National Ratings And Emp Data 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f538267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_to_ratings_2015(ratings_df, nat_emp_df, detailed_occ_2010) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and adds year signifiers to the columns.\n",
    "    \n",
    "    Args:\n",
    "        ratings_df (pd.DataFrame): 2015 DataFrame from step 6.1\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2015.\n",
    "        detailed_occ_2010 (pd.DataFrame): DataFrame with a list of detailed O*NET occupation titles and SOC codes.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame with employment data added to task ratings data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"OCC_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    ratings_df_trimmed = ratings_df[[\"freq_mean\", \"importance\", \"relevance\", \"O*NET-SOC Code\", \"Title\", \"Task\"]].copy()\n",
    "    ratings_df_trimmed.rename(columns={\"O*NET-SOC Code\": \"soc_code_2010\"}, inplace=True)\n",
    "    ratings_df_trimmed[\"soc_code_2010\"] = ratings_df_trimmed[\"soc_code_2010\"].str[:7]\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    merged = ratings_df_trimmed.merge(\n",
    "        emp_df_trimmed,\n",
    "        on=\"soc_code_2010\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Derive 6-digit soc (drop decimals like .03)\n",
    "    detailed_occ_2010[\"soc6\"] = detailed_occ_2010[\"O*NET-SOC 2010 Code\"].str[:7]\n",
    "\n",
    "    # Derive broad code (replace last digit with 0 → e.g. 11-1011 → 11-1010)\n",
    "    detailed_occ_2010[\"broad_code\"] = detailed_occ_2010[\"soc6\"].str.replace(r\"(\\d)$\", \"0\", regex=True)\n",
    "\n",
    "    # Count unique detailed codes per broad\n",
    "    broad_counts_map = (\n",
    "        detailed_occ_2010.groupby(\"broad_code\")[\"soc6\"]\n",
    "            .nunique()\n",
    "            .astype(\"Int64\")\n",
    "            .to_dict()\n",
    "    )\n",
    "\n",
    "    # Derive broad code from each row's SOC 2010 (e.g., 11-1011.03 -> 11-1010)\n",
    "    merged[\"broad_occ_code\"] = (\n",
    "        merged[\"soc_code_2010\"].astype(str).str[:7].str.replace(r\"(\\d)$\", \"0\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Map counts\n",
    "    merged[\"broad_counts\"] = merged[\"broad_occ_code\"].map(broad_counts_map)\n",
    "\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"OCC_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()].copy()\n",
    "    merged_fallback_1st[\"5_digit_soc\"] = merged_fallback_1st[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    fallback_merge[\"TOT_EMP_fallback\"] = fallback_merge[\"TOT_EMP_fallback\"] / fallback_merge[\"broad_counts\"]\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"Title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"Title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"Title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Formatting and cleanup\n",
    "    merged = merged[merged[\"TOT_EMP\"].notna()].reset_index(drop=True)\n",
    "    merged.rename(columns={\"Task\": \"task\", \"Title\": \"title\", \"TOT_EMP\": \"tot_emp_nat\"}, inplace=True)\n",
    "    merged[\"task_normalized\"] = merged[\"task\"].apply(normalize_text)\n",
    "    cols = [\"task\", \"task_normalized\", \"soc_code_2010\", \"title\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\", \"soc_code_2010\", \"title\"]]\n",
    "    merged = merged[cols]\n",
    "    merged.drop(columns=[\"OCC_GROUP\",\"broad_occ_code\", \"broad_counts\", \"TOT_EMP_fallback\"], inplace=True)\n",
    "    merged.sort_values(by=\"soc_code_2010\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv(\"../merged_data_files/ratings_cleaned_2015.csv\")\n",
    "nat_emp_df = pd.read_csv(\"../data/oews_national_2015.csv\")\n",
    "detailed_occ_2010 = pd.read_csv(\"../data/detailed_occ_2010.csv\")\n",
    "\n",
    "ratings_economy_nat_2015_df = emp_to_ratings_2015(ratings_df, nat_emp_df, detailed_occ_2010)\n",
    "\n",
    "if save_files_each_step:\n",
    "    ratings_economy_nat_2015_df.to_csv(\"../merged_data_files/ratings_economy_nat_2015.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464be5c",
   "metadata": {},
   "source": [
    "### 8.3: State Ratings And Emp Data 2025 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2651b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_to_ratings_utah_2025(state_emp_df, ratings_economy_nat_2025_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and adds year signifiers to the columns.\n",
    "    \n",
    "    Args:\n",
    "        state_emp_df (pd.DataFrame): DataFrame of State OEWS data from 2024.\n",
    "        ratings_economy_nat_2025_df (pd.DataFrame): DataFrame from step 8.1\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame with 2024 state employment data added to 2025 task ratings data.\n",
    "    \"\"\"\n",
    "\n",
    "    state_emp_df = state_emp_df[state_emp_df[\"AREA_TITLE\"] == \"Utah\"]\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    merged = ratings_economy_nat_2025_df.merge(\n",
    "        emp_df_trimmed,\n",
    "        on=\"soc_code_2019\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[\n",
    "    (state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"AREA_TITLE\"] == \"Utah\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"tot_emp_nat\"] * utah_share).round())\n",
    "\n",
    "    # Formatting and cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"tot_emp_ut\"}, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_df = pd.read_csv(\"../data/oews_states_2024.csv\")\n",
    "\n",
    "ratings_eco_2025_df = emp_to_ratings_utah_2025(state_emp_df, ratings_economy_nat_2025_df)\n",
    "ratings_eco_2025_df.to_csv(\"../data/ratings_eco_2025.csv\", index=False)\n",
    "\n",
    "if save_files_each_step:\n",
    "    ratings_eco_2025_df.to_csv(\"../merged_data_files/ratings_eco_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c04bdb",
   "metadata": {},
   "source": [
    "### 8.4: State Ratings And Emp Data 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d010fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_to_ratings_utah_2015(state_emp_df, ratings_economy_nat_2015_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and adds year signifiers to the columns.\n",
    "    \n",
    "    Args:\n",
    "        state_emp_df (pd.DataFrame): DataFrame of State OEWS data from 2015.\n",
    "        ratings_economy_nat_2015_df (pd.DataFrame): DataFrame from step 8.2\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame with 2015 state employment data added to 2015 task ratings data.\n",
    "    \"\"\"\n",
    "\n",
    "    state_emp_df = state_emp_df[state_emp_df[\"ST\"] == \"UT\"]\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    merged = ratings_economy_nat_2015_df.merge(\n",
    "        emp_df_trimmed,\n",
    "        on=\"soc_code_2010\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[\n",
    "        (state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"ST\"] == \"UT\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"tot_emp_nat\"] * utah_share).round())\n",
    "\n",
    "    # Formatting and cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"tot_emp_ut\"}, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_df = pd.read_csv(\"../data/oews_states_2015.csv\")\n",
    "\n",
    "ratings_eco_2015_df = emp_to_ratings_utah_2015(state_emp_df, ratings_economy_nat_2015_df)\n",
    "ratings_eco_2015_df.to_csv(\"../data/ratings_eco_2015.csv\", index=False)\n",
    "\n",
    "if save_files_each_step:\n",
    "    ratings_eco_2015_df.to_csv(\"../merged_data_files/ratings_eco_2015.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
