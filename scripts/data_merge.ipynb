{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ab847",
   "metadata": {},
   "source": [
    "## Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe007551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1b0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower().strip()                   # lowercase + trim\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)            # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # collapse multiple spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dac79",
   "metadata": {},
   "source": [
    "## Step 1: Map Anthropic Task %s to O*NET v20.1 Task Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adc1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_to_onet_tasks(pct_df, task_statements_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the tasks and percentage of occurrences from the Anthropic data, and merges it with the tasks statement data. \n",
    "        It normalizes the percents based on a weighted and non weighted approach.\n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_df (pd.DataFrame): DataFrame containing the Anthropic data of percent occurances of every task in their conversation data\n",
    "        task_statements_df (pd.DataFrame): DataFrame containing O*NET tasks and SOC titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with percentage of occurrences added.\n",
    "    \"\"\"\n",
    "\n",
    "    task_statements_df.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"soc_code\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Task ID\": \"task_id\",\n",
    "    \"Task\": \"task\",\n",
    "    \"Task Type\": \"task_type\",\n",
    "    \"Incumbents Responding\": \"n_responding\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Domain Source\": \"domain_source\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Normalize task columns\n",
    "    pct_df[\"task_normalized_temp\"] = pct_df[\"task_name\"].apply(normalize_text)\n",
    "    task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].apply(normalize_text)\n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_df.merge(\n",
    "        task_statements_df,\n",
    "        left_on=\"task_normalized_temp\",\n",
    "        right_on=\"task_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted and normalized percentages\n",
    "    merged[\"n_occurrences\"] = merged.groupby(\"task_normalized\")[\"title\"].transform(\"nunique\")\n",
    "    merged[\"pct_weighted\"] = 100 * merged[\"pct\"] / merged[\"pct\"].sum()\n",
    "    merged[\"pct_normalized\"] = 100 * (merged[\"pct\"] / merged[\"n_occurrences\"]) / (merged[\"pct\"] / merged[\"n_occurrences\"]).sum()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"task_name\", \"task_normalized_temp\", \"pct\"], inplace=True)\n",
    "\n",
    "    # Reorder so `task` is first and `task_normalized` is second\n",
    "    cols = [\"task\", \"task_normalized\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\"]]\n",
    "    merged = merged[cols]\n",
    "    \n",
    "    # Sort by O*NET-SOC Code\n",
    "    merged.sort_values(by=\"soc_code\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "task_statements_df = pd.read_csv(\"../extra_data/task_statements_v20.1.csv\")\n",
    "pct_df = pd.read_csv(\"../original_data/onet_task_mappings.csv\")\n",
    "pct_onet_tasks_df = pct_to_onet_tasks(pct_df, task_statements_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c399688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "#pct_onet_tasks_df.to_csv(\"../merged_data_files/pct_onet_tasks.csv\", index=False)\n",
    "#pct_onet_tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825bff0",
   "metadata": {},
   "source": [
    "## Step 2: Add SOC Major Occupational Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_soc_structure(pct_onet_tasks_df, soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the previous DataFrame and adds major occupational categories to each row based on the soc structure data \n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_onet_tasks_df (pd.DataFrame): DataFrame from previous step containing pcts mapped to task statements and O*NET metadata\n",
    "        soc_structure_df (pd.DataFrame): DataFrame containing the SOC structure with major, minor, and detailed categories for occupations\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with major occupational categories added\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename column\n",
    "    soc_structure_df.rename(columns={\n",
    "    \"SOC or O*NET-SOC 2019 Title\": \"major_occ_category\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Create new df and columns for merging\n",
    "    pct_onet_tasks_df[\"major_group_code\"] = pct_onet_tasks_df[\"soc_code\"].str[:2]\n",
    "    soc_structure_df = soc_structure_df.dropna(subset=['Major Group']).copy()\n",
    "    soc_structure_df[\"major_group_code\"] = soc_structure_df[\"Major Group\"].str[:2]\n",
    "    \n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_onet_tasks_df.merge(\n",
    "        soc_structure_df[['major_group_code', 'major_occ_category']],\n",
    "        on='major_group_code',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"major_group_code\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "soc_structure_df = pd.read_csv(\"../extra_data/soc_structure_2019.csv\")\n",
    "pct_tasks_soc_structure_df = add_soc_structure(pct_onet_tasks_df, soc_structure_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7521495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# pct_tasks_soc_structure_df.to_csv(\"../merged_data_files/pct_tasks_soc_structure.csv\", index=False)\n",
    "# pct_tasks_soc_structure_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f3c3f",
   "metadata": {},
   "source": [
    "## Step 3: Add Wage and Employment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9742d3d",
   "metadata": {},
   "source": [
    "### Step 3.1: Add Updated (2019) SOC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ca5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df of updated SOC codes to merge with up to date wage and employment data\n",
    "\n",
    "def add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles from our main df and their corresponding O*NET-SOC 2019 code (some titles are duplicated as they get split into different SOC codes)\n",
    "    This is so we can merge the wage and employment data separate from our main df and merge all at once. \n",
    "\n",
    "    Args:\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from previous step.\n",
    "        soc_crosswalk_df (pd.DataFrame): DataFrame 2010 and 2019 occupation titles and SOC codes\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'soc_code_2019' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename columns\n",
    "    soc_crosswalk_df = soc_crosswalk_df.rename(\n",
    "        columns={\n",
    "            \"O*NET-SOC 2010 Title\": \"title\",\n",
    "            \"O*NET-SOC 2019 Code\": \"onet_soc_code_2019\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    soc_crosswalk_df['soc_code_2019'] = soc_crosswalk_df['onet_soc_code_2019'].str[:7]\n",
    "\n",
    "    # Get unique titles from rolling DataFrame\n",
    "    titles_df = pct_tasks_soc_structure_df[[\"title\"]].drop_duplicates()\n",
    "\n",
    "    # Merge to attach 2019 SOC codes\n",
    "    merged = titles_df.merge(\n",
    "        soc_crosswalk_df[[\"title\", \"soc_code_2019\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "soc_crosswalk_df = pd.read_csv(\"../extra_data/2010_to_2019_soc_crosswalk.csv\")\n",
    "title_and_2019_soc_df = add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32c6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# title_and_2019_soc_df.to_csv(\"../merged_data_files/title_and_2019_soc.csv\", index=False)\n",
    "# title_and_2019_soc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb78cf",
   "metadata": {},
   "source": [
    "### Step 3.2: Add National Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf237b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_wage(title_and_2019_soc_df, nat_wage_df, scraped_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national annual and hourly median salary. \n",
    "    It also includes a 6 (from previous df) & 5 digit SOC code for use in following merging. \n",
    "\n",
    "    Args:\n",
    "        title_and_2019_soc_df (pd.DataFrame): DataFrame from previous step.\n",
    "        wage_df (pd.DataFrame): DataFrame of OEWS data\n",
    "        scraped_wage_df (pd.DataFrame): DataFrame containing scraped wage data from O*NET's website from Jan 2020 \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national wage data added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = nat_wage_df[[\"OCC_CODE\", \"O_GROUP\", \"H_MEDIAN\", \"A_MEDIAN\"]].copy()\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"H_MEDIAN\", \"A_MEDIAN\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = title_and_2019_soc_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on\n",
    "    merged[\"5_digit_soc\"] = merged[\"soc_code_2019\"].astype(str).str[:6]     \n",
    "    wage_df_trimmed[\"5_digit_soc\"] = wage_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    wage_df_trimmed_fallback_1st = wage_df_trimmed[wage_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        wage_df_trimmed_fallback_1st[[\"5_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from fallback columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "    # Create column to merge on and where annual median is missing\n",
    "    scraped_wage_df[\"title\"] = scraped_wage_df[\"JobName\"]\n",
    "    merged_fallback_2nd = merged[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create 2nd fallback df with scraper wage data\n",
    "    fallback_merge_2nd = merged_fallback_2nd.merge(\n",
    "        scraped_wage_df[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\", how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_2nd_unique_titles = fallback_merge_2nd.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge 2nd fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_2nd_unique_titles[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from scraper median columns and make present value due to inflation\n",
    "    inflation_factor = 1.24\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"MedianSalary\"] * inflation_factor)\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"A_MEDIAN\"].isna() & merged[\"H_MEDIAN\"].notna(), \"A_MEDIAN\"] = (\n",
    "        merged[\"H_MEDIAN\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].notna(), \"H_MEDIAN\"] = (\n",
    "        merged[\"A_MEDIAN\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Create final national wage columns by averaging for any duplicate titles and drop uneeded columns. \n",
    "    merged[\"h_median_national\"] = merged.groupby(\"title\")[\"H_MEDIAN\"].transform(\"mean\")\n",
    "    merged[\"a_median_national\"] = merged.groupby(\"title\")[\"A_MEDIAN\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"H_MEDIAN\", \"A_MEDIAN\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\", \"MedianSalary\", \"O_GROUP\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_wage_df = pd.read_csv(\"../extra_data/wage_and_emp_national.csv\")\n",
    "scraped_wage_df = pd.read_csv(\"../extra_data/scraped_wage_data.csv\")\n",
    "titles_and_nat_wage_df = add_nat_wage(title_and_2019_soc_df, nat_wage_df, scraped_wage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_and_nat_wage_df.to_csv(\"../merged_data_files/titles_and_nat_wage.csv\", index=False)\n",
    "# titles_and_nat_wage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5fa20",
   "metadata": {},
   "source": [
    "### Step 3.3: Add State Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcee71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_wage(titles_and_nat_wage_df, state_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state annual and hourly median salary. \n",
    "\n",
    "    Args:\n",
    "        titles_and_nat_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        wage_df (pd.DataFrame): DataFrame of OEWS data with state level breakdown\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state wage data added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = state_wage_df[[\"OCC_CODE\", \"H_MEDIAN\", \"A_MEDIAN\", \"AREA_TITLE\"]].copy()\n",
    "    wage_df_trimmed = wage_df_trimmed[wage_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\",\n",
    "                                    \"H_MEDIAN\": \"h_median_state\",\n",
    "                                    \"A_MEDIAN\": \"a_median_state\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"h_median_state\", \"a_median_state\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_and_nat_wage_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"a_median_state\"].isna() & merged[\"h_median_state\"].notna(), \"a_median_state\"] = (\n",
    "        merged[\"h_median_state\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"h_median_state\"].isna() & merged[\"a_median_state\"].notna(), \"h_median_state\"] = (\n",
    "        merged[\"a_median_state\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data\n",
    "    merged.loc[merged[\"a_median_state\"].isna(), \"a_median_state\"] = (\n",
    "        merged[\"a_median_national\"]\n",
    "    )\n",
    "    merged.loc[merged[\"h_median_state\"].isna(), \"h_median_state\"] = (\n",
    "        merged[\"h_median_national\"]\n",
    "    )\n",
    "\n",
    "    merged[\"h_median_utah\"] = merged.groupby(\"title\")[\"h_median_state\"].transform(\"mean\")\n",
    "    merged[\"a_median_utah\"] = merged.groupby(\"title\")[\"a_median_state\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"h_median_state\", \"a_median_state\", \"AREA_TITLE\"], inplace=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "state_wage_df = pd.read_csv(\"../extra_data/wage_and_emp_states.csv\")\n",
    "titles_nat_and_state_wage_df = add_state_wage(titles_and_nat_wage_df, state_wage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01aba89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_nat_and_state_wage_df.to_csv(\"../merged_data_files/titles_nat_and_state_wage.csv\", index=False)\n",
    "# titles_nat_and_state_wage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee5942",
   "metadata": {},
   "source": [
    "### 3.4: Add National Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e12cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_emp(titles_nat_and_state_wage_df, nat_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national employment data.  \n",
    "\n",
    "    Args:\n",
    "        titles_nat_and_state_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national employment data added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"O_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_nat_and_state_wage_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurances for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_national\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    # Create percent columns by dividing each emp total by the total for the whole workforce.\n",
    "    total_emp = merged[\"emp_total_national\"].sum()\n",
    "    merged[\"emp_percent_national\"] = (merged[\"emp_total_national\"] / total_emp) * 100\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP_fallback\", \"TOT_EMP\", \"O_GROUP\", \"TOT_EMP_adj\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_emp_df = pd.read_csv(\"../extra_data/wage_and_emp_national.csv\")\n",
    "titles_wage_nat_emp_df = add_nat_emp(titles_nat_and_state_wage_df, nat_emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57b9ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_nat_emp_df.to_csv(\"../merged_data_files/titles_wage_nat_emp.csv\", index=False)\n",
    "# titles_wage_nat_emp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd5aae",
   "metadata": {},
   "source": [
    "### 3.5: Add State Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38846383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_emp(titles_wage_nat_emp_df, state_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state employment data.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_nat_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_emp_df (pd.DataFrame): DataFrame of OEWS data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state employment data added\n",
    "    \"\"\"\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"AREA_TITLE\"]].copy()\n",
    "    emp_df_trimmed = emp_df_trimmed[emp_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_wage_nat_emp_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[\n",
    "    (state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"AREA_TITLE\"] == \"Utah\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"emp_total_national\"] * utah_share).round())\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurances for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_utah\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    # Create percent columns by dividing each emp total by the total for the whole workforce.\n",
    "    total_emp = merged[\"emp_total_utah\"].sum()\n",
    "    merged[\"emp_percent_utah\"] = (merged[\"emp_total_utah\"] / total_emp) * 100\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP\", \"AREA_TITLE\", \"TOT_EMP_adj\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_df = pd.read_csv(\"../extra_data/wage_and_emp_states.csv\")\n",
    "titles_wage_all_emp_df = add_state_emp(titles_wage_nat_emp_df, state_emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8dd7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155229230.0\n"
     ]
    }
   ],
   "source": [
    "state_emp_df = pd.read_csv(\"../extra_data/wage_and_emp_states.csv\")\n",
    "state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "total_utah_emp = state_emp_df[(state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"AREA_TITLE\"] == \"Utah\")]\\\n",
    "\n",
    "print(float(total_nat_emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3b02bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "soc_code_2019",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "5_digit_soc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "h_median_national",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "a_median_national",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "h_median_utah",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "a_median_utah",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emp_total_national",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emp_percent_national",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emp_total_state",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "emp_percent_state",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a9c669ba-51de-444b-8986-4abb9c96624d",
       "rows": [
        [
         "0",
         "Chief Executives",
         "11-1011",
         "11-101",
         "99.24",
         "206420.0",
         "78.84",
         "163980.0",
         "211850.0",
         "0.10030714773062475",
         "3980.0",
         "0.1710699998667545"
        ],
        [
         "1",
         "Chief Sustainability Officers",
         "11-1011",
         "11-101",
         "99.24",
         "206420.0",
         "78.84",
         "163980.0",
         "211850.0",
         "0.10030714773062475",
         "3980.0",
         "0.1710699998667545"
        ],
        [
         "2",
         "General and Operations Managers",
         "11-1021",
         "11-102",
         "49.5",
         "102950.0",
         "43.86",
         "91230.0",
         "3584420.0",
         "1.697158114083578",
         "45910.0",
         "1.9733225361514322"
        ],
        [
         "3",
         "Legislators",
         "11-1031",
         "11-103",
         "21.54326923076923",
         "44810.0",
         "24.326923076923077",
         "50600.0",
         "26510.0",
         "0.012552006071932324",
         "300.0",
         "0.012894723608046825"
        ],
        [
         "4",
         "Advertising and Promotions Managers",
         "11-2011",
         "11-201",
         "61.04",
         "126960.0",
         "52.1",
         "108370.0",
         "21100.0",
         "0.009990468808667372",
         "232.0",
         "0.009971919590222877"
        ],
        [
         "5",
         "Green Marketers",
         "11-2011",
         "11-201",
         "61.04",
         "126960.0",
         "52.1",
         "108370.0",
         "21100.0",
         "0.009990468808667372",
         "232.0",
         "0.009971919590222877"
        ],
        [
         "6",
         "Marketing Managers",
         "11-2021",
         "11-202",
         "77.42",
         "161030.0",
         "65.0",
         "135190.0",
         "384980.0",
         "0.18228107497444382",
         "6560.0",
         "0.2819646228959572"
        ],
        [
         "7",
         "Sales Managers",
         "11-2022",
         "11-202",
         "66.38",
         "138060.0",
         "62.71",
         "130430.0",
         "603710.0",
         "0.2858457784114018",
         "8660.0",
         "0.37222768815228496"
        ],
        [
         "8",
         "Public Relations and Fundraising Managers",
         "11-2032",
         "11-203",
         "62.98",
         "131000.0",
         "50.84",
         "105745.0",
         "56490.0",
         "0.02674699445505307",
         "400.0",
         "0.017192964810729097"
        ],
        [
         "9",
         "Public Relations and Fundraising Managers",
         "11-2033",
         "11-203",
         "62.98",
         "131000.0",
         "50.84",
         "105745.0",
         "56490.0",
         "0.02674699445505307",
         "400.0",
         "0.017192964810729097"
        ],
        [
         "10",
         "Administrative Services Managers",
         "11-3012",
         "11-301",
         "51.22",
         "106540.0",
         "48.1",
         "100055.0",
         "197615.0",
         "0.09356713239927975",
         "1705.0",
         "0.07328501250573279"
        ],
        [
         "11",
         "Administrative Services Managers",
         "11-3013",
         "11-301",
         "51.22",
         "106540.0",
         "48.1",
         "100055.0",
         "197615.0",
         "0.09356713239927975",
         "1705.0",
         "0.07328501250573279"
        ],
        [
         "12",
         "Computer and Information Systems Managers",
         "11-3021",
         "11-302",
         "82.31",
         "171200.0",
         "78.08",
         "162400.0",
         "645970.0",
         "0.3058551249447802",
         "8090.0",
         "0.34772771329699603"
        ],
        [
         "13",
         "Treasurers and Controllers",
         "11-3031",
         "11-303",
         "77.74",
         "161700.0",
         "66.23",
         "137760.0",
         "818620.0",
         "0.3876017808602504",
         "8670.0",
         "0.37265751227255317"
        ],
        [
         "14",
         "Financial Managers, Branch or Department",
         "11-3031",
         "11-303",
         "77.74",
         "161700.0",
         "66.23",
         "137760.0",
         "818620.0",
         "0.3876017808602504",
         "8670.0",
         "0.37265751227255317"
        ],
        [
         "15",
         "Industrial Production Managers",
         "11-3051",
         "11-305",
         "58.39",
         "121440.0",
         "52.1",
         "108370.0",
         "234380.0",
         "0.11097469570499803",
         "2290.0",
         "0.0984297235414241"
        ],
        [
         "16",
         "Quality Control Systems Managers",
         "11-3051",
         "11-305",
         "58.39",
         "121440.0",
         "52.1",
         "108370.0",
         "234380.0",
         "0.11097469570499803",
         "2290.0",
         "0.0984297235414241"
        ],
        [
         "17",
         "Geothermal Production Managers",
         "11-3051",
         "11-305",
         "58.39",
         "121440.0",
         "52.1",
         "108370.0",
         "234380.0",
         "0.11097469570499803",
         "2290.0",
         "0.0984297235414241"
        ],
        [
         "18",
         "Biomass Power Plant Managers",
         "11-3051",
         "11-305",
         "58.39",
         "121440.0",
         "52.1",
         "108370.0",
         "234380.0",
         "0.11097469570499803",
         "2290.0",
         "0.0984297235414241"
        ],
        [
         "19",
         "Purchasing Managers",
         "11-3061",
         "11-306",
         "67.07",
         "139510.0",
         "59.31",
         "123360.0",
         "81240.0",
         "0.03846567232303968",
         "500.0",
         "0.02149120601341137"
        ],
        [
         "20",
         "Transportation Managers",
         "11-3071",
         "11-307",
         "49.05",
         "102010.0",
         "47.11",
         "97980.0",
         "213000.0",
         "0.10085165195479384",
         "2600.0",
         "0.11175427126973914"
        ],
        [
         "21",
         "Storage and Distribution Managers",
         "11-3071",
         "11-307",
         "49.05",
         "102010.0",
         "47.11",
         "97980.0",
         "213000.0",
         "0.10085165195479384",
         "2600.0",
         "0.11175427126973914"
        ],
        [
         "22",
         "Logistics Managers",
         "11-3071",
         "11-307",
         "49.05",
         "102010.0",
         "47.11",
         "97980.0",
         "213000.0",
         "0.10085165195479384",
         "2600.0",
         "0.11175427126973914"
        ],
        [
         "23",
         "Compensation and Benefits Managers",
         "11-3111",
         "11-311",
         "67.48",
         "140360.0",
         "63.91",
         "132920.0",
         "20070.0",
         "0.009502782416585503",
         "170.0",
         "0.007307010044559866"
        ],
        [
         "24",
         "Human Resources Managers",
         "11-3121",
         "11-312",
         "67.32",
         "140030.0",
         "63.02",
         "131070.0",
         "215520.0",
         "0.1020448264286252",
         "2070.0",
         "0.08897359289552308"
        ],
        [
         "25",
         "Training and Development Managers",
         "11-3131",
         "11-313",
         "61.1",
         "127090.0",
         "52.88",
         "109980.0",
         "44960.0",
         "0.02128774775534052",
         "500.0",
         "0.02149120601341137"
        ],
        [
         "26",
         "Nursery and Greenhouse Managers",
         "11-9013",
         "11-901",
         "42.3",
         "87980.0",
         "42.3",
         "87980.0",
         "5910.0",
         "0.002798278230294984",
         "65.0",
         "0.0027938567817434785"
        ],
        [
         "27",
         "Farm and Ranch Managers",
         "11-9013",
         "11-901",
         "42.3",
         "87980.0",
         "42.3",
         "87980.0",
         "5910.0",
         "0.002798278230294984",
         "65.0",
         "0.0027938567817434785"
        ],
        [
         "28",
         "Aquacultural Managers",
         "11-9013",
         "11-901",
         "42.3",
         "87980.0",
         "42.3",
         "87980.0",
         "5910.0",
         "0.002798278230294984",
         "65.0",
         "0.0027938567817434785"
        ],
        [
         "29",
         "Construction Managers",
         "11-9021",
         "11-902",
         "51.43",
         "106980.0",
         "48.03",
         "99900.0",
         "348330.0",
         "0.16492796209114244",
         "4970.0",
         "0.21362258777330906"
        ],
        [
         "30",
         "Education Administrators, Preschool and Childcare Center/Program",
         "11-9031",
         "11-903",
         "27.05",
         "56270.0",
         "28.56",
         "59400.0",
         "71620.0",
         "0.03391077611738185",
         "550.0",
         "0.02364032661475251"
        ],
        [
         "31",
         "Education Administrators, Elementary and Secondary School",
         "11-9032",
         "11-903",
         "48.39",
         "104070.0",
         "59.92788461538461",
         "124650.0",
         "319630.0",
         "0.15133903058361856",
         "2810.0",
         "0.12078057779537192"
        ],
        [
         "32",
         "Education Administrators, Postsecondary",
         "11-9033",
         "11-903",
         "49.98",
         "103960.0",
         "48.13",
         "100110.0",
         "176420.0",
         "0.08353168280687667",
         "2450.0",
         "0.10530690946571573"
        ],
        [
         "33",
         "Distance Learning Coordinators",
         "11-9039",
         "11-903",
         "42.81",
         "89040.0",
         "40.77",
         "84790.0",
         "53330.0",
         "0.025250791543423266",
         "310.0",
         "0.013324547728315051"
        ],
        [
         "34",
         "Fitness and Wellness Coordinators",
         "11-9179",
         "11-917",
         "29.49",
         "61340.0",
         "28.77",
         "59840.0",
         "10490.0",
         "0.004966825488290081",
         "116.0",
         "0.004985959795111438"
        ],
        [
         "35",
         "Architectural and Engineering Managers",
         "11-9041",
         "11-904",
         "80.64",
         "167740.0",
         "72.11",
         "149990.0",
         "210340.0",
         "0.09959219001019405",
         "2480.0",
         "0.10659638182652041"
        ],
        [
         "36",
         "Biofuels/Biodiesel Technology and Product Development Managers",
         "11-9041",
         "11-904",
         "80.64",
         "167740.0",
         "72.11",
         "149990.0",
         "210340.0",
         "0.09959219001019405",
         "2480.0",
         "0.10659638182652041"
        ],
        [
         "37",
         "Food Service Managers",
         "11-9051",
         "11-905",
         "31.4",
         "65310.0",
         "27.62",
         "57450.0",
         "244230.0",
         "0.11563849275548967",
         "2020.0",
         "0.08682447229418194"
        ],
        [
         "38",
         "Funeral Service Managers",
         "11-9171",
         "11-917",
         "36.94",
         "76830.0",
         "42.82",
         "89060.0",
         "13120.0",
         "0.0062120829748680524",
         "170.0",
         "0.007307010044559866"
        ],
        [
         "39",
         "Gaming Managers",
         "11-9071",
         "11-907",
         "41.14",
         "85580.0",
         "41.14",
         "85580.0",
         "4620.0",
         "0.0021874865353575003",
         "51.0",
         "0.00219210301336796"
        ],
        [
         "40",
         "Lodging Managers",
         "11-9081",
         "11-908",
         "32.76",
         "68130.0",
         "30.85",
         "64160.0",
         "41350.0",
         "0.019578477973383687",
         "455.0",
         "0.019556997472204348"
        ],
        [
         "41",
         "Medical and Health Services Managers",
         "11-9111",
         "11-911",
         "56.71",
         "117960.0",
         "51.91",
         "107980.0",
         "565840.0",
         "0.2679150175685472",
         "4980.0",
         "0.21405241189357727"
        ],
        [
         "42",
         "Natural Sciences Managers",
         "11-9121",
         "11-912",
         "77.49",
         "161180.0",
         "56.34",
         "117190.0",
         "100870.0",
         "0.04776012268863875",
         "1150.0",
         "0.04942977383084615"
        ],
        [
         "43",
         "Clinical Research Coordinators",
         "11-9121",
         "11-912",
         "77.49",
         "161180.0",
         "56.34",
         "117190.0",
         "100870.0",
         "0.04776012268863875",
         "1150.0",
         "0.04942977383084615"
        ],
        [
         "44",
         "Water Resource Specialists",
         "11-9121",
         "11-912",
         "77.49",
         "161180.0",
         "56.34",
         "117190.0",
         "100870.0",
         "0.04776012268863875",
         "1150.0",
         "0.04942977383084615"
        ],
        [
         "45",
         "Property, Real Estate, and Community Association Managers",
         "11-9141",
         "11-914",
         "32.07",
         "66700.0",
         "28.42",
         "59100.0",
         "296640.0",
         "0.14045368091957766",
         "3210.0",
         "0.137973542606101"
        ],
        [
         "46",
         "Social and Community Service Managers",
         "11-9151",
         "11-915",
         "37.61",
         "78240.0",
         "37.31",
         "77600.0",
         "195490.0",
         "0.0925609832894021",
         "2020.0",
         "0.08682447229418194"
        ],
        [
         "47",
         "Emergency Management Directors",
         "11-9161",
         "11-916",
         "41.41",
         "86130.0",
         "38.34",
         "79750.0",
         "12570.0",
         "0.005951667911135016",
         "180.0",
         "0.007736834164828094"
        ],
        [
         "48",
         "Regulatory Affairs Managers",
         "11-9199",
         "11-919",
         "65.65",
         "136550.0",
         "65.65",
         "136550.0",
         "630980.0",
         "0.2987576307532198",
         "6090.0",
         "0.2617628892433505"
        ],
        [
         "49",
         "Compliance Managers",
         "11-9199",
         "11-919",
         "65.65",
         "136550.0",
         "65.65",
         "136550.0",
         "630980.0",
         "0.2987576307532198",
         "6090.0",
         "0.2617628892433505"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 782
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>soc_code_2019</th>\n",
       "      <th>5_digit_soc</th>\n",
       "      <th>h_median_national</th>\n",
       "      <th>a_median_national</th>\n",
       "      <th>h_median_utah</th>\n",
       "      <th>a_median_utah</th>\n",
       "      <th>emp_total_national</th>\n",
       "      <th>emp_percent_national</th>\n",
       "      <th>emp_total_state</th>\n",
       "      <th>emp_percent_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>11-1011</td>\n",
       "      <td>11-101</td>\n",
       "      <td>99.240000</td>\n",
       "      <td>206420.0</td>\n",
       "      <td>78.840000</td>\n",
       "      <td>163980.0</td>\n",
       "      <td>211850.0</td>\n",
       "      <td>0.100307</td>\n",
       "      <td>3980.0</td>\n",
       "      <td>0.171070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>11-1011</td>\n",
       "      <td>11-101</td>\n",
       "      <td>99.240000</td>\n",
       "      <td>206420.0</td>\n",
       "      <td>78.840000</td>\n",
       "      <td>163980.0</td>\n",
       "      <td>211850.0</td>\n",
       "      <td>0.100307</td>\n",
       "      <td>3980.0</td>\n",
       "      <td>0.171070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>11-1021</td>\n",
       "      <td>11-102</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>102950.0</td>\n",
       "      <td>43.860000</td>\n",
       "      <td>91230.0</td>\n",
       "      <td>3584420.0</td>\n",
       "      <td>1.697158</td>\n",
       "      <td>45910.0</td>\n",
       "      <td>1.973323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legislators</td>\n",
       "      <td>11-1031</td>\n",
       "      <td>11-103</td>\n",
       "      <td>21.543269</td>\n",
       "      <td>44810.0</td>\n",
       "      <td>24.326923</td>\n",
       "      <td>50600.0</td>\n",
       "      <td>26510.0</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.012895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>11-2011</td>\n",
       "      <td>11-201</td>\n",
       "      <td>61.040000</td>\n",
       "      <td>126960.0</td>\n",
       "      <td>52.100000</td>\n",
       "      <td>108370.0</td>\n",
       "      <td>21100.0</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Loading Machine Operators, Underground Mining</td>\n",
       "      <td>47-5044</td>\n",
       "      <td>47-504</td>\n",
       "      <td>33.110000</td>\n",
       "      <td>68860.0</td>\n",
       "      <td>33.110000</td>\n",
       "      <td>68860.0</td>\n",
       "      <td>6130.0</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Cleaners of Vehicles and Equipment</td>\n",
       "      <td>53-7061</td>\n",
       "      <td>53-706</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>35270.0</td>\n",
       "      <td>16.430000</td>\n",
       "      <td>34170.0</td>\n",
       "      <td>373960.0</td>\n",
       "      <td>0.177063</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>0.191272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Laborers and Freight, Stock, and Material Move...</td>\n",
       "      <td>53-7062</td>\n",
       "      <td>53-706</td>\n",
       "      <td>18.720000</td>\n",
       "      <td>38940.0</td>\n",
       "      <td>19.020000</td>\n",
       "      <td>39570.0</td>\n",
       "      <td>2982530.0</td>\n",
       "      <td>1.412174</td>\n",
       "      <td>29750.0</td>\n",
       "      <td>1.278727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Pump Operators, Except Wellhead Pumpers</td>\n",
       "      <td>53-7072</td>\n",
       "      <td>53-707</td>\n",
       "      <td>28.860000</td>\n",
       "      <td>60020.0</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>73840.0</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.008596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title soc_code_2019  \\\n",
       "0                                     Chief Executives       11-1011   \n",
       "1                        Chief Sustainability Officers       11-1011   \n",
       "2                      General and Operations Managers       11-1021   \n",
       "3                                          Legislators       11-1031   \n",
       "4                  Advertising and Promotions Managers       11-2011   \n",
       "..                                                 ...           ...   \n",
       "777      Loading Machine Operators, Underground Mining       47-5044   \n",
       "778                 Cleaners of Vehicles and Equipment       53-7061   \n",
       "779  Laborers and Freight, Stock, and Material Move...       53-7062   \n",
       "780            Pump Operators, Except Wellhead Pumpers       53-7072   \n",
       "781                                                NaN           NaN   \n",
       "\n",
       "    5_digit_soc  h_median_national  a_median_national  h_median_utah  \\\n",
       "0        11-101          99.240000           206420.0      78.840000   \n",
       "1        11-101          99.240000           206420.0      78.840000   \n",
       "2        11-102          49.500000           102950.0      43.860000   \n",
       "3        11-103          21.543269            44810.0      24.326923   \n",
       "4        11-201          61.040000           126960.0      52.100000   \n",
       "..          ...                ...                ...            ...   \n",
       "777      47-504          33.110000            68860.0      33.110000   \n",
       "778      53-706          16.960000            35270.0      16.430000   \n",
       "779      53-706          18.720000            38940.0      19.020000   \n",
       "780      53-707          28.860000            60020.0      35.500000   \n",
       "781         nan                NaN                NaN            NaN   \n",
       "\n",
       "     a_median_utah  emp_total_national  emp_percent_national  emp_total_state  \\\n",
       "0         163980.0            211850.0              0.100307           3980.0   \n",
       "1         163980.0            211850.0              0.100307           3980.0   \n",
       "2          91230.0           3584420.0              1.697158          45910.0   \n",
       "3          50600.0             26510.0              0.012552            300.0   \n",
       "4         108370.0             21100.0              0.009990            232.0   \n",
       "..             ...                 ...                   ...              ...   \n",
       "777        68860.0              6130.0              0.002902             68.0   \n",
       "778        34170.0            373960.0              0.177063           4450.0   \n",
       "779        39570.0           2982530.0              1.412174          29750.0   \n",
       "780        73840.0             12600.0              0.005966            200.0   \n",
       "781            NaN                 NaN                   NaN              NaN   \n",
       "\n",
       "     emp_percent_state  \n",
       "0             0.171070  \n",
       "1             0.171070  \n",
       "2             1.973323  \n",
       "3             0.012895  \n",
       "4             0.009972  \n",
       "..                 ...  \n",
       "777           0.002923  \n",
       "778           0.191272  \n",
       "779           1.278727  \n",
       "780           0.008596  \n",
       "781                NaN  \n",
       "\n",
       "[782 rows x 11 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_all_emp_df.to_csv(\"../merged_data_files/titles_wage_all_emp.csv\", index=False)\n",
    "titles_wage_all_emp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c465f",
   "metadata": {},
   "source": [
    "## Extra Data (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5af272cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MIGHT NEED THESE REFERENCES FOR STEP 3\n",
    "\n",
    "# # Only fill NaNs from fallback columns\n",
    "# merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "# merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merged[\"2_digit_soc\"] = merged[\"soc_code_2019\"].astype(str).str[:2]     \n",
    "# wage_df_trimmed[\"2_digit_soc\"] = wage_df_trimmed[\"soc_code_2019\"].astype(str).str[:2]\n",
    "# wage_df_trimmed_fallback_2nd = wage_df_trimmed[wage_df_trimmed[\"O_GROUP\"] == \"major\"]\n",
    "# merged_fallback_2nd = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "# fallback_merge_2nd = merged_fallback_2nd.merge(\n",
    "#     wage_df_trimmed_fallback_2nd[[\"2_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "#     on=\"2_digit_soc\", how=\"left\",\n",
    "#     suffixes=(\"\", \"_fallback2nd\")\n",
    "# )\n",
    "\n",
    "# merged = merged.merge(\n",
    "#     fallback_merge_2nd[[\"title\", \"H_MEDIAN_fallback2nd\", \"A_MEDIAN_fallback2nd\"]],\n",
    "#     on=\"title\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# # Only fill NaNs from fallback columns\n",
    "# merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback2nd\"])\n",
    "# merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback2nd\"])\n",
    "\n",
    "\n",
    "\n",
    "# merged[\"h_median_final\"] = merged.groupby(\"title\")[\"H_MEDIAN\"].transform(\"mean\")\n",
    "# merged[\"a_median_final\"] = merged.groupby(\"title\")[\"A_MEDIAN\"].transform(\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merged[\"2_digit_soc\"] = merged[\"soc_code_2019\"].astype(str).str[:2]     \n",
    "# wage_df_trimmed[\"2_digit_soc\"] = wage_df_trimmed[\"soc_code_2019\"].astype(str).str[:2]\n",
    "# wage_df_trimmed_fallback_2nd = wage_df_trimmed[wage_df_trimmed[\"O_GROUP\"] == \"major\"]\n",
    "#scraper_wage_df[\"title\"] = scraper_wage_df[\"JobName\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2940280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def add_emp_wage_data(df) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Description:\n",
    "#         This loads in the employment wage data  and merges it into the given dataframe with the desired columns on the occupation code.\n",
    "#         If a row doesn't match, we will fall back to merging on occupation title. \n",
    "#         All column names in the resulting DataFrame will be lowercase.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input the df with the ONET and Claude data merged.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Merged DataFrame with employment and wage data\n",
    "#     \"\"\"\n",
    "#     emp_wage_df = pd.read_csv(\"../extra_data/emp_wage_national.csv\")\n",
    "\n",
    "#     # Standardize for merges\n",
    "#     df[\"occ_group_code\"] = df[\"occ_group_code\"].str[:7]\n",
    "#     df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "#     emp_wage_df[\"occ_title_normalized\"] = emp_wage_df[\"OCC_TITLE\"].str.lower().str.strip()\n",
    "\n",
    "#     wage_cols = [\n",
    "#             \"OCC_CODE\", \"AREA_TITLE\", \"TOT_EMP\", \"EMP_PRSE\", \"JOBS_1000\",\n",
    "#             \"LOC_QUOTIENT\", \"PCT_TOTAL\", \"PCT_RPT\", \"H_MEAN\", \"A_MEAN\",\n",
    "#             \"MEAN_PRSE\", \"H_PCT10\", \"H_PCT25\", \"H_MEDIAN\", \"H_PCT75\", \"H_PCT90\",\n",
    "#             \"A_PCT10\", \"A_PCT25\", \"A_MEDIAN\", \"A_PCT75\", \"A_PCT90\", \"ANNUAL\", \"HOURLY\", \"occ_title_normalized\"\n",
    "#         ]\n",
    "\n",
    "#     # Perform merge\n",
    "#     merged_df = pd.merge(\n",
    "#         df,\n",
    "#         emp_wage_df[wage_cols],\n",
    "#         left_on=\"occ_group_code\",\n",
    "#         right_on=\"OCC_CODE\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     merged_matched = merged_df[merged_df[\"TOT_EMP\"].notna()]\n",
    "#     unmatched = merged_df[merged_df[\"TOT_EMP\"].isna()]\n",
    "#     unmatched = unmatched.drop(columns=wage_cols, errors=\"ignore\")\n",
    "\n",
    "#     merged_unmatched = pd.merge(\n",
    "#         unmatched,\n",
    "#         emp_wage_df[wage_cols],\n",
    "#         left_on=\"title_normalized\",\n",
    "#         right_on=\"occ_title_normalized\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     final_merged = pd.concat([merged_matched, merged_unmatched], ignore_index=True)\n",
    "#     final_merged.drop(columns=[\"title_normalized\", \"occ_title_normalized\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "#     # Convert all column names to lowercase\n",
    "#     final_merged.columns = [col.lower() for col in final_merged.columns]\n",
    "\n",
    "#     return final_merged\n",
    "\n",
    "# task_emp_wage_df = add_emp_wage_data(task_soc_pct_all)\n",
    "# #display(task_emp_wage_df)\n",
    "# print(\"tot_emp missing:\", task_emp_wage_df[\"tot_emp\"].isna().sum())\n",
    "# print(task_emp_wage_df.loc[task_emp_wage_df[\"tot_emp\"].isna(), \"title\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Task ratings processing\n",
    "\n",
    "# def add_task_ratings():\n",
    "#     \"\"\"\n",
    "#     Description:\n",
    "#         This function reads the task ratings from an Excel file, processes it to extract frequency, importance, and relevance ratings,\n",
    "#         and merges them into a single DataFrame with the desired structure.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input the df with the ONET, Claude, and emp and wage data merged.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Merged DataFrame with task ratings including frequency, importance, and relevance.\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "#     task_ratings_df = pd.read_csv(\"../extra_data/task_ratings.csv\")\n",
    "\n",
    "\n",
    "# # Frequency mapping. Assuming a 52 week year with 5 working days per week, these are corresponding survey questions::\n",
    "# # 1 Once per year or less (Assuming 1 time per year)\n",
    "# # 2 More than once per year (Assuming 3 times per year)\n",
    "# # 3 More than once per month (Assuming 48 times per year, 3 times per month)\n",
    "# # 4 More than once per week (Assuming 130 times per year, 2.5 times per week)\n",
    "# # 5 Daily\n",
    "# # 6 Several times per day (Assuming 3 times per day)\n",
    "# # 7 Hourly or more often (Assuming 12 times per day, 1.5 times per hour)\n",
    "#     frequency_weights = {\n",
    "#         1: 1 / 260,\n",
    "#         2: 3 / 260,\n",
    "#         3: 48 / 260,\n",
    "#         4: 130 / 260,\n",
    "#         5: 1,\n",
    "#         6: 3,\n",
    "#         7: 12\n",
    "#     }\n",
    "\n",
    "\n",
    "#     # Get freq rows, drop unusable ones, generate freq aggregates\n",
    "#     freq_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"FT\"].copy()\n",
    "\n",
    "#     # Drop rows without category or invalid categories\n",
    "#     freq_df = freq_df[pd.to_numeric(freq_df[\"Category\"], errors='coerce').notnull()]\n",
    "#     freq_df[\"Category\"] = freq_df[\"Category\"].astype(int)\n",
    "\n",
    "#     # Apply weights\n",
    "#     freq_df[\"freq_mean\"] = freq_df[\"Data Value\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "#     freq_df[\"freq_lower\"] = freq_df[\"Lower CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "#     freq_df[\"freq_upper\"] = freq_df[\"Upper CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "\n",
    "#     # Sum across categories to get per-task total\n",
    "#     freq_agg = freq_df.groupby([\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"]).agg({\n",
    "#         \"freq_mean\": \"sum\",\n",
    "#         \"freq_lower\": \"sum\",\n",
    "#         \"freq_upper\": \"sum\"\n",
    "#     }).reset_index()\n",
    "\n",
    "\n",
    "#     # Get importance and relevance ratings\n",
    "#     importance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"IM\"].copy()\n",
    "#     importance_df = importance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "#                                 \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "#     importance_df = importance_df.rename(columns={\n",
    "#         \"Data Value\": \"importance\",\n",
    "#         \"Lower CI Bound\": \"importance_lower\",\n",
    "#         \"Upper CI Bound\": \"importance_upper\"\n",
    "#     })\n",
    "\n",
    "#     relevance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"RT\"].copy()\n",
    "#     relevance_df = relevance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "#                                 \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "#     relevance_df = relevance_df.rename(columns={\n",
    "#         \"Data Value\": \"relevance\",\n",
    "#         \"Lower CI Bound\": \"relevance_lower\",\n",
    "#         \"Upper CI Bound\": \"relevance_upper\"\n",
    "#     })\n",
    "\n",
    "\n",
    "#     # Merge ratings\n",
    "#     merged_ratings = freq_agg.merge(importance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "#     merged_ratings = merged_ratings.merge(relevance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "\n",
    "\n",
    "#     merged_ratings[\"task_normalized\"] = merged_ratings[\"Task\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "#     return merged_ratings\n",
    "\n",
    "# ratings_df = add_task_ratings()\n",
    "# #display(ratings_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "669e5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Merge all and final cleanup\n",
    "\n",
    "# def batch_lemmatize(texts):\n",
    "#     \"\"\"\n",
    "#     Efficiently lemmatize a list of strings using spaCy's nlp.pipe().\n",
    "#     Skips punctuation, whitespace, and possessives.\n",
    "#     \"\"\"\n",
    "#     if not texts:\n",
    "#         return []\n",
    "    \n",
    "#     # Handle empty/null strings\n",
    "#     processed_texts = [str(text).strip() if text and str(text).strip() else \" \" for text in texts]\n",
    "    \n",
    "#     cleaned = []\n",
    "#     try:\n",
    "#         for doc in nlp.pipe(processed_texts, batch_size=1000, disable=[\"ner\", \"parser\"]):\n",
    "#             lemmas = [\n",
    "#                 token.lemma_ for token in doc\n",
    "#                 if not token.is_punct and not token.is_space and token.text != \"'s\"\n",
    "#             ]\n",
    "#             result = \" \".join(lemmas).strip()\n",
    "#             cleaned.append(result if result else \"\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in batch_lemmatize: {e}\")\n",
    "#         raise\n",
    "    \n",
    "#     return cleaned\n",
    "\n",
    "# def merge_all_and_cleanup(df, ratings_df):\n",
    "#     \"\"\"\n",
    "#     Description:\n",
    "#         This function merges the task data with the ratings data and performs final cleanup.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): DataFrame containing task data.\n",
    "#         ratings_df (pd.DataFrame): DataFrame containing task ratings.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Final merged DataFrame with all necessary information.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Normalize task names\n",
    "\n",
    "#     # Apply batch lemmatization\n",
    "#     df[\"task_normalized\"] = batch_lemmatize(df[\"task\"].tolist())\n",
    "#     ratings_df[\"task_normalized\"] = batch_lemmatize(ratings_df[\"Task\"].tolist())\n",
    "\n",
    "#     df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "#     ratings_df[\"title_normalized\"] = ratings_df[\"Title\"].str.lower().str.strip()\n",
    "\n",
    "#     # Count how many times each normalized task appears\n",
    "#     task_counts = df[\"task_normalized\"].value_counts()\n",
    "\n",
    "#     # Boolean mask for duplicate vs. unique tasks\n",
    "#     is_duplicate = df[\"task_normalized\"].isin(task_counts[task_counts > 1].index)\n",
    "#     is_unique = ~is_duplicate\n",
    "\n",
    "#     # Split the dataframe\n",
    "#     df_duplicate_tasks = df[is_duplicate].copy()\n",
    "#     df_unique_tasks = df[is_unique].copy()\n",
    "\n",
    "#     # Count how many times each normalized task appears\n",
    "#     task_counts_ratings = ratings_df[\"task_normalized\"].value_counts()\n",
    "\n",
    "#     # Boolean mask for duplicate vs. unique tasks\n",
    "#     is_duplicate_ratings = ratings_df[\"task_normalized\"].isin(task_counts_ratings[task_counts_ratings > 1].index)\n",
    "#     is_unique_ratings = ~is_duplicate_ratings\n",
    "\n",
    "#     # Split the dataframe\n",
    "#     df_duplicate_tasks_ratings = ratings_df[is_duplicate_ratings].copy()\n",
    "#     df_unique_tasks_ratings = ratings_df[is_unique_ratings].copy()\n",
    "\n",
    "#     # Merge on unique tasks\n",
    "#     merged_unique = df_unique_tasks.merge(\n",
    "#         df_unique_tasks_ratings[[\n",
    "#             \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "#             \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "#             \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "#             \"task_normalized\", \"title_normalized\"\n",
    "#         ]],\n",
    "#         on=[\"task_normalized\"],\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Merge on both title and task\n",
    "#     merged_duplicate = df_duplicate_tasks.merge(\n",
    "#         df_duplicate_tasks_ratings[[\n",
    "#             \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "#             \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "#             \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "#             \"task_normalized\", \"title_normalized\"\n",
    "#         ]],\n",
    "#         on=[\"task_normalized\", \"title_normalized\"],\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     merged = pd.concat([merged_unique, merged_duplicate], ignore_index=True)\n",
    "\n",
    "#     # Replace placeholders with NaN\n",
    "#     placeholder_values = [\"#\", \"*\", \"\", \"n/a\", \"na\", \"--\"]\n",
    "#     merged.replace(placeholder_values, pd.NA, inplace=True)\n",
    "\n",
    "#     # Drop fully empty columns\n",
    "#     merged.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "#     # Drop 'occ_code' and 'task_name'\n",
    "#     merged.drop(columns=[\"occ_code\", \"task_name\", \"title_normalized\", \"title_normalized_x\", \"title_normalized_y\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "#     # Reorder columns: make 'task' and 'task_normalized' first\n",
    "#     cols = merged.columns.tolist()\n",
    "#     for col in [\"task_normalized\", \"task\"]:\n",
    "#         if col in cols:\n",
    "#             cols.insert(0, cols.pop(cols.index(col)))\n",
    "#     merged = merged[cols]\n",
    "\n",
    "#     return merged\n",
    "\n",
    "# task_final = merge_all_and_cleanup(task_emp_wage_df, ratings_df)\n",
    "# task_final.to_csv(\"../new_data/tasks_final.csv\", index=False)\n",
    "# #display(task_final.reset_index(drop=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
