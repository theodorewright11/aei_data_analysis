{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna want to have a block of code at the top that you can change variables, like the inflation number. Step 4.1 has one, and step 3.2\n",
    "# We will also want to have a place to make folder for the merged data files\n",
    "# Make sure to have a definite idea of what broad cateogory is\n",
    "# We will want to remove the nlp imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ab847",
   "metadata": {},
   "source": [
    "## Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe007551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "# import spacy\n",
    "# from spacy.cli import download\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1b0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower().strip()                   # lowercase + trim\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)            # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # collapse multiple spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7327ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters\n",
    "\n",
    "# Frequency mapping. Assuming a 52 week year with 5 working days per week, these are corresponding survey questions::\n",
    "# 1 Once per year or less (Assuming 1 time per year)\n",
    "# 2 More than once per year (Assuming 3 times per year)\n",
    "# 3 More than once per month (Assuming 48 times per year, 3 times per month)\n",
    "# 4 More than once per week (Assuming 130 times per year, 2.5 times per week)\n",
    "# 5 Daily\n",
    "# 6 Several times per day (Assuming 3 times per day)\n",
    "# 7 Hourly or more often (Assuming 12 times per day, 1.5 times per hour)\n",
    "frequency_weights = {\n",
    "    1: 1 / 260,\n",
    "    2: 3 / 260,\n",
    "    3: 48 / 260,\n",
    "    4: 130 / 260,\n",
    "    5: 1,\n",
    "    6: 3,\n",
    "    7: 12\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dac79",
   "metadata": {},
   "source": [
    "## Step 1: Map Anthropic Task %s to O*NET v20.1 Task Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adc1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_to_onet_tasks(pct_df, task_statements_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the tasks and percentage of occurrences from the Anthropic data, and merges it with the tasks statement data. \n",
    "        It normalizes the percents based on a weighted and non weighted approach.\n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_df (pd.DataFrame): DataFrame containing the Anthropic data of percent occurances of every task in their conversation data\n",
    "        task_statements_df (pd.DataFrame): DataFrame containing O*NET tasks and SOC titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with percentage of occurrences added.\n",
    "    \"\"\"\n",
    "\n",
    "    task_statements_df.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"soc_code_2010\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Task ID\": \"task_id\",\n",
    "    \"Task\": \"task\",\n",
    "    \"Task Type\": \"task_type\",\n",
    "    \"Incumbents Responding\": \"n_responding\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Domain Source\": \"domain_source\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Normalize task columns\n",
    "    pct_df[\"task_normalized_temp\"] = pct_df[\"task_name\"].apply(normalize_text)\n",
    "    task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].apply(normalize_text)\n",
    "    # task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].str.lower().str.strip()\n",
    "\n",
    "    pct_df = pct_df.groupby(\"task_normalized_temp\", as_index=False).agg({\n",
    "    \"task_name\": \"first\",  # Keep the first task name\n",
    "    \"pct\": \"sum\"  # Sum the percentages for duplicates\n",
    "    })\n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_df.merge(\n",
    "        task_statements_df,\n",
    "        left_on=\"task_normalized_temp\",\n",
    "        right_on=\"task_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted and normalized percentages\n",
    "    merged[\"n_occurrences\"] = merged.groupby(\"task_normalized\")[\"title\"].transform(\"nunique\")\n",
    "    merged[\"pct_weighted\"] = 100 * merged[\"pct\"] / merged[\"pct\"].sum()\n",
    "    merged[\"pct_normalized\"] = 100 * (merged[\"pct\"] / merged[\"n_occurrences\"]) / (merged[\"pct\"] / merged[\"n_occurrences\"]).sum()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"task_name\", \"task_normalized_temp\", \"pct\"], inplace=True)\n",
    "\n",
    "    # Reorder so `task` is first and `task_normalized` is second\n",
    "    cols = [\"task\", \"task_normalized\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\"]]\n",
    "    merged = merged[cols]\n",
    "    \n",
    "    # Sort by O*NET-SOC Code\n",
    "    merged.sort_values(by=\"soc_code_2010\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "task_statements_df = pd.read_csv(\"../extra_data/task_statements_v20.1.csv\")\n",
    "pct_df = pd.read_csv(\"../original_data/onet_task_mappings.csv\")\n",
    "pct_onet_tasks_df = pct_to_onet_tasks(pct_df, task_statements_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c399688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "task",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_normalized",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "soc_code_2010",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "task_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_responding",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "domain_source",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_occurrences",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_weighted",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_normalized",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "major_group_code",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "19842b98-0d48-4115-9194-b45cf50f7584",
       "rows": [
        [
         "0",
         "Interpret and explain policies, rules, regulations, or laws to organizations, government or corporate officials, or individuals.",
         "interpret and explain policies rules regulations or laws to organizations government or corporate officials or individuals",
         "11-1011.00",
         "Chief Executives",
         "8843.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.040047166662958614",
         "0.04948848803504521",
         "11"
        ],
        [
         "1",
         "Confer with board members, organization officials, or staff members to discuss issues, coordinate activities, or resolve problems.",
         "confer with board members organization officials or staff members to discuss issues coordinate activities or resolve problems",
         "11-1011.00",
         "Chief Executives",
         "8824.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.017692795853740926",
         "0.021863961645112506",
         "11"
        ],
        [
         "2",
         "Review reports submitted by staff members to recommend approval or to suggest changes.",
         "review reports submitted by staff members to recommend approval or to suggest changes",
         "11-1011.00",
         "Chief Executives",
         "8830.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.0019070079363313552",
         "0.002356594668335477",
         "11"
        ],
        [
         "3",
         "Serve as liaisons between organizations, shareholders, and outside organizations.",
         "serve as liaisons between organizations shareholders and outside organizations",
         "11-1011.00",
         "Chief Executives",
         "8840.0",
         "Supplemental",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.003072401675200485",
         "0.0037967358545404513",
         "11"
        ],
        [
         "4",
         "Direct, plan, or implement policies, objectives, or activities of organizations or businesses to ensure continuing operations, to maximize returns on investments, or to increase productivity.",
         "direct plan or implement policies objectives or activities of organizations or businesses to ensure continuing operations to maximize returns on investments or to increase productivity",
         "11-1011.00",
         "Chief Executives",
         "8826.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.004237795414069695",
         "0.005236877040745526",
         "11"
        ],
        [
         "5",
         "Direct or coordinate an organization's financial or budget activities to fund operations, maximize investments, or increase efficiency.",
         "direct or coordinate an organizations financial or budget activities to fund operations maximize investments or increase efficiency",
         "11-1011.00",
         "Chief Executives",
         "8823.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.011442047617988131",
         "0.014139568010012862",
         "11"
        ],
        [
         "6",
         "Direct or coordinate activities of businesses involved with buying or selling investment products or financial services.",
         "direct or coordinate activities of businesses involved with buying or selling investment products or financial services",
         "11-1011.00",
         "Chief Executives",
         "8847.0",
         "Supplemental",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.011442047617988131",
         "0.014139568010012862",
         "11"
        ],
        [
         "7",
         "Prepare or present reports concerning activities, expenses, budgets, government statutes or rulings, or other items affecting businesses or program services.",
         "prepare or present reports concerning activities expenses budgets government statutes or rulings or other items affecting businesses or program services",
         "11-1011.00",
         "Chief Executives",
         "8834.0",
         "Core",
         "86.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.0038140158726627103",
         "0.004713189336670954",
         "11"
        ],
        [
         "8",
         "Direct or conduct studies or research on issues affecting areas of responsibility.",
         "direct or conduct studies or research on issues affecting areas of responsibility",
         "11-1011.00",
         "Chief Executives",
         "8848.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.004025905643366203",
         "0.004975033188708239",
         "11"
        ],
        [
         "9",
         "Analyze operations to evaluate performance of a company or its staff in meeting objectives or to determine areas of potential cost reduction, program improvement, or policy change.",
         "analyze operations to evaluate performance of a company or its staff in meeting objectives or to determine areas of potential cost reduction program improvement or policy change",
         "11-1011.00",
         "Chief Executives",
         "8825.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.19610398278607533",
         "0.2423364850604994",
         "11"
        ],
        [
         "10",
         "Review and analyze legislation, laws, or public policy and recommend changes to promote or support interests of the general population or special groups.",
         "review and analyze legislation laws or public policy and recommend changes to promote or support interests of the general population or special groups",
         "11-1011.00",
         "Chief Executives",
         "20461.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.011018268076581144",
         "0.013615880305938287",
         "11"
        ],
        [
         "11",
         "Deliver speeches, write articles, or present information at meetings or conventions to promote services, exchange ideas, or accomplish objectives.",
         "deliver speeches write articles or present information at meetings or conventions to promote services exchange ideas or accomplish objectives",
         "11-1011.00",
         "Chief Executives",
         "8839.0",
         "Core",
         "87.0",
         "07/2014",
         "Incumbent",
         "1.0",
         "0.006568582891807955",
         "0.008117159413155474",
         "11"
        ],
        [
         "12",
         "Research environmental sustainability issues, concerns, or stakeholder interests.",
         "research environmental sustainability issues concerns or stakeholder interests",
         "11-1011.03",
         "Chief Sustainability Officers",
         "15376.0",
         "Core",
         "26.0",
         "07/2013",
         "Occupational Expert",
         "1.0",
         "0.002224842592386513",
         "0.002749360446391306",
         "11"
        ],
        [
         "13",
         "Formulate or implement sustainability campaign or marketing strategies.",
         "formulate or implement sustainability campaign or marketing strategies",
         "11-1011.03",
         "Chief Sustainability Officers",
         "15375.0",
         "Core",
         "26.0",
         "07/2013",
         "Occupational Expert",
         "1.0",
         "0.003072401675200485",
         "0.0037967358545404513",
         "11"
        ],
        [
         "14",
         "Review financial statements, sales or activity reports, or other performance data to measure productivity or goal achievement or to identify areas needing cost reduction or program improvement.",
         "review financial statements sales or activity reports or other performance data to measure productivity or goal achievement or to identify areas needing cost reduction or program improvement",
         "11-1021.00",
         "General and Operations Managers",
         "20699.0",
         "Core",
         "157.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.08772236507124274",
         "0.10840335474343243",
         "11"
        ],
        [
         "15",
         "Plan store layouts or design displays.",
         "plan store layouts or design displays",
         "11-1021.00",
         "General and Operations Managers",
         "949.0",
         "Supplemental",
         "158.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.003708070987310964",
         "0.004582267410652311",
         "11"
        ],
        [
         "16",
         "Set prices or credit terms for goods or services based on forecasts of customer demand.",
         "set prices or credit terms for goods or services based on forecasts of customer demand",
         "11-1021.00",
         "General and Operations Managers",
         "20707.0",
         "Supplemental",
         "158.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.007098307318566687",
         "0.00877176904324869",
         "11"
        ],
        [
         "17",
         "Develop or implement product-marketing strategies, including advertising campaigns or sales promotions.",
         "develop or implement productmarketing strategies including advertising campaigns or sales promotions",
         "11-1021.00",
         "General and Operations Managers",
         "945.0",
         "Supplemental",
         "158.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.0025426772484417525",
         "0.0031421262244472354",
         "11"
        ],
        [
         "18",
         "Evaluate the structure, efficiency, activities, and performance of government agencies.",
         "evaluate the structure efficiency activities and performance of government agencies",
         "11-1031.00",
         "Legislators",
         "15291.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.0028605119044969917",
         "0.0035348920025031648",
         "11"
        ],
        [
         "19",
         "Prepare drafts of amendments, government policies, laws, rules, regulations, budgets, programs and procedures.",
         "prepare drafts of amendments government policies laws rules regulations budgets programs and procedures",
         "11-1031.00",
         "Legislators",
         "15277.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.005085354496883587",
         "0.006284252448894571",
         "11"
        ],
        [
         "20",
         "Represent their parties in negotiations with political executives or members of other parties, and when speaking with the media.",
         "represent their parties in negotiations with political executives or members of other parties and when speaking with the media",
         "11-1031.00",
         "Legislators",
         "15279.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.003708070987310964",
         "0.004582267410652311",
         "11"
        ],
        [
         "21",
         "Determine campaign strategies for media advertising, positions on issues, and public appearances.",
         "determine campaign strategies for media advertising positions on issues and public appearances",
         "11-1031.00",
         "Legislators",
         "15288.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.00529724426758708",
         "0.006546096300931858",
         "11"
        ],
        [
         "22",
         "Represent their government at local, national, and international meetings and conferences.",
         "represent their government at local national and international meetings and conferences",
         "11-1031.00",
         "Legislators",
         "15295.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.00413185052871795",
         "0.005105955114726883",
         "11"
        ],
        [
         "23",
         "Confer with colleagues to formulate positions and strategies pertaining to pending issues.",
         "confer with colleagues to formulate positions and strategies pertaining to pending issues",
         "11-1031.00",
         "Legislators",
         "15269.0",
         null,
         null,
         "06/2006",
         "Analyst",
         "1.0",
         "0.0044496851847731075",
         "0.005498720892782712",
         "11"
        ],
        [
         "24",
         "Contact organizations to explain services and facilities offered.",
         "contact organizations to explain services and facilities offered",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3243.0",
         "Supplemental",
         "61.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.00413185052871795",
         "0.005105955114726883",
         "11"
        ],
        [
         "25",
         "Confer with department heads or staff to discuss topics such as contracts, selection of advertising media, or product to be advertised.",
         "confer with department heads or staff to discuss topics such as contracts selection of advertising media or product to be advertised",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3231.0",
         "Core",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0016951181656278624",
         "0.0020947508162981908",
         "11"
        ],
        [
         "26",
         "Plan and execute advertising policies and strategies for organizations.",
         "plan and execute advertising policies and strategies for organizations",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3238.0",
         "Core",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.007628031745325421",
         "0.009426378673341907",
         "11"
        ],
        [
         "27",
         "Assemble and communicate with a strong, diverse coalition of organizations or public figures, securing their cooperation, support and action, to further campaign goals.",
         "assemble and communicate with a strong diverse coalition of organizations or public figures securing their cooperation support and action to further campaign goals",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3240.0",
         "Supplemental",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0016951181656278624",
         "0.0020947508162981908",
         "11"
        ],
        [
         "28",
         "Monitor and analyze sales promotion results to determine cost effectiveness of promotion campaigns.",
         "monitor and analyze sales promotion results to determine cost effectiveness of promotion campaigns",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3233.0",
         "Supplemental",
         "61.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.046933584210821895",
         "0.05799841322625672",
         "11"
        ],
        [
         "29",
         "Plan and prepare advertising and promotional material to increase sales of products or services, working with customers, company officials, sales departments, and advertising agencies.",
         "plan and prepare advertising and promotional material to increase sales of products or services working with customers company officials sales departments and advertising agencies",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3224.0",
         "Core",
         "61.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0674868919690599",
         "0.0833972668738725",
         "11"
        ],
        [
         "30",
         "Confer with clients to provide marketing or technical advice.",
         "confer with clients to provide marketing or technical advice",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3232.0",
         "Core",
         "61.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0031783465605522312",
         "0.003927657780559094",
         "11"
        ],
        [
         "31",
         "Inspect layouts and advertising copy and edit scripts, audio and video tapes, and other promotional material for adherence to specifications.",
         "inspect layouts and advertising copy and edit scripts audio and video tapes and other promotional material for adherence to specifications",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3226.0",
         "Core",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0082637010574359",
         "0.010211910229453766",
         "11"
        ],
        [
         "32",
         "Read trade journals and professional literature to stay informed on trends, innovations, and changes that affect media planning.",
         "read trade journals and professional literature to stay informed on trends innovations and changes that affect media planning",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3234.0",
         "Core",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.0020129528216831015",
         "0.0024875165943541197",
         "11"
        ],
        [
         "33",
         "Gather and organize information to plan advertising campaigns.",
         "gather and organize information to plan advertising campaigns",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3230.0",
         "Core",
         "62.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.009323149910953281",
         "0.011521129489640096",
         "11"
        ],
        [
         "34",
         "Track program budgets and expenses and campaign response rates to evaluate each campaign based on program objectives and industry norms.",
         "track program budgets and expenses and campaign response rates to evaluate each campaign based on program objectives and industry norms",
         "11-2011.00",
         "Advertising and Promotions Managers",
         "3239.0",
         "Supplemental",
         "61.0",
         "06/2010",
         "Incumbent",
         "1.0",
         "0.005085354496883587",
         "0.006284252448894571",
         "11"
        ],
        [
         "35",
         "Analyze regional energy markets, including energy pricing, market structures, energy generation competition, and energy transmission constraints.",
         "analyze regional energy markets including energy pricing market structures energy generation competition and energy transmission constraints",
         "11-2011.01",
         "Green Marketers",
         "15401.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.007204252203918434",
         "0.008902690969267334",
         "11"
        ],
        [
         "36",
         "Write marketing content for green product web sites, brochures, or other communication media.",
         "write marketing content for green product web sites brochures or other communication media",
         "11-2011.01",
         "Green Marketers",
         "19935.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.002224842592386513",
         "0.002749360446391306",
         "11"
        ],
        [
         "37",
         "Devise or evaluate methods and procedures for collecting data, such as surveys, opinion polls, and questionnaires.",
         "devise or evaluate methods and procedures for collecting data such as surveys opinion polls and questionnaires",
         "11-2011.01",
         "Green Marketers",
         "15389.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.016739291885575294",
         "0.02068566431094482",
         "11"
        ],
        [
         "38",
         "Develop communications materials, advertisements, presentations, or public relations initiatives to promote awareness of green products and technologies.",
         "develop communications materials advertisements presentations or public relations initiatives to promote awareness of green products and technologies",
         "11-2011.01",
         "Green Marketers",
         "19938.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.025532717369769923",
         "0.031552184170491805",
         "11"
        ],
        [
         "39",
         "Generate or identify sales leads for green products and technologies.",
         "generate or identify sales leads for green products and technologies",
         "11-2011.01",
         "Green Marketers",
         "19934.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.0018010630509796084",
         "0.0022256727423168336",
         "11"
        ],
        [
         "40",
         "Coordinate with other marketing team members and workers such as graphic artists to develop and implement marketing programs.",
         "coordinate with other marketing team members and workers such as graphic artists to develop and implement marketing programs",
         "11-2011.01",
         "Green Marketers",
         "19944.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.0027545670191452456",
         "0.003403970076484522",
         "11"
        ],
        [
         "41",
         "Revise existing marketing plans or campaigns for green products, technologies, or services.",
         "revise existing marketing plans or campaigns for green products technologies or services",
         "11-2011.01",
         "Green Marketers",
         "19936.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.005615078923642318",
         "0.006938862078987786",
         "11"
        ],
        [
         "42",
         "Analyze the effectiveness of marketing tactics or channels.",
         "analyze the effectiveness of marketing tactics or channels",
         "11-2011.01",
         "Green Marketers",
         "15400.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.02320192989203158",
         "0.02867190179808175",
         "11"
        ],
        [
         "43",
         "Analyze green product marketing or sales trends to forecast future conditions.",
         "analyze green product marketing or sales trends to forecast future conditions",
         "11-2011.01",
         "Green Marketers",
         "19941.0",
         null,
         null,
         "07/2011",
         "Analyst",
         "1.0",
         "0.0020129528216831015",
         "0.0024875165943541197",
         "11"
        ],
        [
         "44",
         "Identify, develop, or evaluate marketing strategy, based on knowledge of establishment objectives, market characteristics, and cost and markup factors.",
         "identify develop or evaluate marketing strategy based on knowledge of establishment objectives market characteristics and cost and markup factors",
         "11-2021.00",
         "Marketing Managers",
         "951.0",
         "Core",
         "117.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.006780472662511449",
         "0.008379003265192763",
         "11"
        ],
        [
         "45",
         "Evaluate the financial aspects of product development, such as budgets, expenditures, research and development appropriations, or return-on-investment and profit-loss projections.",
         "evaluate the financial aspects of product development such as budgets expenditures research and development appropriations or returnoninvestment and profitloss projections",
         "11-2021.00",
         "Marketing Managers",
         "952.0",
         "Core",
         "116.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.004237795414069695",
         "0.005236877040745526",
         "11"
        ],
        [
         "46",
         "Formulate, direct, or coordinate marketing activities or policies to promote products or services, working with advertising or promotion managers.",
         "formulate direct or coordinate marketing activities or policies to promote products or services working with advertising or promotion managers",
         "11-2021.00",
         "Marketing Managers",
         "20709.0",
         "Core",
         "117.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.059223190911623994",
         "0.07318535664441872",
         "11"
        ],
        [
         "47",
         "Advise business or other groups on local, national, or international factors affecting the buying or selling of products or services.",
         "advise business or other groups on local national or international factors affecting the buying or selling of products or services",
         "11-2021.00",
         "Marketing Managers",
         "962.0",
         "Supplemental",
         "113.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.012289606700802102",
         "0.015186943418162006",
         "11"
        ],
        [
         "48",
         "Initiate market research studies or analyze their findings.",
         "initiate market research studies or analyze their findings",
         "11-2021.00",
         "Marketing Managers",
         "963.0",
         "Core",
         "116.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.0028605119044969917",
         "0.0035348920025031648",
         "11"
        ],
        [
         "49",
         "Confer with legal staff to resolve problems, such as copyright infringement or royalty sharing with outside producers or distributors.",
         "confer with legal staff to resolve problems such as copyright infringement or royalty sharing with outside producers or distributors",
         "11-2021.00",
         "Marketing Managers",
         "960.0",
         "Core",
         "115.0",
         "07/2015",
         "Incumbent",
         "1.0",
         "0.00529724426758708",
         "0.006546096300931858",
         "11"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 4251
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>task_normalized</th>\n",
       "      <th>soc_code_2010</th>\n",
       "      <th>title</th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_type</th>\n",
       "      <th>n_responding</th>\n",
       "      <th>date</th>\n",
       "      <th>domain_source</th>\n",
       "      <th>n_occurrences</th>\n",
       "      <th>pct_weighted</th>\n",
       "      <th>pct_normalized</th>\n",
       "      <th>major_group_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interpret and explain policies, rules, regulat...</td>\n",
       "      <td>interpret and explain policies rules regulatio...</td>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>8843.0</td>\n",
       "      <td>Core</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2014</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040047</td>\n",
       "      <td>0.049488</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confer with board members, organization offici...</td>\n",
       "      <td>confer with board members organization officia...</td>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>8824.0</td>\n",
       "      <td>Core</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2014</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>0.021864</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review reports submitted by staff members to r...</td>\n",
       "      <td>review reports submitted by staff members to r...</td>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>8830.0</td>\n",
       "      <td>Core</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2014</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Serve as liaisons between organizations, share...</td>\n",
       "      <td>serve as liaisons between organizations shareh...</td>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>8840.0</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2014</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Direct, plan, or implement policies, objective...</td>\n",
       "      <td>direct plan or implement policies objectives o...</td>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>8826.0</td>\n",
       "      <td>Core</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2014</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>Stop gathering arms when cars are full.</td>\n",
       "      <td>stop gathering arms when cars are full</td>\n",
       "      <td>53-7033.00</td>\n",
       "      <td>Loading Machine Operators, Underground Mining</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>78.0</td>\n",
       "      <td>06/2008</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>Collect and test samples of cleaning solutions...</td>\n",
       "      <td>collect and test samples of cleaning solutions...</td>\n",
       "      <td>53-7061.00</td>\n",
       "      <td>Cleaners of Vehicles and Equipment</td>\n",
       "      <td>5010.0</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2013</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>Stack cargo in locations such as transit sheds...</td>\n",
       "      <td>stack cargo in locations such as transit sheds...</td>\n",
       "      <td>53-7062.00</td>\n",
       "      <td>Laborers and Freight, Stock, and Material Move...</td>\n",
       "      <td>10795.0</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>87.0</td>\n",
       "      <td>07/2013</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>Test materials and solutions, using testing eq...</td>\n",
       "      <td>test materials and solutions using testing equ...</td>\n",
       "      <td>53-7072.00</td>\n",
       "      <td>Pump Operators, Except Wellhead Pumpers</td>\n",
       "      <td>14622.0</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>105.0</td>\n",
       "      <td>06/2007</td>\n",
       "      <td>Incumbent</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.391572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4251 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   task  \\\n",
       "0     Interpret and explain policies, rules, regulat...   \n",
       "1     Confer with board members, organization offici...   \n",
       "2     Review reports submitted by staff members to r...   \n",
       "3     Serve as liaisons between organizations, share...   \n",
       "4     Direct, plan, or implement policies, objective...   \n",
       "...                                                 ...   \n",
       "4246            Stop gathering arms when cars are full.   \n",
       "4247  Collect and test samples of cleaning solutions...   \n",
       "4248  Stack cargo in locations such as transit sheds...   \n",
       "4249  Test materials and solutions, using testing eq...   \n",
       "4250                                                NaN   \n",
       "\n",
       "                                        task_normalized soc_code_2010  \\\n",
       "0     interpret and explain policies rules regulatio...    11-1011.00   \n",
       "1     confer with board members organization officia...    11-1011.00   \n",
       "2     review reports submitted by staff members to r...    11-1011.00   \n",
       "3     serve as liaisons between organizations shareh...    11-1011.00   \n",
       "4     direct plan or implement policies objectives o...    11-1011.00   \n",
       "...                                                 ...           ...   \n",
       "4246             stop gathering arms when cars are full    53-7033.00   \n",
       "4247  collect and test samples of cleaning solutions...    53-7061.00   \n",
       "4248  stack cargo in locations such as transit sheds...    53-7062.00   \n",
       "4249  test materials and solutions using testing equ...    53-7072.00   \n",
       "4250                                                NaN           NaN   \n",
       "\n",
       "                                                  title  task_id  \\\n",
       "0                                      Chief Executives   8843.0   \n",
       "1                                      Chief Executives   8824.0   \n",
       "2                                      Chief Executives   8830.0   \n",
       "3                                      Chief Executives   8840.0   \n",
       "4                                      Chief Executives   8826.0   \n",
       "...                                                 ...      ...   \n",
       "4246      Loading Machine Operators, Underground Mining  15190.0   \n",
       "4247                 Cleaners of Vehicles and Equipment   5010.0   \n",
       "4248  Laborers and Freight, Stock, and Material Move...  10795.0   \n",
       "4249            Pump Operators, Except Wellhead Pumpers  14622.0   \n",
       "4250                                                NaN      NaN   \n",
       "\n",
       "         task_type  n_responding     date domain_source  n_occurrences  \\\n",
       "0             Core          87.0  07/2014     Incumbent            1.0   \n",
       "1             Core          87.0  07/2014     Incumbent            1.0   \n",
       "2             Core          87.0  07/2014     Incumbent            1.0   \n",
       "3     Supplemental          87.0  07/2014     Incumbent            1.0   \n",
       "4             Core          87.0  07/2014     Incumbent            1.0   \n",
       "...            ...           ...      ...           ...            ...   \n",
       "4246  Supplemental          78.0  06/2008     Incumbent            1.0   \n",
       "4247  Supplemental          87.0  07/2013     Incumbent            1.0   \n",
       "4248  Supplemental          87.0  07/2013     Incumbent            1.0   \n",
       "4249  Supplemental         105.0  06/2007     Incumbent            1.0   \n",
       "4250           NaN           NaN      NaN           NaN            NaN   \n",
       "\n",
       "      pct_weighted  pct_normalized major_group_code  \n",
       "0         0.040047        0.049488               11  \n",
       "1         0.017693        0.021864               11  \n",
       "2         0.001907        0.002357               11  \n",
       "3         0.003072        0.003797               11  \n",
       "4         0.004238        0.005237               11  \n",
       "...            ...             ...              ...  \n",
       "4246      0.002755        0.003404               53  \n",
       "4247      0.002013        0.002488               53  \n",
       "4248      0.002331        0.002880               53  \n",
       "4249      0.001589        0.001964               53  \n",
       "4250      0.391572             NaN              NaN  \n",
       "\n",
       "[4251 rows x 13 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# pct_onet_tasks_df.to_csv(\"../merged_data_files/pct_onet_tasks.csv\", index=False)\n",
    "# pct_onet_tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825bff0",
   "metadata": {},
   "source": [
    "## Step 2: Add SOC Major Occupational Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_soc_structure(pct_onet_tasks_df, soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the previous DataFrame and adds major occupational categories to each row based on the soc structure data \n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_onet_tasks_df (pd.DataFrame): DataFrame from previous step containing pcts mapped to task statements and O*NET metadata\n",
    "        soc_structure_df (pd.DataFrame): DataFrame containing the SOC structure with major, minor, and detailed categories for occupations\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with major occupational categories added\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename column\n",
    "    soc_structure_df.rename(columns={\n",
    "    \"SOC or O*NET-SOC 2019 Title\": \"major_occ_category\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Create new df and columns for merging\n",
    "    pct_onet_tasks_df[\"major_group_code\"] = pct_onet_tasks_df[\"soc_code_2010\"].str[:2]\n",
    "    soc_structure_df = soc_structure_df.dropna(subset=['Major Group']).copy()\n",
    "    soc_structure_df[\"major_group_code\"] = soc_structure_df[\"Major Group\"].str[:2]\n",
    "    \n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_onet_tasks_df.merge(\n",
    "        soc_structure_df[['major_group_code', 'major_occ_category']],\n",
    "        on='major_group_code',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"major_group_code\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "soc_structure_df = pd.read_csv(\"../extra_data/soc_structure_2019.csv\")\n",
    "pct_tasks_soc_structure_df = add_soc_structure(pct_onet_tasks_df, soc_structure_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7521495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# pct_tasks_soc_structure_df.to_csv(\"../merged_data_files/pct_tasks_soc_structure.csv\", index=False)\n",
    "# pct_tasks_soc_structure_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f3c3f",
   "metadata": {},
   "source": [
    "## Step 3: Add 2024 Wage and Employment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9742d3d",
   "metadata": {},
   "source": [
    "### 3.1: Add Updated (2019) SOC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ca5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df of updated SOC codes to merge with up to date wage and employment data\n",
    "\n",
    "def add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles from our main df and their corresponding O*NET-SOC 2019 code (some titles are duplicated as they get split into different SOC codes)\n",
    "    This is so we can merge the wage and employment data separate from our main df and merge all at once. \n",
    "\n",
    "    Args:\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from previous step.\n",
    "        soc_crosswalk_df (pd.DataFrame): DataFrame 2010 and 2019 occupation titles and SOC codes\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'soc_code_2019' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename columns\n",
    "    soc_crosswalk_df = soc_crosswalk_df.rename(\n",
    "        columns={\n",
    "            \"O*NET-SOC 2010 Title\": \"title\",\n",
    "            \"O*NET-SOC 2019 Code\": \"onet_soc_code_2019\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    soc_crosswalk_df['soc_code_2019'] = soc_crosswalk_df['onet_soc_code_2019'].str[:7]\n",
    "\n",
    "    # Get unique titles from rolling DataFrame\n",
    "    titles_df = pct_tasks_soc_structure_df[[\"title\"]].drop_duplicates()\n",
    "\n",
    "    # Merge to attach 2019 SOC codes\n",
    "    merged = titles_df.merge(\n",
    "        soc_crosswalk_df[[\"title\", \"soc_code_2019\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "soc_crosswalk_df = pd.read_csv(\"../extra_data/2010_to_2019_soc_crosswalk.csv\")\n",
    "title_and_2019_soc_df = add_updated_soc_code(pct_tasks_soc_structure_df, soc_crosswalk_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e32c6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# title_and_2019_soc_df.to_csv(\"../merged_data_files/title_and_2019_soc.csv\", index=False)\n",
    "# title_and_2019_soc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb78cf",
   "metadata": {},
   "source": [
    "### 3.2: Add 2024 National Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf237b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_wage_2024(title_and_2019_soc_df, nat_wage_df, scraped_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national annual and hourly median salary from 2024. \n",
    "    It also includes a 6 (from previous df) & 5 digit SOC code for use in following merging. \n",
    "\n",
    "    Args:\n",
    "        title_and_2019_soc_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_wage_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "        scraped_wage_df (pd.DataFrame): DataFrame containing scraped wage data from O*NET's website from Jan 2020 \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = nat_wage_df[[\"OCC_CODE\", \"O_GROUP\", \"H_MEDIAN\", \"A_MEDIAN\"]].copy()\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"H_MEDIAN\", \"A_MEDIAN\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = title_and_2019_soc_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on\n",
    "    merged[\"5_digit_soc\"] = merged[\"soc_code_2019\"].astype(str).str[:6]     \n",
    "    wage_df_trimmed[\"5_digit_soc\"] = wage_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    wage_df_trimmed_fallback_1st = wage_df_trimmed[wage_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        wage_df_trimmed_fallback_1st[[\"5_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from fallback columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "    # Create column to merge on and where annual median is missing\n",
    "    scraped_wage_df[\"title\"] = scraped_wage_df[\"JobName\"]\n",
    "    merged_fallback_2nd = merged[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create 2nd fallback df with scraper wage data\n",
    "    fallback_merge_2nd = merged_fallback_2nd.merge(\n",
    "        scraped_wage_df[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\", how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_2nd_unique_titles = fallback_merge_2nd.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge 2nd fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_2nd_unique_titles[[\"title\", \"MedianSalary\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from scraper median columns and make present value due to inflation\n",
    "    inflation_factor = 1.24\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"MedianSalary\"] * inflation_factor)\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"A_MEDIAN\"].isna() & merged[\"H_MEDIAN\"].notna(), \"A_MEDIAN\"] = (\n",
    "        merged[\"H_MEDIAN\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].notna(), \"H_MEDIAN\"] = (\n",
    "        merged[\"A_MEDIAN\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Create final national wage columns by averaging for any duplicate titles and drop uneeded columns. \n",
    "    merged[\"h_median_national\"] = merged.groupby(\"title\")[\"H_MEDIAN\"].transform(\"mean\")\n",
    "    merged[\"a_median_national\"] = merged.groupby(\"title\")[\"A_MEDIAN\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"H_MEDIAN\", \"A_MEDIAN\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\", \"MedianSalary\", \"O_GROUP\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_wage_2024_df = pd.read_csv(\"../extra_data/oews_national_2024.csv\")\n",
    "scraped_wage_df = pd.read_csv(\"../extra_data/scraped_wage_data.csv\")\n",
    "titles_and_nat_wage_2024_df = add_nat_wage_2024(title_and_2019_soc_df, nat_wage_2024_df, scraped_wage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d64777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_and_nat_wage_2024_df.to_csv(\"../merged_data_files/titles_and_nat_wage_2024.csv\", index=False)\n",
    "# titles_and_nat_wage_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5fa20",
   "metadata": {},
   "source": [
    "### 3.3: Add 2024 State Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bcee71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_wage_2024(titles_and_nat_wage_df, state_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state annual and hourly median salary from 2024. \n",
    "\n",
    "    Args:\n",
    "        titles_and_nat_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        wage_df (pd.DataFrame): DataFrame of OEWS data from 2024 with state level breakdown\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    wage_df_trimmed = state_wage_df[[\"OCC_CODE\", \"H_MEDIAN\", \"A_MEDIAN\", \"AREA_TITLE\"]].copy()\n",
    "    wage_df_trimmed = wage_df_trimmed[wage_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\",\n",
    "                                    \"H_MEDIAN\": \"h_median_state\",\n",
    "                                    \"A_MEDIAN\": \"a_median_state\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"h_median_state\", \"a_median_state\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_and_nat_wage_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"a_median_state\"].isna() & merged[\"h_median_state\"].notna(), \"a_median_state\"] = (\n",
    "        merged[\"h_median_state\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"h_median_state\"].isna() & merged[\"a_median_state\"].notna(), \"h_median_state\"] = (\n",
    "        merged[\"a_median_state\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data\n",
    "    merged.loc[merged[\"a_median_state\"].isna(), \"a_median_state\"] = (\n",
    "        merged[\"a_median_national\"]\n",
    "    )\n",
    "    merged.loc[merged[\"h_median_state\"].isna(), \"h_median_state\"] = (\n",
    "        merged[\"h_median_national\"]\n",
    "    )\n",
    "\n",
    "    merged[\"h_median_utah\"] = merged.groupby(\"title\")[\"h_median_state\"].transform(\"mean\")\n",
    "    merged[\"a_median_utah\"] = merged.groupby(\"title\")[\"a_median_state\"].transform(\"mean\")\n",
    "    merged.drop(columns=[\"h_median_state\", \"a_median_state\", \"AREA_TITLE\"], inplace=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "state_wage_df_2024 = pd.read_csv(\"../extra_data/oews_states_2024.csv\")\n",
    "titles_nat_and_state_wage_2024_df = add_state_wage_2024(titles_and_nat_wage_2024_df, state_wage_df_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01aba89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_nat_and_state_wage_2024_df.to_csv(\"../merged_data_files/titles_nat_and_state_wage_2024.csv\", index=False)\n",
    "# titles_nat_and_state_wage_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee5942",
   "metadata": {},
   "source": [
    "### 3.4: Add 2024 National Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_emp_2024(titles_nat_and_state_wage_df, nat_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national employment data from 2024.  \n",
    "\n",
    "    Args:\n",
    "        titles_nat_and_state_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national employment data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "     # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"O_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_nat_and_state_wage_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2019\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"O_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurences for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_national\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP_fallback\", \"TOT_EMP\", \"O_GROUP\", \"TOT_EMP_adj\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_emp_df_2024 = pd.read_csv(\"../extra_data/oews_national_2024.csv\")\n",
    "titles_wage_nat_emp_2024_df = add_nat_emp_2024(titles_nat_and_state_wage_2024_df, nat_emp_df_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b9ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_nat_emp_2024_df.to_csv(\"../merged_data_files/titles_wage_nat_emp_2024.csv\", index=False)\n",
    "# titles_wage_nat_emp_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd5aae",
   "metadata": {},
   "source": [
    "### 3.5: Add 2024 State Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38846383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_emp_2024(titles_wage_nat_emp_df, state_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state employment data from 2024.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_nat_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_emp_df (pd.DataFrame): DataFrame of OEWS data from 2024.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state employment data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"AREA_TITLE\"]].copy()\n",
    "    emp_df_trimmed = emp_df_trimmed[emp_df_trimmed[\"AREA_TITLE\"] == \"Utah\"]\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2019\"}, inplace=True)\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_wage_nat_emp_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2019\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[\n",
    "    (state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"AREA_TITLE\"] == \"Utah\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"emp_total_national\"] * utah_share).round())\n",
    "\n",
    "    # Create final national emp columns by dividing by number of occurances for each soc code and summing per occupation. \n",
    "    title_counts = merged.groupby(\"title\")[\"soc_code_2019\"].transform(\"count\")\n",
    "    merged[\"TOT_EMP_adj\"] = merged[\"TOT_EMP\"] / title_counts\n",
    "    merged[\"emp_total_utah\"] = merged.groupby(\"title\")[\"TOT_EMP_adj\"].transform(\"sum\")\n",
    "\n",
    "    merged.drop(columns=[\"TOT_EMP\", \"AREA_TITLE\", \"TOT_EMP_adj\"], inplace=True)\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_2024_df = pd.read_csv(\"../extra_data/oews_states_2024.csv\")\n",
    "titles_wage_all_emp_2024_df = add_state_emp_2024(titles_wage_nat_emp_2024_df, state_emp_2024_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b02bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_all_emp_2024_df.to_csv(\"../merged_data_files/titles_wage_all_emp_2024.csv\", index=False)\n",
    "# titles_wage_all_emp_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6269e53",
   "metadata": {},
   "source": [
    "### 3.6: Merge 2024 Wage and Employment Data Into Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d96d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage_emp_to_tasks_2024(titles_wage_all_emp_df, pct_tasks_soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with our wage and employment data from 2024 added to our task data.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_all_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from step 2\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with wage and employment data from 2024 added to task data\n",
    "    \"\"\"\n",
    "\n",
    "    titles_wage_all_emp_df = titles_wage_all_emp_df.drop_duplicates(subset=\"title\").copy()\n",
    "\n",
    "    titles_wage_all_emp_df.drop(columns=[\"5_digit_soc\", \"soc_code_2019\"], inplace=True)\n",
    "\n",
    "    merged = pct_tasks_soc_structure_df.merge(\n",
    "        titles_wage_all_emp_df,\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged.rename(columns={\"h_median_national\": \"h_med_nat_2024\",\n",
    "                                    \"a_median_national\": \"a_med_nat_2024\",\n",
    "                                    \"h_median_utah\": \"h_med_ut_2024\",\n",
    "                                    \"a_median_utah\": \"a_med_ut_2024\",\n",
    "                                    \"emp_total_national\": \"emp_tot_nat_2024\",\n",
    "                                    \"emp_total_utah\": \"emp_tot_ut_2024\"}, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "\n",
    "task_wage_emp_2024_df = wage_emp_to_tasks_2024(titles_wage_all_emp_2024_df, pct_tasks_soc_structure_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a95323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# task_wage_emp_2024_df.to_csv(\"../merged_data_files/task_wage_emp_2024.csv\", index=False)\n",
    "# task_wage_emp_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3e925",
   "metadata": {},
   "source": [
    "## Step 4: Add 2015 Wage and Employment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92805752",
   "metadata": {},
   "source": [
    "### 4.1: Add 2015 National Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfbdec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_wage_2015(pct_tasks_soc_structure_df, nat_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame of titles and their 2010 SOC codes\n",
    "    Returns DataFrame with occupation titles along with their national annual and hourly median salary from 2015 in real and nominal terms merged with titles and SOC codes. \n",
    "    It also includes a 5 digit SOC code for use in following merging. \n",
    "\n",
    "    Args:\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from Step 2.\n",
    "        nat_wage_df (pd.DataFrame): DataFrame of OEWS data from 2015 \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national wage data from 2024 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Make df with titles and SOC codes\n",
    "    title_soc_code_2010_df = pct_tasks_soc_structure_df[[\"title\", \"soc_code_2010\"]].drop_duplicates(subset=\"title\").copy()\n",
    "    title_soc_code_2010_df.reset_index(drop=True, inplace=True)\n",
    "    title_soc_code_2010_df['soc_code_2010'] = title_soc_code_2010_df['soc_code_2010'].str[:7]\n",
    "\n",
    "    # Get only columns needed\n",
    "    wage_df_trimmed = nat_wage_df[[\"OCC_CODE\", \"OCC_GROUP\", \"H_MEDIAN\", \"A_MEDIAN\", \"H_MEAN\", \"A_MEAN\"]].copy()\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"H_MEDIAN\", \"A_MEDIAN\", \"H_MEAN\", \"A_MEAN\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = title_soc_code_2010_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"A_MEDIAN\"].isna() & merged[\"H_MEDIAN\"].notna(), \"A_MEDIAN\"] = (\n",
    "        merged[\"H_MEDIAN\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"H_MEDIAN\"].isna() & merged[\"A_MEDIAN\"].notna(), \"H_MEDIAN\"] = (\n",
    "        merged[\"A_MEDIAN\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on\n",
    "    merged[\"5_digit_soc\"] = merged[\"soc_code_2010\"].astype(str).str[:6]     \n",
    "    wage_df_trimmed[\"5_digit_soc\"] = wage_df_trimmed[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    wage_df_trimmed_fallback_1st = wage_df_trimmed[wage_df_trimmed[\"OCC_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"H_MEDIAN\"].isna() | merged[\"A_MEDIAN\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        wage_df_trimmed_fallback_1st[[\"5_digit_soc\", \"H_MEDIAN\", \"A_MEDIAN\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing median values from fallback columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEDIAN_fallback\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEDIAN_fallback\"])\n",
    "\n",
    "    # Fill missing median values from mean columns\n",
    "    merged[\"H_MEDIAN\"] = merged[\"H_MEDIAN\"].fillna(merged[\"H_MEAN\"])\n",
    "    merged[\"A_MEDIAN\"] = merged[\"A_MEDIAN\"].fillna(merged[\"A_MEAN\"])\n",
    "\n",
    "    # Rename and drop columns for cleanup \n",
    "    merged.rename(columns={\"H_MEDIAN\": \"h_med_nat_nominal\"}, inplace=True)\n",
    "    merged.rename(columns={\"A_MEDIAN\": \"a_med_nat_nominal\"}, inplace=True)\n",
    "    merged.drop(columns=[\"H_MEDIAN_fallback\", \"A_MEDIAN_fallback\", \"H_MEAN\", \"A_MEAN\", \"OCC_GROUP\"], inplace=True)\n",
    "\n",
    "    # Make present value column for inflation\n",
    "    inflation_factor = 1.36\n",
    "    merged[\"h_med_nat_real\"] = merged[\"h_med_nat_nominal\"] * inflation_factor\n",
    "    merged[\"a_med_nat_real\"] = merged[\"a_med_nat_nominal\"] * inflation_factor\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_wage_df_2015 = pd.read_csv(\"../extra_data/oews_national_2015.csv\")\n",
    "titles_and_nat_wage_2015_df = add_nat_wage_2015(pct_tasks_soc_structure_df, nat_wage_df_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "913793cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_and_nat_wage_2015_df.to_csv(\"../merged_data_files/titles_and_nat_wage_2015.csv\", index=False)\n",
    "# titles_and_nat_wage_2015_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ddefb",
   "metadata": {},
   "source": [
    "### 4.2: Add 2015 State Wage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9c10497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_wage_2015(titles_and_nat_wage_df, state_wage_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state annual and hourly median salary from 2015 in nominal and real terms. \n",
    "\n",
    "    Args:\n",
    "        titles_and_nat_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_wage_df (pd.DataFrame): DataFrame of OEWS data from 2015 with state level breakdown\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state wage data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    wage_df_trimmed = state_wage_df[[\"OCC_CODE\", \"H_MEDIAN\", \"A_MEDIAN\", \"ST\"]].copy()\n",
    "    wage_df_trimmed = wage_df_trimmed[wage_df_trimmed[\"ST\"] == \"UT\"]\n",
    "    wage_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\",\n",
    "                                    \"H_MEDIAN\": \"h_median_state\",\n",
    "                                    \"A_MEDIAN\": \"a_median_state\"}, inplace=True)\n",
    "\n",
    "    # Change wage columns to floats\n",
    "    for c in [\"h_median_state\", \"a_median_state\"]:\n",
    "        wage_df_trimmed[c] = pd.to_numeric(wage_df_trimmed[c], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_and_nat_wage_df.merge(\n",
    "        wage_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing annual median using hourly median * 2080 (52 weeks * 40 hours)\n",
    "    merged.loc[merged[\"a_median_state\"].isna() & merged[\"h_median_state\"].notna(), \"a_median_state\"] = (\n",
    "        merged[\"h_median_state\"] * 2080\n",
    "    )\n",
    "\n",
    "    # Fill missing hourly median using annual median / 2080\n",
    "    merged.loc[merged[\"h_median_state\"].isna() & merged[\"a_median_state\"].notna(), \"h_median_state\"] = (\n",
    "        merged[\"a_median_state\"] / 2080\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data\n",
    "    merged.loc[merged[\"a_median_state\"].isna(), \"a_median_state\"] = (\n",
    "        merged[\"a_med_nat_nominal\"]\n",
    "    )\n",
    "    merged.loc[merged[\"h_median_state\"].isna(), \"h_median_state\"] = (\n",
    "        merged[\"h_med_nat_nominal\"]\n",
    "    )\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"h_median_state\": \"h_med_utah_nominal\",\n",
    "                                    \"a_median_state\": \"a_med_utah_nominal\"}, inplace=True)\n",
    "    merged.drop(columns=[\"ST\"], inplace=True)\n",
    "\n",
    "    # Make present value column for inflation\n",
    "    inflation_factor = 1.36\n",
    "    merged[\"h_med_utah_real\"] = merged[\"h_med_utah_nominal\"] * inflation_factor\n",
    "    merged[\"a_med_utah_real\"] = merged[\"a_med_utah_nominal\"] * inflation_factor\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_wage_df_2015 = pd.read_csv(\"../extra_data/oews_states_2015.csv\")\n",
    "titles_nat_and_state_wage_2015_df = add_state_wage_2015(titles_and_nat_wage_2015_df, state_wage_df_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c35ead75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_nat_and_state_wage_2015_df.to_csv(\"../merged_data_files/titles_nat_and_state_wage_2015.csv\", index=False)\n",
    "# titles_nat_and_state_wage_2015_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f22d4",
   "metadata": {},
   "source": [
    "### 4.3: Add 2015 National Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62763f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nat_emp_2015(titles_nat_and_state_wage_df, nat_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their national employment data from 2015.  \n",
    "\n",
    "    Args:\n",
    "        titles_nat_and_state_wage_df (pd.DataFrame): DataFrame from previous step.\n",
    "        nat_emp_df (pd.DataFrame): DataFrame of OEWS data from 2015.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with national employment data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = nat_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"OCC_GROUP\"]].copy()\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    emp_df_trimmed[\"TOT_EMP\"] = pd.to_numeric(emp_df_trimmed[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_nat_and_state_wage_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get 5 digit SOC codes for broad groups to merge on  \n",
    "    emp_df_trimmed[\"5_digit_soc\"] = emp_df_trimmed[\"soc_code_2010\"].astype(str).str[:6]\n",
    "\n",
    "    #Create fallback DataFrames with only broad groups and where median values are missing\n",
    "    emp_df_trimmed_fallback_1st = emp_df_trimmed[emp_df_trimmed[\"OCC_GROUP\"] == \"broad\"]\n",
    "    merged_fallback_1st = merged[merged[\"TOT_EMP\"].isna()]\n",
    "\n",
    "    # Create fallback df with broad group wages\n",
    "    fallback_merge = merged_fallback_1st.merge(\n",
    "        emp_df_trimmed_fallback_1st[[\"5_digit_soc\", \"TOT_EMP\"]],\n",
    "        on=\"5_digit_soc\", how=\"left\",\n",
    "        suffixes=(\"\", \"_fallback\")\n",
    "    )\n",
    "\n",
    "    # Make titles unique so we don't create a Cartesian product when merging into main DataFrame\n",
    "    fallback_merge_unique_titles = fallback_merge.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    # Merge fallback data into the main dataframe\n",
    "    merged = merged.merge(\n",
    "        fallback_merge_unique_titles[[\"title\", \"TOT_EMP_fallback\"]],\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill missing emp values from fallback columns\n",
    "    merged[\"TOT_EMP\"] = merged[\"TOT_EMP\"].fillna(merged[\"TOT_EMP_fallback\"])\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"emp_tot_nat\"}, inplace=True)\n",
    "    merged.drop(columns=[\"TOT_EMP_fallback\", \"OCC_GROUP\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "nat_emp_df_2015 = pd.read_csv(\"../extra_data/oews_national_2015.csv\")\n",
    "titles_wage_nat_emp_2015_df = add_nat_emp_2015(titles_nat_and_state_wage_2015_df, nat_emp_df_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e71123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_nat_emp_2015_df.to_csv(\"../merged_data_files/titles_wage_nat_emp_2015.csv\", index=False)\n",
    "# titles_wage_nat_emp_2015_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97bb12",
   "metadata": {},
   "source": [
    "### 4.4: Add 2015 State Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d2cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_emp_2015(titles_wage_nat_emp_df, state_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with occupation titles along with their state employment data from 2015.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_nat_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        state_emp_df (pd.DataFrame): DataFrame of OEWS data from 2015.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with state employment data from 2015 added\n",
    "    \"\"\"\n",
    "\n",
    "    # Change emp columns to floats\n",
    "    state_emp_df[\"TOT_EMP\"] = pd.to_numeric(state_emp_df[\"TOT_EMP\"], errors=\"coerce\")\n",
    "\n",
    "    # Get only columns needed\n",
    "    emp_df_trimmed = state_emp_df[[\"OCC_CODE\", \"TOT_EMP\", \"ST\"]].copy()\n",
    "    emp_df_trimmed = emp_df_trimmed[emp_df_trimmed[\"ST\"] == \"UT\"]\n",
    "    emp_df_trimmed.rename(columns={\"OCC_CODE\": \"soc_code_2010\"}, inplace=True)\n",
    "\n",
    "    # Initial merge on detailed SOC codes\n",
    "    merged = titles_wage_nat_emp_2015_df.merge(\n",
    "        emp_df_trimmed, \n",
    "        on=\"soc_code_2010\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Fill remaining missing values with national data by multiplying by the proportion of state employment to national employment\n",
    "    total_nat_emp = state_emp_df.loc[state_emp_df[\"OCC_CODE\"] == \"00-0000\", \"TOT_EMP\"].sum()\n",
    "    total_utah_emp = state_emp_df.loc[(state_emp_df[\"OCC_CODE\"] == \"00-0000\") & (state_emp_df[\"ST\"] == \"UT\"), \"TOT_EMP\"].iloc[0]\n",
    "    utah_share = float(total_utah_emp) / float(total_nat_emp)\n",
    "    merged.loc[merged[\"TOT_EMP\"].isna(), \"TOT_EMP\"] = (\n",
    "    (merged[\"emp_tot_nat\"] * utah_share).round())\n",
    "\n",
    "    # Rename and drop columns for cleanup\n",
    "    merged.rename(columns={\"TOT_EMP\": \"emp_tot_utah\"}, inplace=True)\n",
    "    merged.drop(columns=[\"ST\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "state_emp_2015_df = pd.read_csv(\"../extra_data/oews_states_2015.csv\")\n",
    "titles_wage_all_emp_2015_df = add_state_emp_2015(titles_wage_nat_emp_2015_df, state_emp_2015_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1cb4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# titles_wage_all_emp_2015_df.to_csv(\"../merged_data_files/titles_wage_all_emp_2015.csv\", index=False)\n",
    "# titles_wage_all_emp_2015_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef3b3e",
   "metadata": {},
   "source": [
    "### 4.5: Merge 2015 Wage and Employment Data Into Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba12918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage_emp_to_tasks_2015(titles_wage_all_emp_df, pct_tasks_soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with our wage and employment data from 2015 added to our task data.  \n",
    "\n",
    "    Args:\n",
    "        titles_wage_all_emp_df (pd.DataFrame): DataFrame from previous step.\n",
    "        pct_tasks_soc_structure_df (pd.DataFrame): DataFrame from step 2\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with wage and employment data from 2015 added to task data\n",
    "    \"\"\"\n",
    "\n",
    "    titles_wage_all_emp_df = titles_wage_all_emp_df.drop_duplicates(subset=\"title\")\n",
    "\n",
    "    titles_wage_all_emp_df.drop(columns=[\"soc_code_2010\", \"5_digit_soc\"], inplace=True)\n",
    "\n",
    "    merged = pct_tasks_soc_structure_df.merge(\n",
    "        titles_wage_all_emp_df,\n",
    "        on=\"title\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged.rename(columns={\"h_med_nat_nominal\": \"h_med_nat_nominal_2015\",\n",
    "                            \"a_med_nat_nominal\": \"a_med_nat_nominal_2015\",\n",
    "                            \"h_med_nat_real\": \"h_med_nat_real_2015\",\n",
    "                            \"a_med_nat_real\": \"a_med_nat_real_2015\",\n",
    "                            \"h_med_utah_nominal\": \"h_med_ut_nominal_2015\",\n",
    "                            \"a_med_utah_nominal\": \"a_med_ut_nominal_2015\",\n",
    "                            \"h_med_utah_real\": \"h_med_ut_real_2015\",\n",
    "                            \"a_med_utah_real\": \"a_med_ut_real_2015\",\n",
    "                            \"emp_tot_nat\": \"emp_tot_nat_2015\",\n",
    "                            \"emp_tot_utah\": \"emp_tot_ut_2015\"}, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "\n",
    "tasks_all_wage_emp_df = wage_emp_to_tasks_2015(titles_wage_all_emp_2015_df, task_wage_emp_2024_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ace651a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_all_wage_emp_df.to_csv(\"../merged_data_files/tasks_all_wage_emp.csv\", index=False)\n",
    "# tasks_all_wage_emp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af587a",
   "metadata": {},
   "source": [
    "## Step 5: Adjust Employment Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a19927ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_emp(tasks_all_wage_emp_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reallocates employment numbers based on the relative percent of Claude conversations, as we have some duplicate\n",
    "    6 digit SOC codes but different titles  \n",
    "\n",
    "    Args:\n",
    "        tasks_all_wage_emp_df (pd.DataFrame): DataFrame from previous 4.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with correct employment numbers\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_all_wage_emp_df\n",
    "\n",
    "    # 6-digit SOC to remove decimals (e.g., '11-1011.03' -> '11-1011')\n",
    "    df[\"soc6\"] = df[\"soc_code_2010\"].astype(str).str[:7]\n",
    "\n",
    "    # share of each title within its 6-digit SOC based on pct_normalized\n",
    "    title_pct_sum   = df.groupby([\"soc6\",\"title\"])[\"pct_normalized\"].transform(\"sum\")\n",
    "    soc6_pct_sum    = df.groupby(\"soc6\")[\"pct_normalized\"].transform(\"sum\")\n",
    "    df[\"soc6_share\"] = title_pct_sum / soc6_pct_sum\n",
    "\n",
    "    # columns to allocate (only those that exist will be processed)\n",
    "    emp_cols = [c for c in [\"emp_tot_nat_2024\",\"emp_tot_ut_2024\",\n",
    "                            \"emp_tot_nat_2015\",\"emp_tot_ut_2015\"] if c in df.columns]\n",
    "\n",
    "    # Calculate the correct employment numbers by multiplying each by their share in the 6 digit SOC group\n",
    "    for c in emp_cols:\n",
    "        soc6_tot = df.groupby(\"soc6\")[c].transform(\"max\") \n",
    "        alloc_col = f\"{c}_alloc_by_pct\"\n",
    "        df[c] = round(soc6_tot * df[\"soc6_share\"])\n",
    "\n",
    "    # Create percent-of-workforce columns from the reallocated totals\n",
    "    pct_map = {\n",
    "        \"emp_tot_nat_2024\":  \"emp_pct_nat_2024\",\n",
    "        \"emp_tot_ut_2024\":   \"emp_pct_ut_2024\",\n",
    "        \"emp_tot_nat_2015\":  \"emp_pct_nat_2015\",\n",
    "        \"emp_tot_ut_2015\": \"emp_pct_ut_2015\",\n",
    "    }\n",
    "\n",
    "    for tot_col, pct_col in pct_map.items():\n",
    "        if tot_col in df.columns:\n",
    "            total_sum = df[[\"title\", tot_col]].drop_duplicates(\"title\")[tot_col].sum()\n",
    "            df[pct_col] = (df.groupby(\"title\")[tot_col].transform(\"first\") / total_sum) * 100\n",
    "\n",
    "    df.drop(columns=[\"soc6\",\"soc6_share\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_wage_emp_final_df = adjust_emp(tasks_all_wage_emp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9218b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_wage_emp_final_df.to_csv(\"../merged_data_files/tasks_wage_emp_final.csv\", index=False)\n",
    "# tasks_wage_emp_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f1ca7",
   "metadata": {},
   "source": [
    "## Step 6: Add Task Rating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5489a",
   "metadata": {},
   "source": [
    "### 6.1: Bring In 2025 and 2015 Task Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f21e13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task_ratings(task_ratings_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes frequency, relevance, and importance from May 2025 and Oct 2015 task ratings data from O*NET.\n",
    "        Uses frequency mapping weights to get a single number for frequency\n",
    "\n",
    "    Args:\n",
    "        task_ratings_df (pd.DataFrame): DataFrame with the O*NET Task Rating data from 2025 and 2015\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with task ratings added to other task columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Get freq rows, drop unusable ones, generate freq aggregates\n",
    "    freq_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"FT\"].copy()\n",
    "\n",
    "    # Drop rows without category or invalid categories\n",
    "    freq_df = freq_df[pd.to_numeric(freq_df[\"Category\"], errors='coerce').notnull()]\n",
    "    freq_df[\"Category\"] = freq_df[\"Category\"].astype(int)\n",
    "\n",
    "    # Apply weights\n",
    "    freq_df[\"freq_mean\"] = freq_df[\"Data Value\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_lower\"] = freq_df[\"Lower CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_upper\"] = freq_df[\"Upper CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "\n",
    "    # Sum across categories to get per-task total\n",
    "    freq_agg = freq_df.groupby([\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"]).agg({\n",
    "        \"freq_mean\": \"sum\",\n",
    "        \"freq_lower\": \"sum\",\n",
    "        \"freq_upper\": \"sum\"\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Get importance and relevance ratings\n",
    "    importance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"IM\"].copy()\n",
    "    importance_df = importance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    importance_df = importance_df.rename(columns={\n",
    "        \"Data Value\": \"importance\",\n",
    "        \"Lower CI Bound\": \"importance_lower\",\n",
    "        \"Upper CI Bound\": \"importance_upper\"\n",
    "    })\n",
    "\n",
    "    relevance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"RT\"].copy()\n",
    "    relevance_df = relevance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    relevance_df = relevance_df.rename(columns={\n",
    "        \"Data Value\": \"relevance\",\n",
    "        \"Lower CI Bound\": \"relevance_lower\",\n",
    "        \"Upper CI Bound\": \"relevance_upper\"\n",
    "    })\n",
    "\n",
    "    # Merge ratings\n",
    "    merged_ratings = freq_agg.merge(importance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "    merged_ratings = merged_ratings.merge(relevance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "\n",
    "    merged_ratings[\"task_normalized\"] = merged_ratings[\"Task\"].str.lower().str.strip()\n",
    "\n",
    "    return merged_ratings\n",
    "\n",
    "\n",
    "task_ratings_2025_df = pd.read_csv(\"../extra_data/task_ratings_may_2025.csv\")\n",
    "ratings_cleaned_2025_df = add_task_ratings(task_ratings_2025_df)\n",
    "\n",
    "task_ratings_2015_df = pd.read_csv(\"../extra_data/task_ratings_oct_2015.csv\")\n",
    "ratings_cleaned_2015_df = add_task_ratings(task_ratings_2015_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b32e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# ratings_cleaned_2025_df.to_csv(\"../merged_data_files/ratings_cleaned_2025.csv\", index=False)\n",
    "# ratings_cleaned_2025_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49af411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# ratings_cleaned_2015_df.to_csv(\"../merged_data_files/ratings_cleaned_2015.csv\", index=False)\n",
    "# ratings_cleaned_2015_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab8a85",
   "metadata": {},
   "source": [
    "### 6.2: Merge Rating Values Into Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "870e63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_task_ratings(tasks_wage_emp_final_df, ratings_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data for both 2025 and 2015. Some values are missing\n",
    "    \n",
    "    Args:\n",
    "        tasks_wage_emp_final_df (pd.DataFrame): DataFrame from Step 5\n",
    "        ratings_df (pd.DataFrame): DataFrame containing cleaned task ratings (single year).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with task ratings values unfilled.\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_wage_emp_final_df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df[\"task_normalized\"] = df[\"task\"].apply(normalize_text)\n",
    "    ratings_df[\"task_normalized\"] = ratings_df[\"Task\"].apply(normalize_text)\n",
    "    df[\"title_normalized\"] = df[\"title\"].apply(normalize_text)\n",
    "    ratings_df[\"title_normalized\"] = ratings_df[\"Title\"].apply(normalize_text)\n",
    "\n",
    "    merged = df.merge(\n",
    "        ratings_df[\n",
    "            [\"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "             \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "             \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "             \"task_normalized\", \"title_normalized\"]\n",
    "        ],\n",
    "        on=[\"task_normalized\", \"title_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "tasks_final_2025_unfilled_df = merge_task_ratings(tasks_wage_emp_final_df, ratings_cleaned_2025_df)\n",
    "tasks_final_2015_unfilled_df = merge_task_ratings(tasks_wage_emp_final_df, ratings_cleaned_2015_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20d76d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_2025_unfilled_df.to_csv(\"../merged_data_files/tasks_final_2025_unfilled.csv\", index=False)\n",
    "# tasks_final_2025_unfilled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3259ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_2015_unfilled_df.to_csv(\"../merged_data_files/tasks_final_2015_unfilled.csv\", index=False)\n",
    "# tasks_final_2015_unfilled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c8ba5",
   "metadata": {},
   "source": [
    "### 6.3: Fill Missing Task Rating Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e600f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_ratings(tasks_final_unfilled_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function fills missing task rating values.\n",
    "    \n",
    "    Args:\n",
    "        tasks_final_unfilled_df (pd.DataFrame): DataFrame from previous step\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrames with task rating data added for both 2025 and 2015\n",
    "    \"\"\"\n",
    "\n",
    "    df = tasks_final_unfilled_df\n",
    "\n",
    "    # Mark rows that are missing any of the key values\n",
    "    df[\"imputed_rating_mean\"] = False\n",
    "    df[\"imputed_rating_ci\"] = False\n",
    "\n",
    "    # Loop through each metric\n",
    "    for col in [\"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "                \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "                \"relevance\", \"relevance_lower\", \"relevance_upper\"]:\n",
    "        \n",
    "        # Group by title and compute occupation-level mean\n",
    "        occ_means = df.groupby(\"title\")[col].mean()\n",
    "\n",
    "        # Group by major occ category and compute fallback mean\n",
    "        major_occ_means = df.groupby(\"major_occ_category\")[col].mean()\n",
    "\n",
    "        # Go row by row\n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isna(row[col]):\n",
    "                occ_val = occ_means.get(row[\"title\"], None)\n",
    "                occ_count = df[(df[\"title\"] == row[\"title\"]) & (df[col].notna())].shape[0]\n",
    "\n",
    "                if occ_count >= 3 and pd.notna(occ_val):\n",
    "                    df.at[i, col] = occ_val\n",
    "                    if col in [\"freq_mean\", \"importance\", \"relevance\"]:\n",
    "                        df.at[i, \"imputed_rating_mean\"] = True\n",
    "                    else:\n",
    "                        df.at[i, \"imputed_rating_ci\"] = True\n",
    "                else:\n",
    "                    soc_val = major_occ_means.get(row[\"major_occ_category\"], None)\n",
    "                    if pd.notna(soc_val):\n",
    "                        df.at[i, col] = soc_val\n",
    "                        if col in [\"freq_mean\", \"importance\", \"relevance\"]:\n",
    "                            df.at[i, \"imputed_rating_mean\"] = True\n",
    "                        else:\n",
    "                            df.at[i, \"imputed_rating_ci\"] = True\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_final_2025_filled_df = fill_missing_ratings(tasks_final_2025_unfilled_df)\n",
    "tasks_final_2015_filled_df = fill_missing_ratings(tasks_final_2015_unfilled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_2025_filled_df.to_csv(\"../merged_data_files/tasks_final_2025_filled.csv\", index=False)\n",
    "tasks_final_2025_filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ab21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_2015_filled_df.to_csv(\"../merged_data_files/tasks_final_2015_filled.csv\", index=False)\n",
    "tasks_final_2015_filled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44d768",
   "metadata": {},
   "source": [
    "### 6.4 Merge 2015 and 2025 Task Ratings To One DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6eb01327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_task_ratings_to_one_df(base_df, add_df, base_year, add_year):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and performs final cleanup.\n",
    "    \n",
    "    Args:\n",
    "        tasks_wage_emp_final_df (pd.DataFrame): DataFrame from Step 5\n",
    "        ratings_df (pd.DataFrame): DataFrame containing cleaned task ratings from either 2025 or 2015.\n",
    "        year (int): Year of the ratings data (e.g., 2015 or 2025)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information.\n",
    "    \"\"\"\n",
    "\n",
    "    base_df = base_df.rename(columns={\n",
    "            \"freq_mean\": f\"freq_mean_{base_year}\",\n",
    "            \"freq_lower\": f\"freq_lower_{base_year}\",\n",
    "            \"freq_upper\": f\"freq_upper_{base_year}\",\n",
    "            \"importance\": f\"importance_{base_year}\",\n",
    "            \"importance_lower\": f\"importance_lower_{base_year}\",\n",
    "            \"importance_upper\": f\"importance_upper_{base_year}\",\n",
    "            \"relevance\": f\"relevance_{base_year}\",\n",
    "            \"relevance_lower\": f\"relevance_lower_{base_year}\",\n",
    "            \"relevance_upper\": f\"relevance_upper_{base_year}\",\n",
    "            \"imputed_rating_mean\": f\"imputed_rating_mean_{base_year}\",\n",
    "            \"imputed_rating_ci\": f\"imputed_rating_ci_{base_year}\"\n",
    "        })\n",
    "    \n",
    "    add_df = add_df.rename(columns={\n",
    "            \"freq_mean\": f\"freq_mean_{add_year}\",\n",
    "            \"freq_lower\": f\"freq_lower_{add_year}\",\n",
    "            \"freq_upper\": f\"freq_upper_{add_year}\",\n",
    "            \"importance\": f\"importance_{add_year}\",\n",
    "            \"importance_lower\": f\"importance_lower_{add_year}\",\n",
    "            \"importance_upper\": f\"importance_upper_{add_year}\",\n",
    "            \"relevance\": f\"relevance_{add_year}\",\n",
    "            \"relevance_lower\": f\"relevance_lower_{add_year}\",\n",
    "            \"relevance_upper\": f\"relevance_upper_{add_year}\",\n",
    "            \"imputed_rating_mean\": f\"imputed_rating_mean_{add_year}\",\n",
    "            \"imputed_rating_ci\": f\"imputed_rating_ci_{add_year}\"\n",
    "        })\n",
    "\n",
    "\n",
    "    merged = base_df.merge(\n",
    "        add_df[[f\"freq_mean_{add_year}\", f\"freq_lower_{add_year}\", f\"freq_upper_{add_year}\",\n",
    "            f\"importance_{add_year}\", f\"importance_lower_{add_year}\", f\"importance_upper_{add_year}\",\n",
    "            f\"relevance_{add_year}\", f\"relevance_lower_{add_year}\", f\"relevance_upper_{add_year}\",\n",
    "            f\"imputed_rating_mean_{add_year}\", f\"imputed_rating_ci_{add_year}\",\n",
    "            \"task_normalized\", \"title_normalized\", \"soc_code_2010\"]],\n",
    "        on=[\"task_normalized\", \"title_normalized\", \"soc_code_2010\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "tasks_final_uncleaned_df = merge_task_ratings_to_one_df(tasks_final_2025_filled_df, tasks_final_2015_filled_df, 2025, 2015)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b79f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_uncleaned_df.to_csv(\"../merged_data_files/tasks_final_uncleaned.csv\", index=False)\n",
    "# tasks_final_uncleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72d4d4",
   "metadata": {},
   "source": [
    "## Step 7: Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3230a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleanup(df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Does some final cleanup and imputing for our data\n",
    "    \n",
    "    Args:\n",
    "        tasks_final_uncleaned_df (pd.DataFrame): DataFrame from Step 5\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[df[\"task_normalized\"].notna()].copy()\n",
    "    df[\"pct_weighted\"] = 100 * df[\"pct_weighted\"] / df[\"pct_weighted\"].sum()\n",
    "\n",
    "    # Get current column order\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    # Find the index of 'title' and insert 'title_normalized' right after it\n",
    "    title_idx = cols.index('title')\n",
    "    cols.insert(title_idx + 1, cols.pop(cols.index('title_normalized')))\n",
    "\n",
    "    # Reorder the dataframe\n",
    "    df = df[cols]\n",
    "\n",
    "    mask_missing = df[\"task_type\"].isna()\n",
    "    df.loc[mask_missing, \"task_type\"] = df.loc[mask_missing].apply(\n",
    "        lambda row: \"Core\" if (row[\"relevance_2015\"] >= 67 and row[\"importance_2015\"] >= 3.0) else \"Supplemental\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Fill missing n_responding values with the median within the same occupation\n",
    "    df[\"n_responding\"] = df.groupby(\"soc_code_2010\")[\"n_responding\"].transform(\n",
    "        lambda x: x.fillna(x.median()) if not x.isna().all() else x\n",
    "    )\n",
    "\n",
    "    # Drop specific confidence interval and bound columns\n",
    "    cols_to_drop = [\n",
    "    'freq_lower_2025', 'freq_upper_2025', 'importance_lower_2025', 'importance_upper_2025',\n",
    "    'relevance_lower_2025', 'relevance_upper_2025', 'imputed_rating_ci_2025',\n",
    "    'freq_lower_2015', 'freq_upper_2015', 'importance_lower_2015', 'importance_upper_2015', \n",
    "    'relevance_lower_2015', 'relevance_upper_2015', 'imputed_rating_ci_2015'\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    placeholder_values = [\"#\", \"*\", \"\", \"n/a\", \"na\", \"--\"]\n",
    "    df.replace(placeholder_values, pd.NA, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "tasks_final_df = final_cleanup(tasks_final_uncleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1aaba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# tasks_final_df.to_csv(\"../merged_data_files/tasks_final.csv\", index=False)\n",
    "# tasks_final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
