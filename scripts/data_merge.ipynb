{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe007551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1b0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower().strip()                   # lowercase + trim\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)            # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # collapse multiple spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dac79",
   "metadata": {},
   "source": [
    "### Step 1: Map Anthropic Task %s to O*NET v20.1 Task Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_to_onet_tasks(pct_df, task_statements_df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the tasks and percentage of occurrences from the Anthropic data, and merges it with the tasks statement data. \n",
    "        It normalizes the percents based on a weighted and non weighted approach.\n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_df (pd.DataFrame): DataFrame containing the Anthropic data of percent occurances of every task in their conversation data\n",
    "        task_statements_df (pd.DataFrame): DataFrame containing O*NET tasks and SOC titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with percentage of occurrences added.\n",
    "    \"\"\"\n",
    "\n",
    "    task_statements_df.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"soc_code\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Task ID\": \"task_id\",\n",
    "    \"Task\": \"task\",\n",
    "    \"Task Type\": \"task_type\",\n",
    "    \"Incumbents Responding\": \"n_responding\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Domain Source\": \"domain_source\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Normalize task columns\n",
    "    pct_df[\"task_normalized_temp\"] = pct_df[\"task_name\"].apply(normalize_text)\n",
    "    task_statements_df[\"task_normalized\"] = task_statements_df[\"task\"].apply(normalize_text)\n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_df.merge(\n",
    "        task_statements_df,\n",
    "        left_on=\"task_normalized_temp\",\n",
    "        right_on=\"task_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted and normalized percentages\n",
    "    merged[\"n_occurrences\"] = merged.groupby(\"task_normalized\")[\"title\"].transform(\"nunique\")\n",
    "    merged[\"pct_weighted\"] = 100 * merged[\"pct\"] / merged[\"pct\"].sum()\n",
    "    merged[\"pct_normalized\"] = 100 * (merged[\"pct\"] / merged[\"n_occurrences\"]) / (merged[\"pct\"] / merged[\"n_occurrences\"]).sum()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"task_name\", \"task_normalized_temp\", \"pct\"], inplace=True)\n",
    "\n",
    "    # Reorder so `task` is first and `task_normalized` is second\n",
    "    cols = [\"task\", \"task_normalized\"] + [c for c in merged.columns if c not in [\"task\", \"task_normalized\"]]\n",
    "    merged = merged[cols]\n",
    "    \n",
    "    # Sort by O*NET-SOC Code\n",
    "    merged.sort_values(by=\"soc_code\", ascending=True, inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "task_statements_df = pd.read_csv(\"../extra_data/task_statements_v20.1.csv\")\n",
    "pct_df = pd.read_csv(\"../original_data/onet_task_mappings.csv\")\n",
    "pct_onet_tasks_df = pct_to_onet_tasks(pct_df, task_statements_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c399688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "#pct_onet_tasks_df.to_csv(\"../merged_data_files/pct_onet_tasks.csv\", index=False)\n",
    "#pct_onet_tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825bff0",
   "metadata": {},
   "source": [
    "### Step 2: Add SOC Major Occupational Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_soc_structure(pct_onet_tasks_df, soc_structure_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the previous DataFrame and adds major occupational categories to each row based on the soc structure data \n",
    "        See documentation for more details.\n",
    "\n",
    "    Args:\n",
    "        pct_onet_tasks_df (pd.DataFrame): DataFrame from previous step containing pcts mapped to task statements and O*NET metadata\n",
    "        soc_structure_df (pd.DataFrame): DataFrame containing the SOC structure with major, minor, and detailed categories for occupations\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with major occupational categories added\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename column\n",
    "    soc_structure_df.rename(columns={\n",
    "    \"SOC or O*NET-SOC 2019 Title\": \"major_occ_category\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Create new df and columns for merging\n",
    "    pct_onet_tasks_df[\"major_group_code\"] = pct_onet_tasks_df[\"soc_code\"].str[:2]\n",
    "    soc_structure_df = soc_structure_df.dropna(subset=['Major Group']).copy()\n",
    "    soc_structure_df[\"major_group_code\"] = soc_structure_df[\"Major Group\"].str[:2]\n",
    "    \n",
    "    \n",
    "    # Merge dfs\n",
    "    merged = pct_onet_tasks_df.merge(\n",
    "        soc_structure_df[['major_group_code', 'major_occ_category']],\n",
    "        on='major_group_code',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged.drop(columns=[\"major_group_code\"], inplace=True)\n",
    "\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "soc_structure_df = pd.read_csv(\"../extra_data/soc_structure_2019.csv\")\n",
    "pct_tasks_soc_structure_df = add_soc_structure(pct_onet_tasks_df, soc_structure_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional save to csv and show df for inspection\n",
    "\n",
    "# pct_tasks_soc_structure_df.to_csv(\"../merged_data_files/pct_tasks_soc_structure_df.csv\", index=False)\n",
    "# pct_tasks_soc_structure_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9742d3d",
   "metadata": {},
   "source": [
    "### Step 3: Add Updated (2019) SOC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca5f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7c465f",
   "metadata": {},
   "source": [
    "### Extra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2940280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_emp missing: 448\n",
      "['Funeral Service Managers' 'Buyers and Purchasing Agents, Farm Products'\n",
      " 'Wholesale and Retail Buyers, Except Farm Products'\n",
      " 'Purchasing Agents, Except Wholesale, Retail, and Farm Products'\n",
      " 'Assessors' 'Appraisers, Real Estate' 'Informatics Nurse Specialists'\n",
      " 'Software Developers, Applications'\n",
      " 'Software Developers, Systems Software'\n",
      " 'Telecommunications Engineering Specialists'\n",
      " 'Software Quality Assurance Engineers and Testers'\n",
      " 'Computer Systems Engineers/Architects' 'Web Administrators'\n",
      " 'Geospatial Information Scientists and Technologists'\n",
      " 'Geographic Information Systems Technicians'\n",
      " 'Data Warehousing Specialists' 'Business Intelligence Analysts'\n",
      " 'Information Technology Project Managers' 'Search Marketing Strategists'\n",
      " 'Video Game Designers' 'Document Management Specialists'\n",
      " 'Mathematical Technicians' 'Clinical Psychologists'\n",
      " 'Counseling Psychologists' 'Geophysical Data Technicians'\n",
      " 'Geological Sample Test Technicians'\n",
      " 'Substance Abuse and Behavioral Disorder Counselors'\n",
      " 'Mental Health Counselors' 'Court Reporters'\n",
      " 'Graduate Teaching Assistants' 'Librarians'\n",
      " 'Audio-Visual and Multimedia Collections Specialists'\n",
      " 'Teacher Assistants' 'Public Address System and Other Announcers'\n",
      " 'Broadcast News Analysts' 'Reporters and Correspondents'\n",
      " 'Radio Operators' 'Family and General Practitioners'\n",
      " 'Internists, General' 'Allergists and Immunologists' 'Hospitalists'\n",
      " 'Nuclear Medicine Physicians' 'Ophthalmologists' 'Pathologists'\n",
      " 'Physical Medicine and Rehabilitation Physicians'\n",
      " 'Preventive Medicine Physicians' 'Sports Medicine Physicians'\n",
      " 'Naturopathic Physicians' 'Orthoptists'\n",
      " 'Medical and Clinical Laboratory Technologists'\n",
      " 'Cytogenetic Technologists' 'Cytotechnologists'\n",
      " 'Histotechnologists and Histologic Technicians'\n",
      " 'Medical and Clinical Laboratory Technicians'\n",
      " 'Medical Records and Health Information Technicians' 'Home Health Aides'\n",
      " 'Combined Food Preparation and Serving Workers, Including Fast Food'\n",
      " 'Counter Attendants, Cafeteria, Food Concession, and Coffee Shop'\n",
      " 'Baristas' 'Gaming Supervisors' 'Spa Managers' 'Tour Guides and Escorts'\n",
      " 'Travel Guides' 'Personal Care Aides' 'Energy Brokers'\n",
      " 'Stock Clerks, Sales Floor'\n",
      " 'Stock Clerks- Stockroom, Warehouse, or Storage Yard'\n",
      " 'Computer Operators' 'Weatherization Installers and Technicians'\n",
      " 'Electrical and Electronic Equipment Assemblers'\n",
      " 'Electromechanical Equipment Assemblers'\n",
      " 'Computer-Controlled Machine Tool Operators, Metal and Plastic'\n",
      " 'Computer Numerically Controlled Machine Tool Programmers, Metal and Plastic'\n",
      " 'First-Line Supervisors of Helpers, Laborers, and Material Movers, Hand'\n",
      " 'Recycling Coordinators'\n",
      " 'First-Line Supervisors of Transportation and Material-Moving Machine and Vehicle Operators'\n",
      " 'Bus Drivers, School or Special Client' 'Taxi Drivers and Chauffeurs'\n",
      " 'Railroad Brake, Signal, and Switch Operators'\n",
      " 'Excavating and Loading Machine and Dragline Operators'\n",
      " 'Loading Machine Operators, Underground Mining' nan]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_emp_wage_data(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the employment wage data  and merges it into the given dataframe with the desired columns on the occupation code.\n",
    "        If a row doesn't match, we will fall back to merging on occupation title. \n",
    "        All column names in the resulting DataFrame will be lowercase.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input the df with the ONET and Claude data merged.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with employment and wage data\n",
    "    \"\"\"\n",
    "    emp_wage_df = pd.read_csv(\"../extra_data/emp_wage_national.csv\")\n",
    "\n",
    "    # Standardize for merges\n",
    "    df[\"occ_group_code\"] = df[\"occ_group_code\"].str[:7]\n",
    "    df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "    emp_wage_df[\"occ_title_normalized\"] = emp_wage_df[\"OCC_TITLE\"].str.lower().str.strip()\n",
    "\n",
    "    wage_cols = [\n",
    "            \"OCC_CODE\", \"AREA_TITLE\", \"TOT_EMP\", \"EMP_PRSE\", \"JOBS_1000\",\n",
    "            \"LOC_QUOTIENT\", \"PCT_TOTAL\", \"PCT_RPT\", \"H_MEAN\", \"A_MEAN\",\n",
    "            \"MEAN_PRSE\", \"H_PCT10\", \"H_PCT25\", \"H_MEDIAN\", \"H_PCT75\", \"H_PCT90\",\n",
    "            \"A_PCT10\", \"A_PCT25\", \"A_MEDIAN\", \"A_PCT75\", \"A_PCT90\", \"ANNUAL\", \"HOURLY\", \"occ_title_normalized\"\n",
    "        ]\n",
    "\n",
    "    # Perform merge\n",
    "    merged_df = pd.merge(\n",
    "        df,\n",
    "        emp_wage_df[wage_cols],\n",
    "        left_on=\"occ_group_code\",\n",
    "        right_on=\"OCC_CODE\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged_matched = merged_df[merged_df[\"TOT_EMP\"].notna()]\n",
    "    unmatched = merged_df[merged_df[\"TOT_EMP\"].isna()]\n",
    "    unmatched = unmatched.drop(columns=wage_cols, errors=\"ignore\")\n",
    "\n",
    "    merged_unmatched = pd.merge(\n",
    "        unmatched,\n",
    "        emp_wage_df[wage_cols],\n",
    "        left_on=\"title_normalized\",\n",
    "        right_on=\"occ_title_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    final_merged = pd.concat([merged_matched, merged_unmatched], ignore_index=True)\n",
    "    final_merged.drop(columns=[\"title_normalized\", \"occ_title_normalized\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "    # Convert all column names to lowercase\n",
    "    final_merged.columns = [col.lower() for col in final_merged.columns]\n",
    "\n",
    "    return final_merged\n",
    "\n",
    "task_emp_wage_df = add_emp_wage_data(task_soc_pct_all)\n",
    "#display(task_emp_wage_df)\n",
    "print(\"tot_emp missing:\", task_emp_wage_df[\"tot_emp\"].isna().sum())\n",
    "print(task_emp_wage_df.loc[task_emp_wage_df[\"tot_emp\"].isna(), \"title\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task ratings processing\n",
    "\n",
    "def add_task_ratings():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function reads the task ratings from an Excel file, processes it to extract frequency, importance, and relevance ratings,\n",
    "        and merges them into a single DataFrame with the desired structure.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input the df with the ONET, Claude, and emp and wage data merged.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with task ratings including frequency, importance, and relevance.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    task_ratings_df = pd.read_csv(\"../extra_data/task_ratings.csv\")\n",
    "\n",
    "\n",
    "# Frequency mapping. Assuming a 52 week year with 5 working days per week, these are corresponding survey questions::\n",
    "# 1 Once per year or less (Assuming 1 time per year)\n",
    "# 2 More than once per year (Assuming 3 times per year)\n",
    "# 3 More than once per month (Assuming 48 times per year, 3 times per month)\n",
    "# 4 More than once per week (Assuming 130 times per year, 2.5 times per week)\n",
    "# 5 Daily\n",
    "# 6 Several times per day (Assuming 3 times per day)\n",
    "# 7 Hourly or more often (Assuming 12 times per day, 1.5 times per hour)\n",
    "    frequency_weights = {\n",
    "        1: 1 / 260,\n",
    "        2: 3 / 260,\n",
    "        3: 48 / 260,\n",
    "        4: 130 / 260,\n",
    "        5: 1,\n",
    "        6: 3,\n",
    "        7: 12\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get freq rows, drop unusable ones, generate freq aggregates\n",
    "    freq_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"FT\"].copy()\n",
    "\n",
    "    # Drop rows without category or invalid categories\n",
    "    freq_df = freq_df[pd.to_numeric(freq_df[\"Category\"], errors='coerce').notnull()]\n",
    "    freq_df[\"Category\"] = freq_df[\"Category\"].astype(int)\n",
    "\n",
    "    # Apply weights\n",
    "    freq_df[\"freq_mean\"] = freq_df[\"Data Value\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_lower\"] = freq_df[\"Lower CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_upper\"] = freq_df[\"Upper CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "\n",
    "    # Sum across categories to get per-task total\n",
    "    freq_agg = freq_df.groupby([\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"]).agg({\n",
    "        \"freq_mean\": \"sum\",\n",
    "        \"freq_lower\": \"sum\",\n",
    "        \"freq_upper\": \"sum\"\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Get importance and relevance ratings\n",
    "    importance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"IM\"].copy()\n",
    "    importance_df = importance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    importance_df = importance_df.rename(columns={\n",
    "        \"Data Value\": \"importance\",\n",
    "        \"Lower CI Bound\": \"importance_lower\",\n",
    "        \"Upper CI Bound\": \"importance_upper\"\n",
    "    })\n",
    "\n",
    "    relevance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"RT\"].copy()\n",
    "    relevance_df = relevance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    relevance_df = relevance_df.rename(columns={\n",
    "        \"Data Value\": \"relevance\",\n",
    "        \"Lower CI Bound\": \"relevance_lower\",\n",
    "        \"Upper CI Bound\": \"relevance_upper\"\n",
    "    })\n",
    "\n",
    "\n",
    "    # Merge ratings\n",
    "    merged_ratings = freq_agg.merge(importance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "    merged_ratings = merged_ratings.merge(relevance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "\n",
    "\n",
    "    merged_ratings[\"task_normalized\"] = merged_ratings[\"Task\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "    return merged_ratings\n",
    "\n",
    "ratings_df = add_task_ratings()\n",
    "#display(ratings_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669e5157",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    115\u001b[39m     merged = merged[cols]\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m merged\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m task_final = \u001b[43mmerge_all_and_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_emp_wage_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m task_final.to_csv(\u001b[33m\"\u001b[39m\u001b[33m../new_data/tasks_final.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m#display(task_final.reset_index(drop=True))\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mmerge_all_and_cleanup\u001b[39m\u001b[34m(df, ratings_df)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03mDescription:\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m    This function merges the task data with the ratings data and performs final cleanup.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[33;03m    pd.DataFrame: Final merged DataFrame with all necessary information.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Normalize task names\u001b[39;00m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Apply batch lemmatization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtask_normalized\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mbatch_lemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m ratings_df[\u001b[33m\"\u001b[39m\u001b[33mtask_normalized\u001b[39m\u001b[33m\"\u001b[39m] = batch_lemmatize(ratings_df[\u001b[33m\"\u001b[39m\u001b[33mTask\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m     48\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtitle_normalized\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m].str.lower().str.strip()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mbatch_lemmatize\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m     14\u001b[39m cleaned = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlemmas\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemma_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_punct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_space\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\language.py:1622\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1621\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:50\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:130\u001b[39m, in \u001b[36mAttributeRuler.__call__\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    128\u001b[39m error_handler = \u001b[38;5;28mself\u001b[39m.get_error_handler()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     matches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_annotations(doc, matches)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\teddy\\Downloads\\OAIPR\\Technical\\aei_data_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:140\u001b[39m, in \u001b[36mAttributeRuler.match\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    137\u001b[39m matches = \u001b[38;5;28mself\u001b[39m.matcher(doc, allow_missing=\u001b[38;5;28;01mTrue\u001b[39;00m, as_spans=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[32m    139\u001b[39m matches = [\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    141\u001b[39m ]\n\u001b[32m    142\u001b[39m matches.sort()\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Merge all and final cleanup\n",
    "\n",
    "def batch_lemmatize(texts):\n",
    "    \"\"\"\n",
    "    Efficiently lemmatize a list of strings using spaCy's nlp.pipe().\n",
    "    Skips punctuation, whitespace, and possessives.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Handle empty/null strings\n",
    "    processed_texts = [str(text).strip() if text and str(text).strip() else \" \" for text in texts]\n",
    "    \n",
    "    cleaned = []\n",
    "    try:\n",
    "        for doc in nlp.pipe(processed_texts, batch_size=1000, disable=[\"ner\", \"parser\"]):\n",
    "            lemmas = [\n",
    "                token.lemma_ for token in doc\n",
    "                if not token.is_punct and not token.is_space and token.text != \"'s\"\n",
    "            ]\n",
    "            result = \" \".join(lemmas).strip()\n",
    "            cleaned.append(result if result else \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch_lemmatize: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def merge_all_and_cleanup(df, ratings_df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and performs final cleanup.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing task data.\n",
    "        ratings_df (pd.DataFrame): DataFrame containing task ratings.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize task names\n",
    "\n",
    "    # Apply batch lemmatization\n",
    "    df[\"task_normalized\"] = batch_lemmatize(df[\"task\"].tolist())\n",
    "    ratings_df[\"task_normalized\"] = batch_lemmatize(ratings_df[\"Task\"].tolist())\n",
    "\n",
    "    df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "    ratings_df[\"title_normalized\"] = ratings_df[\"Title\"].str.lower().str.strip()\n",
    "\n",
    "    # Count how many times each normalized task appears\n",
    "    task_counts = df[\"task_normalized\"].value_counts()\n",
    "\n",
    "    # Boolean mask for duplicate vs. unique tasks\n",
    "    is_duplicate = df[\"task_normalized\"].isin(task_counts[task_counts > 1].index)\n",
    "    is_unique = ~is_duplicate\n",
    "\n",
    "    # Split the dataframe\n",
    "    df_duplicate_tasks = df[is_duplicate].copy()\n",
    "    df_unique_tasks = df[is_unique].copy()\n",
    "\n",
    "    # Count how many times each normalized task appears\n",
    "    task_counts_ratings = ratings_df[\"task_normalized\"].value_counts()\n",
    "\n",
    "    # Boolean mask for duplicate vs. unique tasks\n",
    "    is_duplicate_ratings = ratings_df[\"task_normalized\"].isin(task_counts_ratings[task_counts_ratings > 1].index)\n",
    "    is_unique_ratings = ~is_duplicate_ratings\n",
    "\n",
    "    # Split the dataframe\n",
    "    df_duplicate_tasks_ratings = ratings_df[is_duplicate_ratings].copy()\n",
    "    df_unique_tasks_ratings = ratings_df[is_unique_ratings].copy()\n",
    "\n",
    "    # Merge on unique tasks\n",
    "    merged_unique = df_unique_tasks.merge(\n",
    "        df_unique_tasks_ratings[[\n",
    "            \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "            \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "            \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "            \"task_normalized\", \"title_normalized\"\n",
    "        ]],\n",
    "        on=[\"task_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Merge on both title and task\n",
    "    merged_duplicate = df_duplicate_tasks.merge(\n",
    "        df_duplicate_tasks_ratings[[\n",
    "            \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "            \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "            \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "            \"task_normalized\", \"title_normalized\"\n",
    "        ]],\n",
    "        on=[\"task_normalized\", \"title_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged = pd.concat([merged_unique, merged_duplicate], ignore_index=True)\n",
    "\n",
    "    # Replace placeholders with NaN\n",
    "    placeholder_values = [\"#\", \"*\", \"\", \"n/a\", \"na\", \"--\"]\n",
    "    merged.replace(placeholder_values, pd.NA, inplace=True)\n",
    "\n",
    "    # Drop fully empty columns\n",
    "    merged.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # Drop 'occ_code' and 'task_name'\n",
    "    merged.drop(columns=[\"occ_code\", \"task_name\", \"title_normalized\", \"title_normalized_x\", \"title_normalized_y\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Reorder columns: make 'task' and 'task_normalized' first\n",
    "    cols = merged.columns.tolist()\n",
    "    for col in [\"task_normalized\", \"task\"]:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    merged = merged[cols]\n",
    "\n",
    "    return merged\n",
    "\n",
    "task_final = merge_all_and_cleanup(task_emp_wage_df, ratings_df)\n",
    "task_final.to_csv(\"../new_data/tasks_final.csv\", index=False)\n",
    "#display(task_final.reset_index(drop=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
